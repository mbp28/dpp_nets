{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "from dpp_nets.utils.io import make_embd, make_tensor_dataset\n",
    "from dpp_nets.my_torch.utilities import pad_tensor\n",
    "\n",
    "from dpp_nets.layers.layers import DeepSetBaseline\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Baseline (Deep Sets) Trainer')\n",
    "\n",
    "parser.add_argument('-a', '--aspect', type=str, choices=['aspect1', 'aspect2', 'aspect3', 'all'],\n",
    "                    help='what is the target?', required=True)\n",
    "parser.add_argument('--remote', type=int,\n",
    "                    help='training locally or on cluster?', required=True)\n",
    "\n",
    "parser.add_argument('--data_path_local', type=str, default='/Users/Max/data/beer_reviews',\n",
    "                    help='where is the data folder locally?')\n",
    "parser.add_argument('--data_path_remote', type=str, default='/cluster/home/paulusm/data/beer_reviews',\n",
    "                    help='where is the data folder?')\n",
    "\n",
    "parser.add_argument('--ckp_path_local', type=str, default='/Users/Max/checkpoints/beer_reviews',\n",
    "                    help='where is the data folder locally?')\n",
    "\n",
    "parser.add_argument('--ckp_path_remote', type=str, default='/cluster/home/paulusm/checkpoints/beer_reviews',\n",
    "                    help='where is the data folder?')\n",
    "\n",
    "parser.add_argument('-b', '--batch-size', default=50, type=int,\n",
    "                    metavar='N', help='mini-batch size (default: 50)')\n",
    "parser.add_argument('--epochs', default=100, type=int, metavar='N',\n",
    "                    help='number of total epochs to run')\n",
    "#parser.add_argument('--lr-k', '--learning-rate-k', default=0.1, type=float,\n",
    "#                    metavar='LRk', help='initial learning rate for kernel net')\n",
    "#parser.add_argument('--lr-p', '--learning-rate-p', default=0.1, type=float,\n",
    "#                    metavar='LRp', help='initial learning rate for pred net')\n",
    "parser.add_argument('--lr', '--learning-rate', default=1e-4, type=float,\n",
    "                    metavar='LR', help='initial learning rate for baseline')\n",
    "#parser.add_argument('--reg', type=float, required=True,\n",
    "#                    metavar='reg', help='regularization constant')\n",
    "#parser.add_argument('--reg-mean', type=float, required=True,\n",
    "#                    metavar='reg_mean', help='regularization_mean')\n",
    "\n",
    "args = parser.parse_args(\"-a all --remote 0\".split())\n",
    "\n",
    "val_path   = os.path.join(args.data_path_local, str.join(\".\",['reviews', args.aspect, 'heldout.txt.gz']))\n",
    "embd_path = os.path.join(args.data_path_local, 'review+wiki.filtered.200.txt.gz')\n",
    "#embd, word_to_ix = make_embd(embd_path)\n",
    "#val_set = make_tensor_dataset(val_path, word_to_ix)\n",
    "#val_loader = DataLoader(val_set, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_path   = os.path.join(args.data_path_local, str.join(\".\",['reviews', args.aspect, 'train.txt.gz']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dpp_nets.utils.language import create_clean_vocabulary\n",
    "nlp, vocab, embd = create_clean_vocabulary(embd_path, train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embd.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dpp_nets.utils.language import BeerDataset, process_batch\n",
    "from torch.utils.data import DataLoader\n",
    "ds = BeerDataset(val_path)\n",
    "dl = DataLoader(ds, 1, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object filter_stops.<locals>.<genexpr> at 0x15e5433b8>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_stops(doc.sents[0], vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def yield_sen_vec(doc, vocab, embd):\n",
    "    seen = set()\n",
    "    for s in doc.sents:\n",
    "        t = tuple((filter_stops(s, vocab)))\n",
    "        if t and t not in seen:\n",
    "            seen.add(t)\n",
    "            ixs = torch.LongTensor([vocab.word2index[word] for word in t])\n",
    "            embd_mat = embd(Variable(ixs)).mean(0)\n",
    "            yield embd_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch_sens(nlp, vocab, embd, batch):\n",
    "\n",
    "    MAX_CHUNK_LENGTH = 271\n",
    "    MAX_SENS_NO = 397\n",
    "\n",
    "    # maxi = 0\n",
    "    # for review in batch['review']:\n",
    "     #   doc = nlp(review)\n",
    "     #   rep = torch.stack(list(yield_chunk_vec(doc, vocab, embd))).squeeze()\n",
    "     #   maxi = max(maxi, rep.size(0))\n",
    "\n",
    "    reps = []\n",
    "    for review in batch['review']:\n",
    "        doc = nlp(review)\n",
    "        rep = torch.stack(list(yield_sen_vec(doc, vocab, embd))).squeeze()\n",
    "        rep = torch.cat([rep, Variable(torch.zeros(MAX_SENS_NO + 1 - rep.size(0),rep.size(1)))],dim=0)\n",
    "        reps.append(rep)\n",
    "\n",
    "    data_tensor =  torch.stack(reps)\n",
    "    target_tensor = Variable(torch.stack(batch['target']).t().float())\n",
    "    \n",
    "    return data_tensor, target_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = ChunkTrainer(200,500,200,200,3)(data_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.cat([rep, Variable(torch.zeros(maxi + 1 - rep.size(0),rep.size(1)))],dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l = []\n",
    "for chunk in list(yield_chunks(doc, vocab)):\n",
    "    c = embd(Variable(chunk)).mean(0)\n",
    "    l.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "\n",
    "anno_path = os.path.join(args.data_path_local, 'annotations.json')\n",
    "\n",
    "class BeerDatasetAnnotated(Dataset):\n",
    "    \"\"\"BeerDataset.\"\"\"\n",
    "\n",
    "    def __init__(self, anno_path, aspect='all'):\n",
    "        \n",
    "        # Compute size of the data set      \n",
    "        self.aspect = aspect\n",
    "        \n",
    "\n",
    "        self.lines = []\n",
    "        with open(anno_path) as f:\n",
    "            for line in f:\n",
    "                item = json.loads(line)\n",
    "\n",
    "                # Get doc\n",
    "                doc = nlp(ast.literal_eval(item['raw'])['review/text']\n",
    "\n",
    "                # Get annotations\n",
    "                a0 = item['0']\n",
    "                a1 = item['1']\n",
    "                a2 = item['2']\n",
    "\n",
    "                # Get target\n",
    "                target = item['y']\n",
    "\n",
    "                self.lines.append((doc,(a0, a1, a2), target))\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lines)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        instance = self.lines[idx]            \n",
    "        sample = {'review': instance[0], 'target': instance[2]}\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ds = BeerDataset(val_path)\n",
    "dl = DataLoader(ds, batch_size=12, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "process_batch(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dpp_nets.my_torch.utilities import pad_tensor\n",
    "\n",
    "def filter_stops(tree, vocab):\n",
    "    return (token.text for token in tree if not token.is_stop and token.text in vocab.word2index)\n",
    "\n",
    "#def yield_words(doc, vocab)\n",
    "\n",
    "def yield_chunks(doc, vocab, MAX_CHUNK_LENGTH):\n",
    "    seen = set()\n",
    "    for token in doc:\n",
    "        t = tuple((filter_stops(token.subtree, vocab)))\n",
    "        if t and t not in seen:\n",
    "            seen.add(t)\n",
    "            #ixs = [vocab.word2index[word] if word in vocab.word2index else print(word) for word in t]\n",
    "            ixs = torch.LongTensor([vocab.word2index[word] for word in t])\n",
    "            ixs = pad_tensor(ixs,0,0,MAX_CHUNK_LENGTH)\n",
    "            yield ixs\n",
    "            \n",
    "def yield_sentences(doc, vocab, MAX_SENTENCE_LENGTH):\n",
    "    seen = set()\n",
    "    for sen in doc.sents:\n",
    "        t = tuple((filter_stops(sen)))\n",
    "        if t and t not in seen:\n",
    "            seen.add(t)\n",
    "            #ixs = [vocab.word2index[word] for word in t]\n",
    "            ixs = torch.LongTensor([vocab.word2index[word] for word in t])\n",
    "            ixs = pad_tensor(ixs,0,0,MAX_SENTENCE_LENGTH)\n",
    "            yield ixs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "210000it [33:12, 105.37it/s]\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import tqdm\n",
    "MAX_SENS_NO = 0\n",
    "\n",
    "with gzip.open(train_path, 'rt') as f:\n",
    "    for line in tqdm.tqdm(f):\n",
    "        target, sep, review = line.partition('\\t')\n",
    "        doc = nlp(review)\n",
    "        MAX_SENS_NO = max(len(list(doc.sents)), MAX_SENS_NO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open(short_path, 'wt') as f:\n",
    "    for line in lines[:100]:\n",
    "        f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_CHUNK_LENGTH = max([len(m) for l in measure_list for m in l])\n",
    "Max_CHUNK_NO = max([len(l) for l in measure_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from spacy.symbols import nsubj, VERB\n",
    "\n",
    "sentence = list(doc.sents)[5]\n",
    "for token in sentence:\n",
    "    #print(token, list(token.children), token.head, token.dep_, list(token.lefts), list(token.rights), list(token.subtree))\n",
    "    #print(token, list(token.subtree)) #,list(token.lefts),list(token.rights),token.left_edge, token.right_edge)\n",
    "    print((token, list(token.subtree)))\n",
    "    #print(token, list(token.ancestors))\n",
    "    #print(token, token.vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_iterator(data_path):\n",
    "    with gzip.open(data_path, 'rt') as f:\n",
    "        for line in f:\n",
    "            target, sep, words = line.partition(\"\\t\")\n",
    "            words, target = words.split(), target.split()\n",
    "            if len(words):\n",
    "                target = torch.Tensor([float(v) for v in target])\n",
    "                yield words, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "n = 0\n",
    "maxi = 0\n",
    "mean = M2 = 0.0\n",
    "\n",
    "with gzip.open(val_path, 'rt') as f:\n",
    "    for line in f:\n",
    "        target, sep, review = line.partition(\"\\t\")\n",
    "        n_sentences = len(nltk.sent_tokenize(review))\n",
    "        x = n_sentences\n",
    "        \n",
    "        n += 1\n",
    "        if x == 101: \n",
    "            break\n",
    "        maxi = max(x, maxi)\n",
    "        delta = x - mean\n",
    "        mean += delta/n\n",
    "        delta2 = x - mean\n",
    "        M2 += delta*delta2\n",
    "\n",
    "        #i += 1\n",
    "        #if i > 21:\n",
    "         #   break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# How to create sentences from a review\n",
    "sentences = nltk.sent_tokenize(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# How to create words from a sentence\n",
    "words = nltk.word_tokenize(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# How to remove stop words from a sentence\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "operators = set(('no', 'not'))\n",
    "punctuation = set(string.punctuation)\n",
    "stop = (set(stopwords.words('english')) | punctuation) - operators \n",
    "fwords = [[word for word in words if word not in stop] for words in [nltk.word_tokenize(sen) for sen in sentences]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# How to create bigrams (after stop-word removal? - would do so)\n",
    "\n",
    "print(list(nltk.bigrams(words)))\n",
    "print(30*'-')\n",
    "list(nltk.bigrams(fwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "anno_path = os.path.join(args.data_path_local, 'annotations.json')\n",
    "def read_rationales(path):\n",
    "    \"\"\"\n",
    "    This reads the json.annotations file. \n",
    "    Creates a list of dictionaries, which holds the 994 reviews for which\n",
    "    sentence-level annotations are available. \n",
    "    \"\"\"\n",
    "    data = []\n",
    "    fopen = gzip.open if path.endswith(\".gz\") else open\n",
    "    with fopen(path) as fin:\n",
    "        for line in fin:\n",
    "            item = json.loads(line)\n",
    "            data.append(item)\n",
    "    return data\n",
    "\n",
    "rationales = read_rationales(anno_path)\n",
    "\n",
    "rationales[0].keys()\n",
    "\n",
    "nltk.sent_tokenize(rationales[1]['raw'])[10]\n",
    "\n",
    "import string\n",
    "raw = \"\".join([\" \"+i if not i.startswith(\"'\") and i not in string.punctuation else i for i in rationales[2]['x']]).strip()\n",
    "nltk.sent_tokenize(raw)\n",
    "\n",
    "print(rationales[2]['2'][0])\n",
    "rationales[2]['x'][49:63]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "doc = nlp(ast.literal_eval(data[0]['raw']) ['review/text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data[0]['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import tqdm\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "path = os.path.join(args.data_path_local, str.join(\".\",['reviews', args.aspect, 'heldout.txt.gz']))\n",
    "def simple_iterator(path):\n",
    "    with gzip.open(path, 'rt') as f:\n",
    "        for line in f:\n",
    "            target, sep, review = line.partition(\"\\t\")\n",
    "            yield review, target\n",
    "            \n",
    "# Create set of filtered sentences for each review:\n",
    "operators = set(('no', 'not'))\n",
    "punctuation = set(string.punctuation)\n",
    "stop = set(stopwords.words('english')) | punctuation | set('...')  - operators \n",
    "\n",
    "for review, target in tqdm.tqdm(simple_iterator(path)):\n",
    "\n",
    "    # Split review into sentences\n",
    "    sens = nltk.sent_tokenize(review)\n",
    "    \n",
    "    # Split each sentence into words\n",
    "    sens = [nltk.word_tokenize(sen) for sen in sens]\n",
    "    \n",
    "    # Remove stop words\n",
    "    fwords = [[word for word in sen if word not in stop] for sen in sens]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Noun-Chunks\n",
    "def noun_chunks(doc):\n",
    "    my_processed_review = []\n",
    "    for chunk in doc.noun_chunks:\n",
    "        chunk = tuple(filter(lambda token: not token.is_stop, chunk))\n",
    "        chunk = tuple(word.text for word in chunk)\n",
    "        my_processed_review.append(chunk)\n",
    "    my_processed_review = list(filter(None, my_processed_review))\n",
    "    return my_processed_review\n",
    "\n",
    "# Sub-Trees\n",
    "def sub_trees(doc):\n",
    "    my_processed_review = []\n",
    "    for sen in doc.sents:\n",
    "        for token in sen:\n",
    "            chunk = token.subtree\n",
    "            chunk = tuple(filter(lambda token: not token.is_stop, chunk))\n",
    "            #chunk = tuple(word.text for word in chunk)\n",
    "            my_processed_review.append(chunk)\n",
    "    my_processed_review = list(filter(None, my_processed_review))\n",
    "    my_processed_review = list(set(my_processed_review))\n",
    "    return my_processed_review\n",
    "\n",
    "# Sentence-Level processing. \n",
    "def sentences(doc):\n",
    "    my_processed_review = []\n",
    "    for sen in doc.sents:\n",
    "        chunk = tuple(token for token in sen if not token.is_stop)\n",
    "        chunk = tuple(word.text for word in chunk)\n",
    "        my_processed_review.append(chunk)\n",
    "    my_processed_review = list(filter(None, my_processed_review))\n",
    "    return my_processed_review\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
