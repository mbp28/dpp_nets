{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "from dpp_nets.utils.io import make_embd, make_tensor_dataset\n",
    "from dpp_nets.my_torch.utilities import pad_tensor\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "import string\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "import torch\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "\n",
    "import gzip\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = '/Users/Max/data/beer_reviews/reviews.all.train.chunks.txt.gz'\n",
    "word_path = '/Users/Max/data/beer_reviews/reviews.all.train.words.txt.gz'\n",
    "embd_path = '/Users/Max/data/beer_reviews/review+wiki.filtered.200.txt.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        # Basic Indexing\n",
    "        self.word2index = {}\n",
    "        self.index2word = {}\n",
    "        \n",
    "        # Keeping track of vocabulary\n",
    "        self.vocab_size = 0 \n",
    "        self.word2count = {}\n",
    "        \n",
    "        # Vector Dictionaries\n",
    "        self.pretrained = {}\n",
    "        self.random = {}\n",
    "        self.word2vec = {}\n",
    "        self.index2vec = {}\n",
    "\n",
    "        # Set of Stop Words\n",
    "        self.stop_words = set()\n",
    "        \n",
    "        self.Embedding = None\n",
    "        self.EmbeddingBag = None\n",
    "    \n",
    "    def setStops(self):\n",
    "        \n",
    "        self.stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "        make_stops = set(string.punctuation + '\\n' + '\\t' + '...')\n",
    "        unmake_stops = set(('no', 'not'))\n",
    "\n",
    "        self.stop_words = self.stop_words.union(make_stops)\n",
    "        self.stop_words = self.stop_words.difference(unmake_stops)      \n",
    "        \n",
    "    def loadPretrained(self, embd_path):\n",
    "        \n",
    "        self.pretrained = {}\n",
    "        with gzip.open(embd_path, 'rt') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    word, *embd = line.split()\n",
    "                    vec = torch.FloatTensor([float(dim) for dim in embd])            \n",
    "                    self.pretrained[word]  = vec\n",
    "                    \n",
    "    def loadCorpus(self, word_path):\n",
    "        \n",
    "        with gzip.open(data_path, 'rt') as f:\n",
    "\n",
    "            for line in f:\n",
    "                _, review = line.split('\\D')\n",
    "                review = tuple(tuple(chunk.split('\\W')) for chunk in review.split('\\T'))\n",
    "\n",
    "                for words in review:\n",
    "                    vocab.addWords(words)\n",
    "            \n",
    "    def addWords(self, words):\n",
    "        \"\"\"\n",
    "        words: seq containing variable no of words\n",
    "        \"\"\"\n",
    "        for word in words:\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "\n",
    "        if word not in self.word2index:\n",
    "            \n",
    "            # Keeping track of vocabulary\n",
    "            self.vocab_size += 1\n",
    "            self.word2count[word] = 1\n",
    "            \n",
    "            # Basic Indexing\n",
    "            self.word2index[word] = self.vocab_size\n",
    "            self.index2word[self.vocab_size] = word\n",
    "            \n",
    "            # Add word vector\n",
    "            if word in self.pretrained:\n",
    "                vec = self.pretrained[word]\n",
    "                self.word2vec[word] = vec\n",
    "                self.index2vec[self.vocab_size] = vec\n",
    "                \n",
    "            else:\n",
    "                vec = torch.randn(200)\n",
    "                self.random[word] = vec\n",
    "                self.word2vec[word] = vec\n",
    "                self.index2vec[self.vocab_size] = vec\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "            \n",
    "    def updateEmbedding(self):\n",
    "        \n",
    "        vocab_size = len(self.index2vec) + 1\n",
    "        EMBD_DIM = 200\n",
    "        \n",
    "        self.Embedding = nn.Embedding(vocab_size, EMBD_DIM, padding_idx=0)\n",
    "        self.EmbeddingBag = nn.EmbeddingBag(vocab_size, EMBD_DIM)\n",
    "        embd_matrix = torch.zeros(vocab_size, EMBD_DIM)\n",
    "        \n",
    "        for ix, vec in vocab.index2vec.items():\n",
    "            embd_matrix[ix] = vec\n",
    "        \n",
    "        embd_dict = OrderedDict([('weight', embd_matrix)])\n",
    "        self.Embedding.load_state_dict(embd_dict)\n",
    "        self.EmbeddingBag.load_state_dict(embd_dict)\n",
    "    \n",
    "    def checkWord(self, word, min_count):\n",
    "        if word not in vocab.stop_words and word in vocab.word2index and vocab.word2index[word] > min_count:\n",
    "            return word\n",
    "            \n",
    "    def filterReview(self, review):\n",
    "        \"\"\"\n",
    "        review should be like our data set\n",
    "        \"\"\"\n",
    "        f_review = []\n",
    "        seen = set()\n",
    "        \n",
    "        for tup in review:\n",
    "            f_tuple = []\n",
    "            \n",
    "            for word in tup:\n",
    "                word = self.checkWord(word, 10)\n",
    "                if word:\n",
    "                    f_tuple.append(word)\n",
    "            \n",
    "            f_tuple = tuple(f_tuple)    \n",
    "            \n",
    "            if f_tuple and f_tuple not in seen:\n",
    "                seen.add(f_tuple)\n",
    "                f_review.append(f_tuple)\n",
    "                \n",
    "        return f_review\n",
    "    \n",
    "    def mapIndicesBatch(self, reviews):\n",
    "        \n",
    "        f_review = []\n",
    "        offset = []\n",
    "        i = 0\n",
    "\n",
    "        for review in reviews:\n",
    "            seen = set()\n",
    "            \n",
    "            for tup in review: \n",
    "                f_tuple = []\n",
    "                \n",
    "                for word in tup:\n",
    "                    word = vocab.checkWord(word, 10)\n",
    "                    if word:\n",
    "                        f_tuple.append(word)\n",
    "\n",
    "                f_tuple = tuple(f_tuple)    \n",
    "\n",
    "                if f_tuple and f_tuple not in seen:\n",
    "                    seen.add(f_tuple)\n",
    "                    f_review.extend([vocab.word2index[word] for word in f_tuple])\n",
    "                    offset.append(i)\n",
    "                    i += len(f_tuple)\n",
    "            \n",
    "        f_review, offset = torch.LongTensor(f_review), torch.LongTensor(offset)   \n",
    "        return f_review, offset\n",
    "    \n",
    "    def mapIndices(self, review):\n",
    "        \n",
    "        f_review = []\n",
    "        offset = []\n",
    "        seen = set()\n",
    "        i = 0\n",
    "\n",
    "        for tup in review:\n",
    "            f_tuple = []\n",
    "\n",
    "            for word in tup:\n",
    "                word = vocab.checkWord(word, 10)\n",
    "                if word:\n",
    "                    f_tuple.append(word)\n",
    "\n",
    "            f_tuple = tuple(f_tuple)    \n",
    "\n",
    "            if f_tuple and f_tuple not in seen:\n",
    "                seen.add(f_tuple)\n",
    "                f_review.extend([vocab.word2index[word] for word in f_tuple])\n",
    "                offset.append(i)\n",
    "                i += len(f_tuple)\n",
    "\n",
    "        f_review, offset = torch.LongTensor(f_review), torch.LongTensor(offset)   \n",
    "        return f_review, offset\n",
    "    \n",
    "    def returnEmbds(self, review):\n",
    "        \n",
    "        f_review = []\n",
    "        offset = []\n",
    "        seen = set()\n",
    "        i = 0\n",
    "\n",
    "        for tup in review:\n",
    "            f_tuple = []\n",
    "\n",
    "            for word in tup:\n",
    "                word = vocab.checkWord(word, 10)\n",
    "                if word:\n",
    "                    f_tuple.append(word)\n",
    "\n",
    "            f_tuple = tuple(f_tuple)    \n",
    "\n",
    "            if f_tuple and f_tuple not in seen:\n",
    "                seen.add(f_tuple)\n",
    "                f_review.extend([vocab.word2index[word] for word in f_tuple])\n",
    "                offset.append(i)\n",
    "                i += len(f_tuple)\n",
    "\n",
    "        f_review, offset = Variable(torch.LongTensor(f_review)), Variable(torch.LongTensor(offset))\n",
    "        embd = self.EmbeddingBag(f_review, offset)\n",
    "\n",
    "        return embd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BeerDataset(Dataset):\n",
    "    \"\"\"BeerDataset.\"\"\"\n",
    "\n",
    "    def __init__(self, data_path, aspect='all'):\n",
    "        \n",
    "        # Compute size of the data set      \n",
    "        self.aspect = aspect\n",
    "        self.vocab = vocab\n",
    "        \n",
    "        with gzip.open(data_path, 'rt') as f:\n",
    "            self.lines = f.readlines()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lines)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        # Decode\n",
    "        target, review = self.lines[idx].split('\\D')\n",
    "        \n",
    "        # Target\n",
    "        target = torch.FloatTensor([float(t) for t in target.split()[:3]])\n",
    "        \n",
    "        # Review\n",
    "        review = tuple(tuple(chunk.split('\\W')) for chunk in review.split('\\T'))\n",
    "        #ixs, offset = self.vocab.mapIndices(review)\n",
    "        \n",
    "        #sample = {'ixs': ixs, 'offset': offset, 'target': target}\n",
    "        sample = {'review': review, 'target': target}\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = Vocabulary()\n",
    "vocab.loadPretrained(embd_path)\n",
    "vocab.setStops()\n",
    "vocab.loadCorpus(word_path)\n",
    "vocab.updateEmbedding()\n",
    "\n",
    "ds = BeerDataset(data_path, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abc/def'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 'abc'\n",
    "b = 'def'\n",
    "os.path.join(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65.42647176500031\n"
     ]
    }
   ],
   "source": [
    "# Solution 2 - using mycollate2 + new KernelNetwork\n",
    "from dpp_nets.layers.layers import MarginalSampler, PredNet\n",
    "\n",
    "def my_collate2(batch, vocab=vocab):\n",
    "\n",
    "    # Create indices\n",
    "    s_ix, e_ix, i = [], [], 0\n",
    "\n",
    "    for l in [len(vocab.filterReview(d['review'])) for d in batch]:\n",
    "        s_ix.append(i)\n",
    "        i += l\n",
    "        e_ix.append(i)\n",
    "    \n",
    "    # Map to Embeddings\n",
    "    batch_review = [review['review'] for review in batch]\n",
    "    ixs, offsets =  vocab.mapIndicesBatch(batch_review)\n",
    "    embd = vocab.EmbeddingBag(Variable(ixs), Variable(offsets))\n",
    "\n",
    "    # Create target vector\n",
    "    target_tensor = Variable(torch.stack([d['target'] for d in batch]))\n",
    "    \n",
    "    return embd, target_tensor, s_ix, e_ix\n",
    "\n",
    "class KernelVar(nn.Module):\n",
    "\n",
    "    def __init__(self, embd_dim, hidden_dim, kernel_dim):\n",
    "        \"\"\"\n",
    "        Currently, this creates a 2-hidden-layer network \n",
    "        with ELU non-linearities.\n",
    "\n",
    "        \"\"\"\n",
    "        super(KernelVar, self).__init__()\n",
    "        self.embd_dim = embd_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_dim = kernel_dim\n",
    "\n",
    "        self.layer1 = nn.Linear(2 * embd_dim, hidden_dim)\n",
    "        self.layer2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.layer3 = nn.Linear(hidden_dim, kernel_dim)\n",
    "\n",
    "        self.net = nn.Sequential(self.layer1, nn.Tanh(), self.layer2, nn.Tanh(), self.layer3)\n",
    "\n",
    "        self.s_ix = None\n",
    "        self.e_ix = None\n",
    "\n",
    "\n",
    "    def forward(self, embd):\n",
    "        \"\"\"\n",
    "        Given words, returns batch_kernel of dimension\n",
    "        [-1, kernel_dim]\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create context\n",
    "        context = []\n",
    "        for s, e in zip(self.s_ix, self.e_ix):\n",
    "            text = embd[s:e].sum(0, keepdim=True).expand_as(embd[s:e])\n",
    "            context.append(text)\n",
    "        context = torch.cat(context, dim=0)\n",
    "        batch_x = torch.cat([embd, context], dim=1)\n",
    "        \n",
    "        batch_kernel = self.net(batch_x)\n",
    "\n",
    "        return batch_kernel , embd \n",
    "\n",
    "from timeit import default_timer\n",
    "start = default_timer()\n",
    "\n",
    "dl = DataLoader(ds, batch_size=500, collate_fn=my_collate2)\n",
    "for batch in dl:\n",
    "    break\n",
    "\n",
    "embd, target, s_ix, e_ix = batch\n",
    "\n",
    "embd_dim = 200\n",
    "hidden_dim = 500\n",
    "kernel_dim = 200\n",
    "enc_dim = 200\n",
    "target_dim = 3\n",
    "\n",
    "kernel_net = KernelVar(embd_dim, hidden_dim, kernel_dim)\n",
    "kernel_net.s_ix, kernel_net.e_ix = s_ix, e_ix\n",
    "\n",
    "sampler = MarginalSampler()\n",
    "pred_net = PredNet(embd_dim, hidden_dim, enc_dim, target_dim)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "activation = nn.Sigmoid()\n",
    "\n",
    "pred = None\n",
    "\n",
    "pred_loss = None \n",
    "reg_loss = None\n",
    "loss = None\n",
    "\n",
    "reg = 10\n",
    "reg_mean = 0.1\n",
    "\n",
    "kernel, words = kernel_net(embd) # returned words are masked now!\n",
    "\n",
    "sampler.s_ix = kernel_net.s_ix\n",
    "sampler.e_ix = kernel_net.e_ix\n",
    "\n",
    "weighted_words = sampler(kernel, words) \n",
    "\n",
    "pred_net.s_ix = sampler.s_ix\n",
    "pred_net.e_ix = sampler.e_ix\n",
    "\n",
    "pred = pred_net(weighted_words)\n",
    "\n",
    "target = batch[1]\n",
    "\n",
    "if activation:\n",
    "    pred = activation(pred)\n",
    "\n",
    "pred_loss = criterion(pred, target)\n",
    "\n",
    "if reg:\n",
    "    reg_loss = reg * (torch.stack(sampler.exp_sizes) - reg_mean).pow(2).mean()\n",
    "    loss = pred_loss + reg_loss\n",
    "else:\n",
    "    loss = pred_loss\n",
    "\n",
    "loss.backward()\n",
    "duration = default_timer() - start\n",
    "print(duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cat received an invalid combination of arguments - got (tuple, int), but expected one of:\n * (sequence[torch.DoubleTensor] seq)\n * (sequence[torch.DoubleTensor] seq, int dim)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mtuple\u001b[0m, \u001b[32;1mint\u001b[0m)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-d98e5ffca5ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mdl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmy_collate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Coding/anaconda2/envs/torch2/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-56-d98e5ffca5ab>\u001b[0m in \u001b[0;36mmy_collate\u001b[0;34m(batch, vocab)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mrep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturnEmbds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'review'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mrep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_no_chunks\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mrep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mreps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Coding/anaconda2/envs/torch2/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mcat\u001b[0;34m(iterable, dim)\u001b[0m\n\u001b[1;32m    895\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 897\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mConcat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Coding/anaconda2/envs/torch2/lib/python3.6/site-packages/torch/autograd/_functions/tensor.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, dim, *inputs)\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_sizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cat received an invalid combination of arguments - got (tuple, int), but expected one of:\n * (sequence[torch.DoubleTensor] seq)\n * (sequence[torch.DoubleTensor] seq, int dim)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mtuple\u001b[0m, \u001b[32;1mint\u001b[0m)\n"
     ]
    }
   ],
   "source": [
    "def my_collate(batch, vocab=vocab):\n",
    "\n",
    "    # Count sizes\n",
    "    max_no_chunks = 0\n",
    "    for d in batch:\n",
    "        max_no_chunks = max(max_no_chunks, len(vocab.filterReview(d['review'])))\n",
    "    \n",
    "    # Map to Embeddings\n",
    "    reps = []\n",
    "    for d in batch:\n",
    "        rep = vocab.returnEmbds(d['review'])\n",
    "        rep = torch.cat([rep, Variable(torch.zeros(max_no_chunks + 1 - rep.size(0), rep.size(1)))], dim=0)\n",
    "        reps.append(rep)\n",
    "    \n",
    "    data_tensor = torch.stack(reps) \n",
    "    \n",
    "    # Create target vector\n",
    "    # target_tensor = Variable(torch.stack([d['target'] for d in batch]))\n",
    "    target_tensor = Variable(torch.stack([d['target'] for d in batch]))\n",
    "    \n",
    "    return data_tensor, target_tensor\n",
    "\n",
    "# Solution 1 using my_collate\n",
    "from timeit import default_timer\n",
    "from dpp_nets.layers.layers import KernelVar, MarginalSampler, PredNet\n",
    "\n",
    "\n",
    "start = default_timer()\n",
    "\n",
    "dl = DataLoader(ds, batch_size=500, collate_fn=my_collate)\n",
    "for batch in dl:\n",
    "    break\n",
    "words = batch[0]\n",
    "\n",
    "kernel_net = KernelVar(200,500,200)\n",
    "\n",
    "embd_dim = 200\n",
    "hidden_dim = 500\n",
    "kernel_dim = 200\n",
    "enc_dim = 200\n",
    "target_dim = 3\n",
    "\n",
    "kernel_net = KernelVar(embd_dim, hidden_dim, kernel_dim)\n",
    "sampler = MarginalSampler()\n",
    "pred_net = PredNet(embd_dim, hidden_dim, enc_dim, target_dim)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "activation = nn.Sigmoid()\n",
    "\n",
    "pred = None\n",
    "\n",
    "pred_loss = None \n",
    "reg_loss = None\n",
    "loss = None\n",
    "\n",
    "reg = 10\n",
    "reg_mean = 0.1\n",
    "\n",
    "kernel, words = kernel_net(words) # returned words are masked now!\n",
    "\n",
    "sampler.s_ix = kernel_net.s_ix\n",
    "sampler.e_ix = kernel_net.e_ix\n",
    "\n",
    "weighted_words = sampler(kernel, words) \n",
    "\n",
    "pred_net.s_ix = sampler.s_ix\n",
    "pred_net.e_ix = sampler.e_ix\n",
    "\n",
    "pred = pred_net(weighted_words)\n",
    "\n",
    "target = batch[1]\n",
    "\n",
    "if activation:\n",
    "    pred = activation(pred)\n",
    "\n",
    "pred_loss = criterion(pred, target)\n",
    "\n",
    "if reg:\n",
    "    reg_loss = reg * (torch.stack(sampler.exp_sizes) - reg_mean).pow(2).mean()\n",
    "    loss = pred_loss + reg_loss\n",
    "else:\n",
    "    loss = pred_loss\n",
    "\n",
    "\n",
    "loss.backward()\n",
    "duration = default_timer() - start\n",
    "print(duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.utils.backcompat.broadcast_warning.enabled = True\n",
    "torch.utils.backcompat.keepdim_warning.enabled = True\n",
    "words = Variable(torch.FloatTensor([[[1,2,3,4],[3,4,5,6],[0,0,0,0]],[[1,2,3,4],[0,0,0,0],[0,0,0,0]]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for batch in dl:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x12bee79e8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.EmbeddingBag.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "1.00000e-02 *\n",
       " -2.3152\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.EmbeddingBag.weight[3,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EmbeddingBag(112232, 200, mode=mean)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.EmbeddingBag.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "my_collate2() missing 1 required positional argument: 'batch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-7a2948b49dad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmy_collate2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: my_collate2() missing 1 required positional argument: 'batch'"
     ]
    }
   ],
   "source": [
    "my_collate2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A = torch.randn(5,5)\n",
    "L = A.mm(A.t())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "A = torch.randn(5,5)\n",
    "L = A.mm(A.t())\n",
    "\n",
    "A = A.numpy()\n",
    "L = L.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = L.shape[0]\n",
    "no_choice = list(range(n))\n",
    "choice = []\n",
    "\n",
    "# update L\n",
    "identity = np.zeros(n)\n",
    "identity[no_choice] = np.ones(len(no_choice))\n",
    "inverse = np.linalg.inv(L + identity)\n",
    "inverse_select = inverse[np.ix_(no_choice, no_choice)]\n",
    "LA = np.linalg.inverse(inverse_select) - np.identity(len(no_choice))\n",
    "\n",
    "\n",
    "K = L.dot(np.linalg.inv(L+np.eye(n)))\n",
    "K_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeMAP(L):\n",
    "\n",
    "    # initialization\n",
    "    n = L.shape[0]\n",
    "    no_choice = list(range(n))\n",
    "    choice = []\n",
    "    best_p = 0\n",
    "\n",
    "    while True:\n",
    "\n",
    "        candidates = [choice + [j] for j in no_choice]\n",
    "        submats = [L[np.ix_(cand, cand)] for cand in candidates]\n",
    "        probs = [np.linalg.det(submat) - best_p for submat in submats]\n",
    "\n",
    "        if all(p <= 0 for p in probs):\n",
    "            return choice\n",
    "        else:\n",
    "            which = np.argmax(np.array(probs))\n",
    "            choice = candidates[which]\n",
    "            which_elem = choice[-1]\n",
    "            no_choice.remove(which_elem)\n",
    "            best_p += probs[which]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import chain, combinations\n",
    "\n",
    "def exactMAP(L):\n",
    "\n",
    "    n = L.shape[0]\n",
    "    \n",
    "    # Generate powerset\n",
    "    s = list(range(n))\n",
    "    powerset = list(chain.from_iterable(combinations(s, r) for r in range(len(s)+1)))\n",
    "    \n",
    "    # Compute Probabilities \n",
    "    probs = np.array([np.linalg.det(L[np.ix_(choice, choice)]) for choice in powerset])\n",
    "    which = np.argmax(probs)\n",
    "    MAP = powerset[which], probs[which]\n",
    "    \n",
    "    return MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "choice = computeMAP(L)\n",
    "print(choice)\n",
    "print(len(choice))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from itertools import chain, combinations\n",
    "def powerset(iterable):\n",
    "    \"powerset([1,2,3]) --> () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)\"\n",
    "    s = list(iterable)\n",
    "    return chain.from_iterable(combinations(s, r) for r in range(len(s)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[(choice, np.linalg.det(L[np.ix_(choice, choice)])) for choice in list(powerset(range(6)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Variable containing:\n",
       " ( 0 ,.,.) = \n",
       "   0.0382 -0.1186 -0.0850  ...  -0.0401 -0.1082 -0.0871\n",
       "  -0.1019 -0.0845 -0.0379  ...  -0.0387 -0.0539  0.0405\n",
       "  -0.0130  0.0048  0.0105  ...  -0.0123 -0.0478 -0.0229\n",
       "            ...             ⋱             ...          \n",
       "   0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       "   0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       "   0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       " \n",
       " ( 1 ,.,.) = \n",
       "  -0.0259  0.0692  0.0024  ...  -0.0073 -0.0043 -0.0124\n",
       "  -0.0828 -0.0015 -0.0338  ...   0.0101  0.0463 -0.0250\n",
       "   0.0057 -0.0006 -0.0263  ...  -0.0249 -0.0389 -0.0218\n",
       "            ...             ⋱             ...          \n",
       "   0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       "   0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       "   0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       " \n",
       " ( 2 ,.,.) = \n",
       "   0.0195 -0.0219 -0.0247  ...  -0.0158 -0.0756 -0.0718\n",
       "  -0.1637  0.0951 -0.0571  ...  -0.0486 -0.0456 -0.0826\n",
       "   0.0421 -0.0581 -0.0280  ...   0.0181 -0.1355 -0.1369\n",
       "            ...             ⋱             ...          \n",
       "   0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       "   0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       "   0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       " ... \n",
       " \n",
       " (497,.,.) = \n",
       "   0.0108  0.0406  0.0744  ...   0.0114 -0.0296 -0.0225\n",
       "  -0.0145  0.0345  0.1035  ...   0.0576 -0.0295 -0.0046\n",
       "   0.0439 -0.0228  0.1279  ...  -0.0327 -0.0286  0.0267\n",
       "            ...             ⋱             ...          \n",
       "   0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       "   0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       "   0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       " \n",
       " (498,.,.) = \n",
       "  -0.0209  0.0224  0.0771  ...   0.0186 -0.0303 -0.0095\n",
       "  -0.0032 -0.0297  0.0874  ...   0.1341 -0.0426  0.0885\n",
       "   0.0511  0.1347  0.0643  ...   0.0043 -0.0220 -0.0243\n",
       "            ...             ⋱             ...          \n",
       "   0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       "   0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       "   0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       " \n",
       " (499,.,.) = \n",
       "  -0.0774  0.0184  0.1064  ...   0.0228 -0.0134 -0.0485\n",
       "  -0.0084  0.0071 -0.0297  ...  -0.0925 -0.0551  0.0825\n",
       "  -0.0384  0.0011  0.0173  ...  -0.0163 -0.0419  0.0363\n",
       "            ...             ⋱             ...          \n",
       "   0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       "   0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       "   0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       " [torch.FloatTensor of size 500x297x200], Variable containing:\n",
       "  0.8000  1.0000  0.8000\n",
       "  0.5000  0.2000  0.4000\n",
       "  1.0000  0.6000  0.9000\n",
       "            ⋮            \n",
       "  0.6000  0.5000  0.7000\n",
       "  0.8000  0.9000  0.8000\n",
       "  0.7000  0.5000  0.7000\n",
       " [torch.FloatTensor of size 500x3])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Max/Coding/anaconda2/envs/torch2/lib/python3.6/site-packages/torch/autograd/_functions/reduce.py:21: UserWarning: backwards compatibility: call to \"sum\" uses default value for keepdim which has changed default to False.  Consider passing as kwarg.\n",
      "  return input.sum(dim)\n",
      "/Users/Max/Coding/anaconda2/envs/torch2/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: backwards compatibility: call to \"sum\" uses default value for keepdim which has changed default to False.  Consider passing as kwarg.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-2.3880e-02 -1.4313e-02  1.9481e-02  ...   1.3393e-02 -1.9901e-02 -4.3429e-02\n",
       "-1.8973e-02  1.8574e-02  4.9568e-02  ...   8.4320e-03 -1.8898e-02 -5.9302e-02\n",
       "-1.0988e-02 -1.6745e-02  1.5342e-02  ...   2.7094e-02 -2.7952e-02 -4.0182e-02\n",
       "                ...                   ⋱                   ...                \n",
       "-2.8851e-02  1.2475e-03  2.1513e-02  ...   1.9931e-02 -1.2227e-02 -2.9908e-02\n",
       "-4.4926e-02  1.3410e-02  2.7187e-02  ...   2.0730e-02 -1.7321e-02 -4.2113e-02\n",
       "-5.9178e-02 -2.5057e-02  4.0354e-02  ...   1.4352e-02 -1.9933e-02 -2.8511e-02\n",
       "[torch.FloatTensor of size 500x200]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words, target = batch\n",
    "batch_size, max_set_size, embd_dim = words.size()\n",
    "word_sums = words.sum(1) \n",
    "lengths = Variable(words.data.sum(2, keepdim=True).abs().sign().sum(1).expand_as(word_sums))\n",
    "word_means = word_sums / lengths\n",
    "word_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Variable containing:\n",
       " ( 0 ,.,.) = \n",
       "   0.0382 -0.1186 -0.0850  ...  -0.0401 -0.1082 -0.0871\n",
       "  -0.1019 -0.0845 -0.0379  ...  -0.0387 -0.0539  0.0405\n",
       "  -0.0130  0.0048  0.0105  ...  -0.0123 -0.0478 -0.0229\n",
       "            ...             ⋱             ...          \n",
       "   0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       "   0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       "   0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       " \n",
       " ( 1 ,.,.) = \n",
       "  -0.0259  0.0692  0.0024  ...  -0.0073 -0.0043 -0.0124\n",
       "  -0.0828 -0.0015 -0.0338  ...   0.0101  0.0463 -0.0250\n",
       "   0.0057 -0.0006 -0.0263  ...  -0.0249 -0.0389 -0.0218\n",
       "            ...             ⋱             ...          \n",
       "   0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       "   0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       "   0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       " \n",
       " ( 2 ,.,.) = \n",
       "   0.0195 -0.0219 -0.0247  ...  -0.0158 -0.0756 -0.0718\n",
       "  -0.1637  0.0951 -0.0571  ...  -0.0486 -0.0456 -0.0826\n",
       "   0.0421 -0.0581 -0.0280  ...   0.0181 -0.1355 -0.1369\n",
       "            ...             ⋱             ...          \n",
       "   0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       "   0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       "   0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       " ... \n",
       " \n",
       " (497,.,.) = \n",
       "   0.0108  0.0406  0.0744  ...   0.0114 -0.0296 -0.0225\n",
       "  -0.0145  0.0345  0.1035  ...   0.0576 -0.0295 -0.0046\n",
       "   0.0439 -0.0228  0.1279  ...  -0.0327 -0.0286  0.0267\n",
       "            ...             ⋱             ...          \n",
       "   0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       "   0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       "   0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       " \n",
       " (498,.,.) = \n",
       "  -0.0209  0.0224  0.0771  ...   0.0186 -0.0303 -0.0095\n",
       "  -0.0032 -0.0297  0.0874  ...   0.1341 -0.0426  0.0885\n",
       "   0.0511  0.1347  0.0643  ...   0.0043 -0.0220 -0.0243\n",
       "            ...             ⋱             ...          \n",
       "   0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       "   0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       "   0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       " \n",
       " (499,.,.) = \n",
       "  -0.0774  0.0184  0.1064  ...   0.0228 -0.0134 -0.0485\n",
       "  -0.0084  0.0071 -0.0297  ...  -0.0925 -0.0551  0.0825\n",
       "  -0.0384  0.0011  0.0173  ...  -0.0163 -0.0419  0.0363\n",
       "            ...             ⋱             ...          \n",
       "   0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       "   0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       "   0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       " [torch.FloatTensor of size 500x297x200], Variable containing:\n",
       "  0.8000  1.0000  0.8000\n",
       "  0.5000  0.2000  0.4000\n",
       "  1.0000  0.6000  0.9000\n",
       "            ⋮            \n",
       "  0.6000  0.5000  0.7000\n",
       "  0.8000  0.9000  0.8000\n",
       "  0.7000  0.5000  0.7000\n",
       " [torch.FloatTensor of size 500x3])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-4d31b308f4c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Send through encoder network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0menc_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menc_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Compilation of encoded words for each instance in sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "batch_size, max_set_size, embd_dim = words.size()\n",
    "\n",
    "# Unpacking to send through encoder network\n",
    "# Register indices of individual instances in batch for reconstruction\n",
    "lengths = words.data.sum(2, keepdim=True).abs().sign().sum(1, keepdim=True)\n",
    "s_ix = list(lengths.squeeze().cumsum(0).long() - lengths.squeeze().long())\n",
    "e_ix = list(lengths.squeeze().cumsum(0).long())\n",
    "\n",
    "# Filter out zero words \n",
    "mask = words.data.sum(2, keepdim=True).abs().sign().expand_as(words).byte()\n",
    "words = words.masked_select(Variable(mask)).view(-1, embd_dim)\n",
    "\n",
    "# Send through encoder network\n",
    "enc_words = self.enc_net(words)\n",
    "\n",
    "# Compilation of encoded words for each instance in sample\n",
    "# Produce summed representation (code) for each instance in batch using encoded words:\n",
    "codes = []\n",
    "\n",
    "for i, (s, e) in enumerate(zip(s_ix, e_ix)):\n",
    "    code = enc_words[s:e].mean(0, keepdim=True)\n",
    "    codes.append(code)\n",
    "\n",
    "codes = torch.stack(codes).squeeze(1)\n",
    "assert batch_size == codes.size(0)\n",
    "assert enc_dim == codes.size(1)\n",
    "\n",
    "# Produce predictions using codes\n",
    "pred = self.pred_net(codes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_x Variable containing:\n",
      " 1.0000  2.0000  3.0000  1.5000  2.0000  2.5000\n",
      " 2.0000  2.0000  2.0000  1.5000  2.0000  2.5000\n",
      " 1.0000  2.0000  3.0000  1.0000  2.0000  3.0000\n",
      "[torch.FloatTensor of size 3x6]\n",
      "\n",
      "attention_unnorm Variable containing:\n",
      " 0.2140\n",
      " 0.3386\n",
      " 0.0283\n",
      "[torch.FloatTensor of size 3x1]\n",
      "\n",
      "weighted_words Variable containing:\n",
      " 1.5311  2.0000  2.4689\n",
      " 1.0000  2.0000  3.0000\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Max/Coding/anaconda2/envs/torch2/lib/python3.6/site-packages/torch/autograd/_functions/reduce.py:21: UserWarning: backwards compatibility: call to \"sum\" uses default value for keepdim which has changed default to False.  Consider passing as kwarg.\n",
      "  return input.sum(dim)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.1759  0.0761\n",
       " 0.1767  0.0303\n",
       "[torch.FloatTensor of size 2x2]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class AttentionBaseline(nn.Module):\n",
    "    \"\"\"\n",
    "    Works with different set sizes, i.e. it does masking!\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embd_dim, hidden_dim, target_dim):\n",
    "\n",
    "        super(AttentionBaseline, self).__init__()\n",
    "\n",
    "        self.embd_dim = embd_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.target_dim = target_dim\n",
    "\n",
    "        # Attention Network \n",
    "        self.attention_layer = nn.Sequential(nn.Linear(2 * embd_dim, hidden_dim), nn.Tanh())\n",
    "        self.v = nn.Parameter(torch.randn(hidden_dim, 1))\n",
    "\n",
    "        # Uses the sum of the encoded vectors to make a final prediction\n",
    "        self.pred_layer1 = nn.Linear(embd_dim ,hidden_dim)\n",
    "        self.pred_layer2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.pred_layer3 = nn.Linear(hidden_dim, target_dim)\n",
    "        self.pred_net = nn.Sequential(self.pred_layer1, nn.ReLU(), self.pred_layer2, nn.ReLU(), self.pred_layer3)\n",
    "\n",
    "        self.s_ix = []\n",
    "        self.e_ix = []\n",
    "        \n",
    "        self.attention_unnorm = None\n",
    "        self.attention = None\n",
    "        self.words = None\n",
    "        self.weighted_words = None\n",
    "\n",
    "    def forward(self, words):\n",
    "        \"\"\"\n",
    "        words is a 3D tensor with dimension: batch_size x max_set_size x embd_dim\n",
    "\n",
    "        \"\"\"\n",
    "        embd_dim = self.embd_dim\n",
    "        hidden_dim = self.hidden_dim\n",
    "        target_dim = self.target_dim\n",
    "\n",
    "        batch_size, max_set_size, embd_dim = words.size()\n",
    "\n",
    "        # Create context\n",
    "        lengths = words.sum(2, keepdim=True).abs().sign().sum(1, keepdim=True)\n",
    "        context = (words.sum(1, keepdim=True) / lengths.expand_as(words.sum(1, keepdim=True))).expand_as(words)\n",
    "\n",
    "        # Filter out zero words \n",
    "        mask = words.data.sum(2, keepdim=True).abs().sign().expand_as(words).byte()\n",
    "        self.words = words.masked_select(Variable(mask)).view(-1, embd_dim)\n",
    "        context = context.masked_select(Variable(mask)).view(-1, embd_dim)\n",
    "\n",
    "        # Concatenate and compute attention\n",
    "        batch_x = torch.cat([self.words, context], dim=1)\n",
    "        print('batch_x', batch_x)\n",
    "        self.attention_unnorm = self.attention_layer(batch_x).mm(self.v)\n",
    "        print('attention_unnorm', self.attention_unnorm)\n",
    "\n",
    "        self.s_ix = list(lengths.squeeze().cumsum(0).long().data - lengths.squeeze().long().data)\n",
    "        self.e_ix = list(lengths.squeeze().cumsum(0).long().data)\n",
    "\n",
    "        # Apply attention\n",
    "        reps = []\n",
    "        for i, (s, e) in enumerate(zip(self.s_ix, self.e_ix)):\n",
    "            self.attention = (nn.Softmax()(self.attention_unnorm[s:e].t())).t()\n",
    "            rep = (self.attention * self.words[s:e]).sum(0)\n",
    "            reps.append(rep)\n",
    "            \n",
    "        self.weighted_words = torch.stack(reps)\n",
    "        \n",
    "        assert self.weighted_words.size(0) == batch_size\n",
    "        print('weighted_words', self.weighted_words)\n",
    "\n",
    "        pred = self.pred_net(self.weighted_words)\n",
    "\n",
    "        return pred \n",
    "torch.manual_seed(0)\n",
    "words = Variable(torch.FloatTensor([[[1,2,3],[2,2,2],[0,0,0],[0,0,0]],[[1,2,3],[0,0,0],[0,0,0],[0,0,0]]]))\n",
    "net = AttentionBaseline(3, 10, 2)\n",
    "net(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0.4689  0.4689  0.4689\n",
      " 0.5311  0.5311  0.5311\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n",
      "Variable containing:\n",
      " 1  1  1\n",
      "[torch.FloatTensor of size 1x3]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Max/Coding/anaconda2/envs/torch2/lib/python3.6/site-packages/torch/autograd/_functions/reduce.py:21: UserWarning: backwards compatibility: call to \"sum\" uses default value for keepdim which has changed default to False.  Consider passing as kwarg.\n",
      "  return input.sum(dim)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 1.5311  2.0000  2.4689\n",
       " 1.0000  2.0000  3.0000\n",
       "[torch.FloatTensor of size 2x3]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reps = []\n",
    "for s, e in zip(net.s_ix, net.e_ix):\n",
    "    attention = (nn.Softmax()(net.attention_unnorm[s:e].t())).t().expand_as(net.words[s:e])\n",
    "    print(attention)\n",
    "    rep = (attention * net.words[s:e]).sum(0)\n",
    "    reps.append(rep)\n",
    "torch.stack(reps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 1\n",
       "[torch.FloatTensor of size 1x1]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Max/Coding/anaconda2/envs/torch2/lib/python3.6/site-packages/torch/autograd/_functions/reduce.py:21: UserWarning: backwards compatibility: call to \"sum\" uses default value for keepdim which has changed default to False.  Consider passing as kwarg.\n",
      "  return input.sum(dim)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.1759  0.0761\n",
       " 0.1767  0.0303\n",
       "[torch.FloatTensor of size 2x2]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class AttentionBaseline(nn.Module):\n",
    "    \"\"\"\n",
    "    Works with different set sizes, i.e. it does masking!\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embd_dim, hidden_dim, target_dim):\n",
    "\n",
    "        super(AttentionBaseline, self).__init__()\n",
    "\n",
    "        self.embd_dim = embd_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.target_dim = target_dim\n",
    "\n",
    "        # Attention Network \n",
    "        self.attention_layer = nn.Sequential(nn.Linear(2 * embd_dim, hidden_dim), nn.Tanh())\n",
    "        self.v = nn.Parameter(torch.randn(hidden_dim, 1))\n",
    "\n",
    "        # Uses the sum of the encoded vectors to make a final prediction\n",
    "        self.pred_layer1 = nn.Linear(embd_dim ,hidden_dim)\n",
    "        self.pred_layer2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.pred_layer3 = nn.Linear(hidden_dim, target_dim)\n",
    "        self.pred_net = nn.Sequential(self.pred_layer1, nn.ReLU(), self.pred_layer2, nn.ReLU(), self.pred_layer3)\n",
    "\n",
    "        self.s_ix = []\n",
    "        self.e_ix = []\n",
    "\n",
    "        self.attention = []\n",
    "\n",
    "    def forward(self, words):\n",
    "        \"\"\"\n",
    "        words is a 3D tensor with dimension: batch_size x max_set_size x embd_dim\n",
    "\n",
    "        \"\"\"\n",
    "        embd_dim = self.embd_dim\n",
    "        hidden_dim = self.hidden_dim\n",
    "        target_dim = self.target_dim\n",
    "\n",
    "        batch_size, max_set_size, embd_dim = words.size()\n",
    "\n",
    "        # Create context\n",
    "        lengths = words.sum(2, keepdim=True).abs().sign().sum(1, keepdim=True)\n",
    "        context = (words.sum(1, keepdim=True) / lengths.expand_as(words.sum(1, keepdim=True))).expand_as(words)\n",
    "\n",
    "        # Filter out zero words \n",
    "        mask = words.data.sum(2, keepdim=True).abs().sign().expand_as(words).byte()\n",
    "        words = words.masked_select(Variable(mask)).view(-1, embd_dim)\n",
    "        context = context.masked_select(Variable(mask)).view(-1, embd_dim)\n",
    "\n",
    "        # Concatenate and compute attention\n",
    "        batch_x = torch.cat([words, context], dim=1)\n",
    "        attention_unnorm = self.attention_layer(batch_x).mm(self.v)\n",
    "\n",
    "        self.s_ix = list(lengths.squeeze().cumsum(0).long().data - lengths.squeeze().long().data)\n",
    "        self.e_ix = list(lengths.squeeze().cumsum(0).long().data)\n",
    "\n",
    "        # Apply attention\n",
    "        reps = []\n",
    "        for i, (s, e) in enumerate(zip(self.s_ix, self.e_ix)):\n",
    "            attention = (nn.Softmax()(attention_unnorm[s:e].t())).t()\n",
    "            self.attention.append(attention.data)\n",
    "            rep = (attention * words[s:e]).sum(0)\n",
    "            reps.append(rep)\n",
    "\n",
    "        weighted_words = torch.stack(reps)\n",
    "        assert weighted_words.size(0) == batch_size\n",
    "\n",
    "\n",
    "        pred = self.pred_net(weighted_words)\n",
    "\n",
    "        return pred \n",
    "torch.manual_seed(0)\n",
    "words = Variable(torch.FloatTensor([[[1,2,3],[2,2,2],[0,0,0],[0,0,0]],[[1,2,3],[0,0,0],[0,0,0],[0,0,0]]]))\n",
    "net = AttentionBaseline(3, 10, 2)\n",
    "net(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# trained models\n",
    "root_path = '/Users/Max/checkpoints/beer_reviews/'\n",
    "model = 'allchunksreg0.01reg_mean10.0lr0.001marginal_best_ckp.pth.tar'\n",
    "name = root_path + model\n",
    "my_d = torch.load(name, map_location=lambda storage, loc: storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['epoch:', 'model', 'state_dict', 'lowest_loss', 'optimizer']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(my_d.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dpp_nets.utils.language import Vocabulary, BeerDataset, custom_collate\n",
    "from dpp_nets.layers.layers import ChunkTrainer\n",
    "\n",
    "train_path = '/Users/Max/data/beer_reviews/' + 'reviews.' + 'all' + '.train.' + 'chunks' + '.txt.gz'\n",
    "val_path = '/Users/Max/data/beer_reviews/' + 'reviews.' + 'all' + '.heldout.' + 'chunks' + '.txt.gz'\n",
    "embd_path = '/Users/Max/data/beer_reviews/' + 'review+wiki.filtered.200.txt.gz'\n",
    "word_path = '/Users/Max/data/beer_reviews/' + 'reviews.' + 'all' + '.train.' + 'words.txt.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = Vocabulary()\n",
    "vocab.loadPretrained(embd_path)\n",
    "vocab.setStops()\n",
    "vocab.loadCorpus(word_path)\n",
    "vocab.updateEmbedding()\n",
    "vocab.setCuda(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainer = ChunkTrainer(200, 500, 200, 200, 3)\n",
    "trainer.activation = nn.Sigmoid()\n",
    "trainer.reg = 0.1\n",
    "trainer.reg_mean = 10\n",
    "trainer.load_state_dict(my_d['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_collate = custom_collate(vocab, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_set = BeerDataset(val_path)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, collate_fn=my_collate, batch_size=10)\n",
    "train_set = BeerDataset(train_path)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, collate_fn=my_collate, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validate(loader, trainer):\n",
    "\n",
    "    trainer.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_pred_loss = 0.0\n",
    "    total_reg_loss = 0.0\n",
    "\n",
    "    for i, batch in enumerate(loader, 1):\n",
    "\n",
    "        review, target = batch\n",
    "\n",
    "        trainer(review, target)\n",
    "\n",
    "        loss = trainer.loss.data[0]\n",
    "        pred_loss = trainer.pred_loss.data[0]\n",
    "        reg_loss = trainer.reg_loss.data[0]\n",
    "\n",
    "        delta = loss - total_loss\n",
    "        total_loss += (delta / i)\n",
    "        delta = pred_loss - total_pred_loss \n",
    "        total_pred_loss += (delta / i)\n",
    "        delta = reg_loss - total_reg_loss\n",
    "        total_reg_loss += (delta / i)\n",
    "\n",
    "        # print(\"validated one batch\")\n",
    "\n",
    "    return total_loss, total_pred_loss, total_reg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.282385425224899, 0.029764174521279788, 1.2526212519432145)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate(val_loader, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Variable containing:\n",
       " -1.9418 -1.6623  1.7433  ...   0.8431  2.7226  0.3328\n",
       " -2.4954 -1.5230  4.1865  ...   0.5212  2.6975  0.1213\n",
       " -0.8185  1.6773 -2.3425  ...  -1.6988  3.2755  3.0228\n",
       "           ...             ⋱             ...          \n",
       " -1.1800 -2.2931  3.2514  ...   0.5663  2.9770 -0.8179\n",
       " -1.0904  0.0158 -0.0062  ...  -0.1544  5.2335  2.5472\n",
       " -2.0173 -2.8696  3.4641  ...  -0.1246  4.9017  0.8438\n",
       " [torch.FloatTensor of size 14x200], Variable containing:\n",
       "  4.1406e-02 -2.2289e-03  3.9951e-02  ...   1.3364e-04 -5.6739e-02 -5.9640e-02\n",
       "  3.2634e-03  2.3399e-02 -9.9318e-03  ...  -2.2967e-02 -6.6968e-02  1.9021e-02\n",
       " -2.2690e-02  9.4700e-04  2.5908e-02  ...   2.0340e-02 -1.7681e-02 -3.6136e-02\n",
       "                 ...                   ⋱                   ...                \n",
       " -2.1147e-03 -5.0989e-02 -2.1682e-02  ...  -1.7501e-02 -5.7269e-02 -2.1876e-02\n",
       "  1.0210e-02 -1.3399e-02  1.0572e-02  ...   4.6790e-03 -5.3796e-02 -5.2150e-02\n",
       " -1.9828e-02 -9.2585e-02 -1.4802e-02  ...  -5.5002e-06 -2.9345e-02 -5.4205e-02\n",
       " [torch.FloatTensor of size 14x200])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.kernel_net(batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "( 0 ,.,.) = \n",
       "  0.0414 -0.0022  0.0400  ...   0.0001 -0.0567 -0.0596\n",
       "  0.0033  0.0234 -0.0099  ...  -0.0230 -0.0670  0.0190\n",
       " -0.0227  0.0009  0.0259  ...   0.0203 -0.0177 -0.0361\n",
       "           ...             ⋱             ...          \n",
       "  0.0102 -0.0134  0.0106  ...   0.0047 -0.0538 -0.0521\n",
       " -0.0198 -0.0926 -0.0148  ...  -0.0000 -0.0293 -0.0542\n",
       "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       "[torch.FloatTensor of size 1x15x200]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-62bd740f6949>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpred_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms_ix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms_ix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpred_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me_ix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me_ix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kernel, words = trainer.kernel_net(batch[0])\n",
    "trainer.sampler.s_ix = trainer.kernel_net.s_ix\n",
    "trainer.sampler.e_ix = trainer.kernel_net.e_ix\n",
    "weighted_words = trainer.sampler(kernel, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.6517\n",
       " 0.2896\n",
       " 0.4587\n",
       " 0.5311\n",
       " 0.4152\n",
       " 0.5253\n",
       " 0.3540\n",
       " 0.4208\n",
       " 0.5471\n",
       " 0.5934\n",
       " 0.4787\n",
       " 0.8125\n",
       " 0.2178\n",
       " 0.4362\n",
       " 0.5165\n",
       " 0.7257\n",
       " 0.4486\n",
       " 0.4068\n",
       " 0.2543\n",
       " 0.4597\n",
       " 0.3139\n",
       " 0.9362\n",
       " 0.9884\n",
       " 0.9587\n",
       " 0.9937\n",
       " 0.9953\n",
       " 0.9290\n",
       " 0.9754\n",
       " 0.5675\n",
       " 0.1470\n",
       " 0.4389\n",
       " 0.3450\n",
       " 0.4050\n",
       " 0.2112\n",
       " 0.4281\n",
       " 0.4106\n",
       " 0.9373\n",
       " 0.5862\n",
       " 0.3574\n",
       " 0.2361\n",
       " 0.5830\n",
       " 0.2821\n",
       " 0.4981\n",
       " 0.2088\n",
       " 0.5895\n",
       " 0.3613\n",
       " 0.5043\n",
       " 0.3466\n",
       " 0.2690\n",
       " 0.2637\n",
       " 0.4807\n",
       " 0.1704\n",
       " 0.9723\n",
       " 0.9496\n",
       " 0.7807\n",
       " 0.9368\n",
       " 0.8544\n",
       " 0.3600\n",
       " 0.7408\n",
       " 0.6496\n",
       " 0.9412\n",
       " 0.7422\n",
       " 0.7238\n",
       " 0.6654\n",
       " 0.5697\n",
       " 0.8817\n",
       " 0.7893\n",
       " 0.8870\n",
       " 0.7256\n",
       " 0.8695\n",
       " 0.6865\n",
       " 0.9968\n",
       " 0.9765\n",
       " 0.9909\n",
       " 0.9930\n",
       " 0.9775\n",
       " 0.9729\n",
       " 0.9324\n",
       " 0.9735\n",
       " 0.9904\n",
       " 0.9939\n",
       " 0.9564\n",
       " 0.9524\n",
       " 0.9023\n",
       " 0.9391\n",
       " 0.9214\n",
       " 0.9384\n",
       " 0.9418\n",
       " 0.5597\n",
       " 0.8184\n",
       " 0.9087\n",
       " 0.9697\n",
       " 0.9596\n",
       " 0.7505\n",
       " 0.9877\n",
       " 0.9936\n",
       " 0.9862\n",
       " 0.9914\n",
       " 0.9850\n",
       " 0.9916\n",
       " 0.9930\n",
       "[torch.FloatTensor of size 101]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(weighted_words / words)[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.5391\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((words[2] * words[6]).sum() / (torch.sqrt((words[2]**2).sum()) * torch.sqrt((words[6]**2).sum())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for batch in train_loader: \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.0017\n",
       " 0.0280\n",
       " 0.0339\n",
       " 0.0081\n",
       " 0.0189\n",
       " 0.0482\n",
       " 0.0113\n",
       " 0.0130\n",
       " 0.0256\n",
       " 0.0174\n",
       " 0.0547\n",
       " 0.0059\n",
       " 0.0558\n",
       " 0.0288\n",
       " 0.0121\n",
       " 0.0304\n",
       " 0.0218\n",
       " 0.0111\n",
       " 0.0352\n",
       " 0.0508\n",
       " 0.0381\n",
       " 0.0025\n",
       " 0.0018\n",
       " 0.0236\n",
       " 0.0111\n",
       " 0.0416\n",
       " 0.0189\n",
       " 0.0027\n",
       " 0.0460\n",
       " 0.0260\n",
       " 0.0209\n",
       " 0.0513\n",
       " 0.0213\n",
       " 0.0217\n",
       " 0.0007\n",
       " 0.0588\n",
       " 0.0466\n",
       " 0.0174\n",
       " 0.0123\n",
       " 0.0366\n",
       " 0.0300\n",
       " 0.0045\n",
       " 0.0324\n",
       " 0.0077\n",
       " 0.0339\n",
       " 0.0058\n",
       " 0.0380\n",
       " 0.0437\n",
       " 0.0330\n",
       " 0.0209\n",
       " 0.0135\n",
       " 0.0344\n",
       " 0.0564\n",
       " 0.0169\n",
       " 0.1091\n",
       " 0.0729\n",
       " 0.0191\n",
       " 0.0220\n",
       " 0.0248\n",
       " 0.0215\n",
       " 0.0379\n",
       " 0.0013\n",
       " 0.0058\n",
       " 0.0347\n",
       " 0.0047\n",
       " 0.0005\n",
       " 0.0274\n",
       " 0.0246\n",
       " 0.0165\n",
       " 0.0141\n",
       " 0.0107\n",
       " 0.0019\n",
       " 0.0202\n",
       " 0.0820\n",
       " 0.0742\n",
       " 0.0311\n",
       " 0.1113\n",
       " 0.0288\n",
       " 0.0007\n",
       " 0.0160\n",
       " 0.0025\n",
       " 0.0401\n",
       " 0.0538\n",
       " 0.0325\n",
       " 0.0025\n",
       " 0.0458\n",
       " 0.0371\n",
       " 0.0667\n",
       " 0.0090\n",
       " 0.0260\n",
       " 0.0019\n",
       " 0.0163\n",
       " 0.0038\n",
       " 0.0292\n",
       " 0.0227\n",
       " 0.0208\n",
       " 0.0627\n",
       " 0.0180\n",
       " 0.0156\n",
       " 0.0461\n",
       " 0.0085\n",
       " 0.0127\n",
       " 0.0106\n",
       " 0.0603\n",
       " 0.0396\n",
       " 0.0799\n",
       " 0.0123\n",
       " 0.0591\n",
       " 0.0127\n",
       " 0.0137\n",
       " 0.0208\n",
       " 0.0902\n",
       " 0.0504\n",
       " 0.0085\n",
       " 0.0462\n",
       " 0.0800\n",
       " 0.0221\n",
       " 0.0137\n",
       " 0.0465\n",
       " 0.0067\n",
       " 0.0318\n",
       " 0.0812\n",
       " 0.0096\n",
       " 0.0412\n",
       " 0.0167\n",
       " 0.0352\n",
       " 0.0052\n",
       " 0.0139\n",
       " 0.0318\n",
       " 0.0197\n",
       " 0.0097\n",
       " 0.0201\n",
       " 0.1010\n",
       " 0.0307\n",
       " 0.0120\n",
       " 0.0070\n",
       " 0.0101\n",
       " 0.0321\n",
       " 0.0336\n",
       " 0.0161\n",
       " 0.0040\n",
       " 0.0202\n",
       " 0.0474\n",
       " 0.0384\n",
       " 0.0055\n",
       " 0.0321\n",
       " 0.0514\n",
       " 0.0948\n",
       " 0.0243\n",
       " 0.0959\n",
       " 0.0495\n",
       " 0.0279\n",
       " 0.0075\n",
       " 0.0106\n",
       " 0.0353\n",
       " 0.0300\n",
       " 0.0153\n",
       " 0.0195\n",
       " 0.0327\n",
       " 0.0519\n",
       " 0.0340\n",
       " 0.0081\n",
       " 0.0208\n",
       " 0.0401\n",
       " 0.0188\n",
       " 0.0357\n",
       " 0.0612\n",
       " 0.0225\n",
       " 0.0409\n",
       " 0.0270\n",
       " 0.0092\n",
       " 0.0301\n",
       " 0.0162\n",
       " 0.0511\n",
       " 0.0252\n",
       " 0.0461\n",
       " 0.0313\n",
       " 0.0120\n",
       " 0.0261\n",
       " 0.0303\n",
       " 0.0050\n",
       " 0.0043\n",
       " 0.0805\n",
       " 0.0418\n",
       " 0.0208\n",
       " 0.0544\n",
       " 0.0058\n",
       " 0.0145\n",
       " 0.0017\n",
       " 0.0026\n",
       " 0.0161\n",
       " 0.0393\n",
       " 0.0251\n",
       " 0.0145\n",
       " 0.0052\n",
       " 0.0324\n",
       " 0.0067\n",
       " 0.0240\n",
       " 0.0207\n",
       " 0.0311\n",
       "[torch.FloatTensor of size 200]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sqrt(words[2]**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e81bfcdbc38b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "np.argsort(train_set[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.argsort(np.array([0,1,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
