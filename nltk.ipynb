{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "from dpp_nets.utils.io import make_embd, make_tensor_dataset\n",
    "from dpp_nets.my_torch.utilities import pad_tensor\n",
    "\n",
    "from dpp_nets.layers.layers import DeepSetBaseline\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "import string\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "import torch\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "\n",
    "import gzip\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = '/Users/Max/data/beer_reviews/reviews.all.train.chunks.txt.gz'\n",
    "word_path = '/Users/Max/data/beer_reviews/reviews.all.train.words.txt.gz'\n",
    "embd_path = '/Users/Max/data/beer_reviews/review+wiki.filtered.200.txt.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        # Basic Indexing\n",
    "        self.word2index = {}\n",
    "        self.index2word = {}\n",
    "        \n",
    "        # Keeping track of vocabulary\n",
    "        self.vocab_size = 0 \n",
    "        self.word2count = {}\n",
    "        \n",
    "        # Vector Dictionaries\n",
    "        self.pretrained = {}\n",
    "        self.random = {}\n",
    "        self.word2vec = {}\n",
    "        self.index2vec = {}\n",
    "\n",
    "        # Set of Stop Words\n",
    "        self.stop_words = set()\n",
    "        \n",
    "        self.Embedding = None\n",
    "        self.EmbeddingBag = None\n",
    "    \n",
    "    def setStops(self):\n",
    "        \n",
    "        self.stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "        make_stops = set(string.punctuation + '\\n' + '\\t' + '...')\n",
    "        unmake_stops = set(('no', 'not'))\n",
    "\n",
    "        self.stop_words = self.stop_words.union(make_stops)\n",
    "        self.stop_words = self.stop_words.difference(unmake_stops)      \n",
    "        \n",
    "    def loadPretrained(self, embd_path):\n",
    "        \n",
    "        self.pretrained = {}\n",
    "        with gzip.open(embd_path, 'rt') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    word, *embd = line.split()\n",
    "                    vec = torch.FloatTensor([float(dim) for dim in embd])            \n",
    "                    self.pretrained[word]  = vec\n",
    "                    \n",
    "    def loadCorpus(self, word_path):\n",
    "        \n",
    "        with gzip.open(data_path, 'rt') as f:\n",
    "\n",
    "            for line in f:\n",
    "                _, review = line.split('\\D')\n",
    "                review = tuple(tuple(chunk.split('\\W')) for chunk in review.split('\\T'))\n",
    "\n",
    "                for words in review:\n",
    "                    vocab.addWords(words)\n",
    "            \n",
    "    def addWords(self, words):\n",
    "        \"\"\"\n",
    "        words: seq containing variable no of words\n",
    "        \"\"\"\n",
    "        for word in words:\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "\n",
    "        if word not in self.word2index:\n",
    "            \n",
    "            # Keeping track of vocabulary\n",
    "            self.vocab_size += 1\n",
    "            self.word2count[word] = 1\n",
    "            \n",
    "            # Basic Indexing\n",
    "            self.word2index[word] = self.vocab_size\n",
    "            self.index2word[self.vocab_size] = word\n",
    "            \n",
    "            # Add word vector\n",
    "            if word in self.pretrained:\n",
    "                vec = self.pretrained[word]\n",
    "                self.word2vec[word] = vec\n",
    "                self.index2vec[self.vocab_size] = vec\n",
    "                \n",
    "            else:\n",
    "                vec = torch.randn(200)\n",
    "                self.random[word] = vec\n",
    "                self.word2vec[word] = vec\n",
    "                self.index2vec[self.vocab_size] = vec\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "            \n",
    "    def updateEmbedding(self):\n",
    "        \n",
    "        vocab_size = len(self.index2vec) + 1\n",
    "        EMBD_DIM = 200\n",
    "        \n",
    "        self.Embedding = nn.Embedding(vocab_size, EMBD_DIM, padding_idx=0)\n",
    "        self.EmbeddingBag = nn.EmbeddingBag(vocab_size, EMBD_DIM)\n",
    "        embd_matrix = torch.zeros(vocab_size, EMBD_DIM)\n",
    "        \n",
    "        for ix, vec in vocab.index2vec.items():\n",
    "            embd_matrix[ix] = vec\n",
    "        \n",
    "        embd_dict = OrderedDict([('weight', embd_matrix)])\n",
    "        self.Embedding.load_state_dict(embd_dict)\n",
    "        self.EmbeddingBag.load_state_dict(embd_dict)\n",
    "    \n",
    "    def checkWord(self, word, min_count):\n",
    "        if word not in vocab.stop_words and word in vocab.word2index and vocab.word2index[word] > min_count:\n",
    "            return word\n",
    "            \n",
    "    def filterReview(self, review):\n",
    "        \"\"\"\n",
    "        review should be like our data set\n",
    "        \"\"\"\n",
    "        f_review = []\n",
    "        seen = set()\n",
    "        \n",
    "        for tup in review:\n",
    "            f_tuple = []\n",
    "            \n",
    "            for word in tup:\n",
    "                word = self.checkWord(word, 10)\n",
    "                if word:\n",
    "                    f_tuple.append(word)\n",
    "            \n",
    "            f_tuple = tuple(f_tuple)    \n",
    "            \n",
    "            if f_tuple and f_tuple not in seen:\n",
    "                seen.add(f_tuple)\n",
    "                f_review.append(f_tuple)\n",
    "                \n",
    "        return f_review\n",
    "    \n",
    "    def mapIndicesBatch(self, reviews):\n",
    "        \n",
    "        f_review = []\n",
    "        offset = []\n",
    "        i = 0\n",
    "\n",
    "        for review in reviews:\n",
    "            seen = set()\n",
    "            \n",
    "            for tup in review: \n",
    "                f_tuple = []\n",
    "                \n",
    "                for word in tup:\n",
    "                    word = vocab.checkWord(word, 10)\n",
    "                    if word:\n",
    "                        f_tuple.append(word)\n",
    "\n",
    "                f_tuple = tuple(f_tuple)    \n",
    "\n",
    "                if f_tuple and f_tuple not in seen:\n",
    "                    seen.add(f_tuple)\n",
    "                    f_review.extend([vocab.word2index[word] for word in f_tuple])\n",
    "                    offset.append(i)\n",
    "                    i += len(f_tuple)\n",
    "            \n",
    "        f_review, offset = torch.LongTensor(f_review), torch.LongTensor(offset)   \n",
    "        return f_review, offset\n",
    "    \n",
    "    def mapIndices(self, review):\n",
    "        \n",
    "        f_review = []\n",
    "        offset = []\n",
    "        seen = set()\n",
    "        i = 0\n",
    "\n",
    "        for tup in review:\n",
    "            f_tuple = []\n",
    "\n",
    "            for word in tup:\n",
    "                word = vocab.checkWord(word, 10)\n",
    "                if word:\n",
    "                    f_tuple.append(word)\n",
    "\n",
    "            f_tuple = tuple(f_tuple)    \n",
    "\n",
    "            if f_tuple and f_tuple not in seen:\n",
    "                seen.add(f_tuple)\n",
    "                f_review.extend([vocab.word2index[word] for word in f_tuple])\n",
    "                offset.append(i)\n",
    "                i += len(f_tuple)\n",
    "\n",
    "        f_review, offset = torch.LongTensor(f_review), torch.LongTensor(offset)   \n",
    "        return f_review, offset\n",
    "    \n",
    "    def returnEmbds(self, review):\n",
    "        \n",
    "        f_review = []\n",
    "        offset = []\n",
    "        seen = set()\n",
    "        i = 0\n",
    "\n",
    "        for tup in review:\n",
    "            f_tuple = []\n",
    "\n",
    "            for word in tup:\n",
    "                word = vocab.checkWord(word, 10)\n",
    "                if word:\n",
    "                    f_tuple.append(word)\n",
    "\n",
    "            f_tuple = tuple(f_tuple)    \n",
    "\n",
    "            if f_tuple and f_tuple not in seen:\n",
    "                seen.add(f_tuple)\n",
    "                f_review.extend([vocab.word2index[word] for word in f_tuple])\n",
    "                offset.append(i)\n",
    "                i += len(f_tuple)\n",
    "\n",
    "        f_review, offset = Variable(torch.LongTensor(f_review)), Variable(torch.LongTensor(offset))\n",
    "        embd = self.EmbeddingBag(f_review, offset)\n",
    "\n",
    "        return embd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BeerDataset(Dataset):\n",
    "    \"\"\"BeerDataset.\"\"\"\n",
    "\n",
    "    def __init__(self, data_path, aspect='all'):\n",
    "        \n",
    "        # Compute size of the data set      \n",
    "        self.aspect = aspect\n",
    "        self.vocab = vocab\n",
    "        \n",
    "        with gzip.open(data_path, 'rt') as f:\n",
    "            self.lines = f.readlines()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lines)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        # Decode\n",
    "        target, review = self.lines[idx].split('\\D')\n",
    "        \n",
    "        # Target\n",
    "        target = torch.FloatTensor([float(t) for t in target.split()[:3]])\n",
    "        \n",
    "        # Review\n",
    "        review = tuple(tuple(chunk.split('\\W')) for chunk in review.split('\\T'))\n",
    "        #ixs, offset = self.vocab.mapIndices(review)\n",
    "        \n",
    "        #sample = {'ixs': ixs, 'offset': offset, 'target': target}\n",
    "        sample = {'review': review, 'target': target}\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = Vocabulary()\n",
    "vocab.loadPretrained(embd_path)\n",
    "vocab.setStops()\n",
    "vocab.loadCorpus(word_path)\n",
    "vocab.updateEmbedding()\n",
    "\n",
    "ds = BeerDataset(data_path, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abc/def'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 'abc'\n",
    "b = 'def'\n",
    "os.path.join(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_collate(batch):\n",
    "    \"Puts each data field into a tensor with outer dimension batch size\"\n",
    "    return {'review': [d['review']for d in batch], 'target': torch.stack([d['target'] for d in batch], 0)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64.01037289399937\n"
     ]
    }
   ],
   "source": [
    "# Solution 2 - using mycollate2 + new KernelNetwork\n",
    "from dpp_nets.layers.layers import MarginalSampler, PredNet\n",
    "\n",
    "def my_collate2(batch, vocab=vocab):\n",
    "\n",
    "    # Create indices\n",
    "    s_ix, e_ix, i = [], [], 0\n",
    "\n",
    "    for l in [len(vocab.filterReview(d['review'])) for d in batch]:\n",
    "        s_ix.append(i)\n",
    "        i += l\n",
    "        e_ix.append(i)\n",
    "    \n",
    "    # Map to Embeddings\n",
    "    batch_review = [review['review'] for review in batch]\n",
    "    ixs, offsets =  vocab.mapIndicesBatch(batch_review)\n",
    "    embd = vocab.EmbeddingBag(Variable(ixs), Variable(offsets))\n",
    "\n",
    "    # Create target vector\n",
    "    target_tensor = Variable(torch.stack([d['target'] for d in batch]))\n",
    "    \n",
    "    return embd, target_tensor, s_ix, e_ix\n",
    "\n",
    "class KernelVar(nn.Module):\n",
    "\n",
    "    def __init__(self, embd_dim, hidden_dim, kernel_dim):\n",
    "        \"\"\"\n",
    "        Currently, this creates a 2-hidden-layer network \n",
    "        with ELU non-linearities.\n",
    "\n",
    "        \"\"\"\n",
    "        super(KernelVar, self).__init__()\n",
    "        self.embd_dim = embd_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_dim = kernel_dim\n",
    "\n",
    "        self.layer1 = nn.Linear(2 * embd_dim, hidden_dim)\n",
    "        self.layer2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.layer3 = nn.Linear(hidden_dim, kernel_dim)\n",
    "\n",
    "        self.net = nn.Sequential(self.layer1, nn.Tanh(), self.layer2, nn.Tanh(), self.layer3)\n",
    "\n",
    "        self.s_ix = None\n",
    "        self.e_ix = None\n",
    "\n",
    "\n",
    "    def forward(self, embd):\n",
    "        \"\"\"\n",
    "        Given words, returns batch_kernel of dimension\n",
    "        [-1, kernel_dim]\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create context\n",
    "        context = []\n",
    "        for s, e in zip(self.s_ix, self.e_ix):\n",
    "            text = embd[s:e].sum(0, keepdim=True).expand_as(embd[s:e])\n",
    "            context.append(text)\n",
    "        context = torch.cat(context, dim=0)\n",
    "        batch_x = torch.cat([embd, context], dim=1)\n",
    "        \n",
    "        batch_kernel = self.net(batch_x)\n",
    "\n",
    "        return batch_kernel , embd \n",
    "\n",
    "from timeit import default_timer\n",
    "start = default_timer()\n",
    "\n",
    "dl = DataLoader(ds, batch_size=500, collate_fn=my_collate2)\n",
    "for batch in dl:\n",
    "    break\n",
    "\n",
    "embd, target, s_ix, e_ix = batch\n",
    "\n",
    "embd_dim = 200\n",
    "hidden_dim = 500\n",
    "kernel_dim = 200\n",
    "enc_dim = 200\n",
    "target_dim = 3\n",
    "\n",
    "kernel_net = KernelVar(embd_dim, hidden_dim, kernel_dim)\n",
    "kernel_net.s_ix, kernel_net.e_ix = s_ix, e_ix\n",
    "\n",
    "sampler = MarginalSampler()\n",
    "pred_net = PredNet(embd_dim, hidden_dim, enc_dim, target_dim)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "activation = nn.Sigmoid()\n",
    "\n",
    "pred = None\n",
    "\n",
    "pred_loss = None \n",
    "reg_loss = None\n",
    "loss = None\n",
    "\n",
    "reg = 10\n",
    "reg_mean = 0.1\n",
    "\n",
    "kernel, words = kernel_net(embd) # returned words are masked now!\n",
    "\n",
    "sampler.s_ix = kernel_net.s_ix\n",
    "sampler.e_ix = kernel_net.e_ix\n",
    "\n",
    "weighted_words = sampler(kernel, words) \n",
    "\n",
    "pred_net.s_ix = sampler.s_ix\n",
    "pred_net.e_ix = sampler.e_ix\n",
    "\n",
    "pred = pred_net(weighted_words)\n",
    "\n",
    "target = batch[1]\n",
    "\n",
    "if activation:\n",
    "    pred = activation(pred)\n",
    "\n",
    "pred_loss = criterion(pred, target)\n",
    "\n",
    "if reg:\n",
    "    reg_loss = reg * (torch.stack(sampler.exp_sizes) - reg_mean).pow(2).mean()\n",
    "    loss = pred_loss + reg_loss\n",
    "else:\n",
    "    loss = pred_loss\n",
    "\n",
    "loss.backward()\n",
    "duration = default_timer() - start\n",
    "print(duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93.63630560999991\n"
     ]
    }
   ],
   "source": [
    "def my_collate(batch, vocab=vocab):\n",
    "\n",
    "    # Count sizes\n",
    "    max_no_chunks = 0\n",
    "    for d in batch:\n",
    "        max_no_chunks = max(max_no_chunks, len(vocab.filterReview(d['review'])))\n",
    "    \n",
    "    # Map to Embeddings\n",
    "    reps = []\n",
    "    for d in batch:\n",
    "        rep = vocab.returnEmbds(d['review'])\n",
    "        rep = torch.cat([rep, Variable(torch.zeros(max_no_chunks + 1 - rep.size(0), rep.size(1)))], dim=0)\n",
    "        reps.append(rep)\n",
    "    \n",
    "    data_tensor = torch.stack(reps) \n",
    "    \n",
    "    # Create target vector\n",
    "    # target_tensor = Variable(torch.stack([d['target'] for d in batch]))\n",
    "    target_tensor = Variable(torch.stack([d['target'] for d in batch]))\n",
    "    \n",
    "    return data_tensor, target_tensor\n",
    "\n",
    "# Solution 1 using my_collate\n",
    "from timeit import default_timer\n",
    "from dpp_nets.layers.layers import KernelVar, MarginalSampler, PredNet\n",
    "\n",
    "\n",
    "start = default_timer()\n",
    "\n",
    "dl = DataLoader(ds, batch_size=500, collate_fn=my_collate)\n",
    "for batch in dl:\n",
    "    break\n",
    "words = batch[0]\n",
    "\n",
    "kernel_net = KernelVar(200,500,200)\n",
    "\n",
    "embd_dim = 200\n",
    "hidden_dim = 500\n",
    "kernel_dim = 200\n",
    "enc_dim = 200\n",
    "target_dim = 3\n",
    "\n",
    "kernel_net = KernelVar(embd_dim, hidden_dim, kernel_dim)\n",
    "sampler = MarginalSampler()\n",
    "pred_net = PredNet(embd_dim, hidden_dim, enc_dim, target_dim)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "activation = nn.Sigmoid()\n",
    "\n",
    "pred = None\n",
    "\n",
    "pred_loss = None \n",
    "reg_loss = None\n",
    "loss = None\n",
    "\n",
    "reg = 10\n",
    "reg_mean = 0.1\n",
    "\n",
    "kernel, words = kernel_net(words) # returned words are masked now!\n",
    "\n",
    "sampler.s_ix = kernel_net.s_ix\n",
    "sampler.e_ix = kernel_net.e_ix\n",
    "\n",
    "weighted_words = sampler(kernel, words) \n",
    "\n",
    "pred_net.s_ix = sampler.s_ix\n",
    "pred_net.e_ix = sampler.e_ix\n",
    "\n",
    "pred = pred_net(weighted_words)\n",
    "\n",
    "target = batch[1]\n",
    "\n",
    "if activation:\n",
    "    pred = activation(pred)\n",
    "\n",
    "pred_loss = criterion(pred, target)\n",
    "\n",
    "if reg:\n",
    "    reg_loss = reg * (torch.stack(sampler.exp_sizes) - reg_mean).pow(2).mean()\n",
    "    loss = pred_loss + reg_loss\n",
    "else:\n",
    "    loss = pred_loss\n",
    "\n",
    "\n",
    "loss.backward()\n",
    "duration = default_timer() - start\n",
    "print(duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.utils.backcompat.broadcast_warning.enabled = True\n",
    "torch.utils.backcompat.keepdim_warning.enabled = True\n",
    "words = Variable(torch.FloatTensor([[[1,2,3,4],[3,4,5,6],[0,0,0,0]],[[1,2,3,4],[0,0,0,0],[0,0,0,0]]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dl:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x1319bd9e8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.EmbeddingBag.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "1.00000e-02 *\n",
       " -2.3152\n",
       "[torch.DoubleTensor of size 1]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.EmbeddingBag.weight[3,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EmbeddingBag(112232, 200, mode=mean)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.EmbeddingBag.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.my_collate2>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_collate2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
