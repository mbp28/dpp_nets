{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "from dpp_nets.utils.io import make_embd, make_tensor_dataset\n",
    "from dpp_nets.my_torch.utilities import pad_tensor\n",
    "\n",
    "from dpp_nets.layers.layers import DeepSetBaseline\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Baseline (Deep Sets) Trainer')\n",
    "\n",
    "parser.add_argument('-a', '--aspect', type=str, choices=['aspect1', 'aspect2', 'aspect3', 'all'],\n",
    "                    help='what is the target?', required=True)\n",
    "parser.add_argument('--remote', type=int,\n",
    "                    help='training locally or on cluster?', required=True)\n",
    "\n",
    "parser.add_argument('--data_path_local', type=str, default='/Users/Max/data/beer_reviews',\n",
    "                    help='where is the data folder locally?')\n",
    "parser.add_argument('--data_path_remote', type=str, default='/cluster/home/paulusm/data/beer_reviews',\n",
    "                    help='where is the data folder?')\n",
    "\n",
    "parser.add_argument('--ckp_path_local', type=str, default='/Users/Max/checkpoints/beer_reviews',\n",
    "                    help='where is the data folder locally?')\n",
    "\n",
    "parser.add_argument('--ckp_path_remote', type=str, default='/cluster/home/paulusm/checkpoints/beer_reviews',\n",
    "                    help='where is the data folder?')\n",
    "\n",
    "parser.add_argument('-b', '--batch-size', default=50, type=int,\n",
    "                    metavar='N', help='mini-batch size (default: 50)')\n",
    "parser.add_argument('--epochs', default=100, type=int, metavar='N',\n",
    "                    help='number of total epochs to run')\n",
    "#parser.add_argument('--lr-k', '--learning-rate-k', default=0.1, type=float,\n",
    "#                    metavar='LRk', help='initial learning rate for kernel net')\n",
    "#parser.add_argument('--lr-p', '--learning-rate-p', default=0.1, type=float,\n",
    "#                    metavar='LRp', help='initial learning rate for pred net')\n",
    "parser.add_argument('--lr', '--learning-rate', default=1e-4, type=float,\n",
    "                    metavar='LR', help='initial learning rate for baseline')\n",
    "#parser.add_argument('--reg', type=float, required=True,\n",
    "#                    metavar='reg', help='regularization constant')\n",
    "#parser.add_argument('--reg-mean', type=float, required=True,\n",
    "#                    metavar='reg_mean', help='regularization_mean')\n",
    "\n",
    "args = parser.parse_args(\"-a all --remote 0\".split())\n",
    "\n",
    "val_path   = os.path.join(args.data_path_local, str.join(\".\",['reviews', args.aspect, 'heldout.txt.gz']))\n",
    "embd_path = os.path.join(args.data_path_local, 'review+wiki.filtered.200.txt.gz')\n",
    "#embd, word_to_ix = make_embd(embd_path)\n",
    "#val_set = make_tensor_dataset(val_path, word_to_ix)\n",
    "#val_loader = DataLoader(val_set, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dpp_nets.utils.language import create_clean_vocabulary\n",
    "nlp, vocab, embd = create_clean_vocabulary(embd_path, val_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embd.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dpp_nets.utils.language import BeerDataset, process_batch\n",
    "from torch.utils.data import DataLoader\n",
    "ds = BeerDataset(val_path)\n",
    "dl = DataLoader(ds, args.batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it\n",
      "it\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-2361416ea391>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mdata_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"it\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Max/git/dpp_nets/dpp_nets/utils/language.py\u001b[0m in \u001b[0;36mprocess_batch\u001b[0;34m(nlp, vocab, embd, batch)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mreview\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'review'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mrep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myield_chunk_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0mmaxi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Max/git/dpp_nets/dpp_nets/utils/language.py\u001b[0m in \u001b[0;36myield_chunk_vec\u001b[0;34m(doc, vocab, embd)\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mseen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mixs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0membd_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mixs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0membd_mat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for batch in dl: \n",
    "    data_tensor, target_tensor = process_batch(nlp, vocab, embd, batch)\n",
    "    print(\"it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Variable containing:\n",
       " ( 0 ,.,.) = \n",
       "   0.0277 -0.0263 -0.0170  ...  -0.0088  0.0221 -0.0420\n",
       "   0.0054 -0.0133 -0.0240  ...   0.0082  0.0281 -0.0722\n",
       "  -0.0010 -0.0129 -0.0357  ...  -0.0021 -0.0015 -0.0907\n",
       "            ...             ⋱             ...          \n",
       "   0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       "   0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       "   0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       " \n",
       " ( 1 ,.,.) = \n",
       "   0.0327  0.0120 -0.0418  ...   0.0364 -0.0414  0.0139\n",
       "  -0.0467 -0.0343  0.0037  ...   0.0424  0.0082 -0.0401\n",
       "   0.0372  0.0420 -0.0348  ...   0.0193 -0.0675  0.0350\n",
       "            ...             ⋱             ...          \n",
       "   0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       "   0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       "   0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       " \n",
       " ( 2 ,.,.) = \n",
       "   0.1333  0.0398  0.0170  ...  -0.0371 -0.0792 -0.0131\n",
       "   0.0797  0.0166  0.0091  ...  -0.0135 -0.0384  0.0055\n",
       "   0.1370  0.0370 -0.0155  ...  -0.0296 -0.0769 -0.0169\n",
       "            ...             ⋱             ...          \n",
       "   0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       "   0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       "   0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       " ... \n",
       " \n",
       " (47 ,.,.) = \n",
       "  -0.0001 -0.0333  0.0060  ...   0.0504 -0.0743 -0.0398\n",
       "   0.0326 -0.0203  0.0554  ...   0.0171 -0.0046 -0.0462\n",
       "   0.0272  0.0252  0.0513  ...   0.0098 -0.0557 -0.0072\n",
       "            ...             ⋱             ...          \n",
       "   0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       "   0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       "   0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       " \n",
       " (48 ,.,.) = \n",
       "   0.0641 -0.0107  0.0164  ...   0.0477 -0.0298  0.0403\n",
       "   0.0153  0.0256  0.0094  ...   0.0380  0.0161  0.0608\n",
       "  -0.0003  0.1492  0.0800  ...  -0.0639 -0.0589  0.1012\n",
       "            ...             ⋱             ...          \n",
       "   0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       "   0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       "   0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       " \n",
       " (49 ,.,.) = \n",
       "   0.0096  0.0059  0.0144  ...  -0.0240 -0.0654 -0.0478\n",
       "   0.0189  0.0117  0.0051  ...  -0.0298 -0.0554 -0.0524\n",
       "   0.0231 -0.0244 -0.0315  ...  -0.0536 -0.0497 -0.0368\n",
       "            ...             ⋱             ...          \n",
       "   0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       "   0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       "   0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       " [torch.FloatTensor of size 50x261x200], Variable containing:\n",
       "  0.8000  0.9000  1.0000\n",
       "  0.9000  0.8000  1.0000\n",
       "  0.9000  0.9000  0.5000\n",
       "  0.8000  0.7000  1.0000\n",
       "  0.8000  0.9000  1.0000\n",
       "  0.8000  0.2000  0.4000\n",
       "  0.8000  0.8000  0.7000\n",
       "  0.8000  0.7000  0.8000\n",
       "  0.9000  0.8000  0.6000\n",
       "  1.0000  0.7000  1.0000\n",
       "  0.6000  0.7000  0.9000\n",
       "  0.6000  0.3000  0.6000\n",
       "  0.6000  0.8000  0.5000\n",
       "  0.8000  0.5000  0.9000\n",
       "  0.8000  0.6000  0.7000\n",
       "  0.9000  0.7000  1.0000\n",
       "  0.7000  0.8000  0.6000\n",
       "  0.6000  0.7000  0.4000\n",
       "  0.8000  0.4000  0.6000\n",
       "  0.8000  0.8000  0.9000\n",
       "  0.6000  0.9000  0.5000\n",
       "  0.6000  0.5000  0.8000\n",
       "  0.9000  0.8000  0.8000\n",
       "  0.8000  0.3000  0.8000\n",
       "  0.9000  0.6000  0.6000\n",
       "  0.9000  0.6000  0.7000\n",
       "  0.9000  0.6000  0.8000\n",
       "  0.7000  0.9000  0.7000\n",
       "  0.9000  0.9000  0.7000\n",
       "  0.8000  0.9000  0.7000\n",
       "  0.7000  0.6000  0.4000\n",
       "  0.6000  0.4000  0.7000\n",
       "  0.8000  0.6000  0.4000\n",
       "  0.7000  0.5000  0.4000\n",
       "  0.8000  0.8000  0.7000\n",
       "  0.8000  0.9000  1.0000\n",
       "  1.0000  1.0000  0.7000\n",
       "  0.8000  0.4000  0.8000\n",
       "  0.6000  0.7000  0.9000\n",
       "  0.6000  0.7000  0.8000\n",
       "  0.6000  0.9000  0.6000\n",
       "  1.0000  0.7000  0.8000\n",
       "  0.6000  0.5000  0.3000\n",
       "  0.9000  0.5000  0.8000\n",
       "  0.9000  0.8000  1.0000\n",
       "  0.8000  0.9000  0.8000\n",
       "  0.7000  0.8000  0.6000\n",
       "  1.0000  0.8000  1.0000\n",
       "  0.8000  0.9000  1.0000\n",
       "  0.7000  0.6000  0.9000\n",
       " [torch.FloatTensor of size 50x3])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_batch(nlp, vocab, embd, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.0367\n",
       "-0.0959\n",
       "-0.0581\n",
       "-0.0023\n",
       " 0.1548\n",
       "-0.0854\n",
       "-0.0370\n",
       "-0.0280\n",
       " 0.0993\n",
       " 0.0641\n",
       "-0.0265\n",
       "-0.0419\n",
       "-0.0986\n",
       "-0.0960\n",
       "-0.0230\n",
       " 0.0312\n",
       " 0.0301\n",
       " 0.1630\n",
       "-0.0755\n",
       " 0.0054\n",
       " 0.0351\n",
       "-0.0297\n",
       " 0.0244\n",
       " 0.0297\n",
       " 0.0638\n",
       "-0.0877\n",
       "-0.0553\n",
       " 0.0934\n",
       " 0.0759\n",
       "-0.0686\n",
       " 0.0066\n",
       " 0.0122\n",
       "-0.0917\n",
       " 0.0939\n",
       "-0.0702\n",
       "-0.0314\n",
       " 0.0387\n",
       "-0.0120\n",
       "-0.0851\n",
       " 0.0803\n",
       " 0.0434\n",
       " 0.0422\n",
       " 0.0522\n",
       "-0.0545\n",
       " 0.1232\n",
       " 0.0263\n",
       "-0.0414\n",
       "-0.0113\n",
       " 0.1006\n",
       "-0.0199\n",
       " 0.0947\n",
       " 0.0259\n",
       " 0.0531\n",
       " 0.1443\n",
       "-0.0282\n",
       "-0.0111\n",
       " 0.0706\n",
       " 0.0402\n",
       "-0.0584\n",
       "-0.0849\n",
       "-0.0133\n",
       "-0.0563\n",
       " 0.0745\n",
       " 0.0277\n",
       "-0.0845\n",
       "-0.0642\n",
       "-0.0139\n",
       " 0.0541\n",
       "-0.1611\n",
       " 0.1006\n",
       " 0.0100\n",
       "-0.0166\n",
       " 0.0019\n",
       "-0.0643\n",
       "-0.1549\n",
       " 0.0205\n",
       "-0.1119\n",
       "-0.0787\n",
       "-0.1421\n",
       " 0.1119\n",
       " 0.0762\n",
       " 0.0568\n",
       "-0.0469\n",
       "-0.0706\n",
       " 0.0210\n",
       "-0.0191\n",
       " 0.0906\n",
       " 0.0095\n",
       "-0.0663\n",
       " 0.0055\n",
       " 0.0235\n",
       " 0.0365\n",
       "-0.0737\n",
       " 0.0623\n",
       "-0.0876\n",
       " 0.0021\n",
       "-0.0215\n",
       " 0.1084\n",
       " 0.0117\n",
       "-0.0342\n",
       " 0.1523\n",
       " 0.0126\n",
       " 0.0504\n",
       "-0.0937\n",
       "-0.0206\n",
       " 0.1593\n",
       " 0.0720\n",
       "-0.0469\n",
       "-0.0405\n",
       " 0.1108\n",
       " 0.1328\n",
       " 0.1019\n",
       " 0.0182\n",
       "-0.0722\n",
       "-0.0062\n",
       "-0.0360\n",
       "-0.0678\n",
       "-0.0839\n",
       "-0.0755\n",
       " 0.0167\n",
       "-0.1317\n",
       "-0.0321\n",
       " 0.0907\n",
       " 0.0195\n",
       " 0.0517\n",
       "-0.1689\n",
       " 0.0318\n",
       "-0.1241\n",
       " 0.0558\n",
       " 0.0799\n",
       "-0.0350\n",
       " 0.0811\n",
       " 0.0924\n",
       "-0.0775\n",
       " 0.0356\n",
       "-0.1355\n",
       " 0.0014\n",
       " 0.0389\n",
       " 0.0652\n",
       "-0.0270\n",
       "-0.0676\n",
       " 0.0426\n",
       " 0.0208\n",
       " 0.0099\n",
       " 0.1733\n",
       " 0.0143\n",
       "-0.0217\n",
       "-0.0494\n",
       "-0.0447\n",
       "-0.1581\n",
       " 0.0649\n",
       " 0.0421\n",
       " 0.0236\n",
       " 0.1333\n",
       " 0.0318\n",
       "-0.0882\n",
       " 0.1076\n",
       " 0.0363\n",
       "-0.0310\n",
       " 0.0211\n",
       " 0.0480\n",
       "-0.0786\n",
       "-0.0762\n",
       "-0.0397\n",
       " 0.0160\n",
       " 0.0489\n",
       "-0.0098\n",
       "-0.0463\n",
       "-0.0669\n",
       " 0.0847\n",
       " 0.0367\n",
       " 0.0008\n",
       " 0.0440\n",
       " 0.0739\n",
       "-0.1244\n",
       " 0.0006\n",
       "-0.0692\n",
       " 0.0252\n",
       " 0.0209\n",
       " 0.0013\n",
       " 0.0078\n",
       "-0.0029\n",
       "-0.0530\n",
       "-0.1246\n",
       " 0.0973\n",
       " 0.0686\n",
       "-0.0064\n",
       "-0.0424\n",
       "-0.0722\n",
       "-0.0107\n",
       " 0.0397\n",
       " 0.0364\n",
       "-0.0402\n",
       "-0.0297\n",
       "-0.0222\n",
       " 0.0783\n",
       "-0.0056\n",
       " 0.1171\n",
       "-0.0728\n",
       " 0.0014\n",
       "[torch.FloatTensor of size 200]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tensor[0][60,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dpp_nets.layers.layers import ChunkTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = ChunkTrainer(200,500,200,200,3)(data_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.cat([rep, Variable(torch.zeros(maxi + 1 - rep.size(0),rep.size(1)))],dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l = []\n",
    "for chunk in list(yield_chunks(doc, vocab)):\n",
    "    c = embd(Variable(chunk)).mean(0)\n",
    "    l.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_batch_chunks(batch):\n",
    "    \"\"\"\n",
    "    Batch is a dictioary.\n",
    "    \"\"\"\n",
    "    MAX_CHUNK_LENGTH = 271\n",
    "    MAX_CHUNK_NO = 397\n",
    "\n",
    "    data_tensor = []\n",
    "    for review in batch['review']:\n",
    "        doc = nlp(review)\n",
    "        chunks = list(yield_chunks(doc, vocab, MAX_CHUNK_LENGTH))\n",
    "        chunks = pad_tensor(torch.stack(chunks),0,0, MAX_CHUNK_NO)\n",
    "        data_tensor.append(chunks)\n",
    "    data_tensor = torch.stack(data_tensor)\n",
    "    \n",
    "    target_tensor = torch.stack(batch['target']).t().float()\n",
    "    \n",
    "    return data_tensor, target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_batch_sens(batch):\n",
    "    \"\"\"\n",
    "    Batch is a dictioary.\n",
    "    \"\"\"\n",
    "    MAX_CHUNK_LENGTH = 271\n",
    "    MAX_CHUNK_NO = 397\n",
    "\n",
    "    data_tensor = []\n",
    "    for review in batch['review']:\n",
    "        doc = nlp(review)\n",
    "        chunks = list(yield_chunks(doc, vocab, MAX_CHUNK_LENGTH))\n",
    "        chunks = pad_tensor(torch.stack(chunks),0,0, MAX_CHUNK_NO)\n",
    "        data_tensor.append(chunks)\n",
    "    data_tensor = torch.stack(data_tensor)\n",
    "    \n",
    "    target_tensor = torch.stack(batch['target']).t().float()\n",
    "    \n",
    "    return data_tensor, target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "\n",
    "anno_path = os.path.join(args.data_path_local, 'annotations.json')\n",
    "\n",
    "class BeerDatasetAnnotated(Dataset):\n",
    "    \"\"\"BeerDataset.\"\"\"\n",
    "\n",
    "    def __init__(self, anno_path, aspect='all'):\n",
    "        \n",
    "        # Compute size of the data set      \n",
    "        self.aspect = aspect\n",
    "        \n",
    "\n",
    "        self.lines = []\n",
    "        with open(anno_path) as f:\n",
    "            for line in f:\n",
    "                item = json.loads(line)\n",
    "\n",
    "                # Get doc\n",
    "                doc = nlp(ast.literal_eval(item['raw'])['review/text']\n",
    "\n",
    "                # Get annotations\n",
    "                a0 = item['0']\n",
    "                a1 = item['1']\n",
    "                a2 = item['2']\n",
    "\n",
    "                # Get target\n",
    "                target = item['y']\n",
    "\n",
    "                self.lines.append((doc,(a0, a1, a2), target))\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lines)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        instance = self.lines[idx]            \n",
    "        sample = {'review': instance[0], 'target': instance[2]}\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ds = BeerDataset(val_path)\n",
    "dl = DataLoader(ds, batch_size=12, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "process_batch(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dpp_nets.my_torch.utilities import pad_tensor\n",
    "\n",
    "def filter_stops(tree, vocab):\n",
    "    return (token.text for token in tree if not token.is_stop and token.text in vocab.word2index)\n",
    "\n",
    "#def yield_words(doc, vocab)\n",
    "\n",
    "def yield_chunks(doc, vocab, MAX_CHUNK_LENGTH):\n",
    "    seen = set()\n",
    "    for token in doc:\n",
    "        t = tuple((filter_stops(token.subtree, vocab)))\n",
    "        if t and t not in seen:\n",
    "            seen.add(t)\n",
    "            #ixs = [vocab.word2index[word] if word in vocab.word2index else print(word) for word in t]\n",
    "            ixs = torch.LongTensor([vocab.word2index[word] for word in t])\n",
    "            ixs = pad_tensor(ixs,0,0,MAX_CHUNK_LENGTH)\n",
    "            yield ixs\n",
    "            \n",
    "def yield_sentences(doc, vocab, MAX_SENTENCE_LENGTH):\n",
    "    seen = set()\n",
    "    for sen in doc.sents:\n",
    "        t = tuple((filter_stops(sen)))\n",
    "        if t and t not in seen:\n",
    "            seen.add(t)\n",
    "            #ixs = [vocab.word2index[word] for word in t]\n",
    "            ixs = torch.LongTensor([vocab.word2index[word] for word in t])\n",
    "            ixs = pad_tensor(ixs,0,0,MAX_SENTENCE_LENGTH)\n",
    "            yield ixs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "short_path = os.path.join(args.data_path_local, str.join(\".\",['reviews', 'short', 'train.txt.gz']))\n",
    "# Let's measure the lengths. \n",
    "with gzip.open(short_path, 'rt') as f:\n",
    "        lines = f.readlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open(short_path, 'wt') as f:\n",
    "    for line in lines[:100]:\n",
    "        f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_CHUNK_LENGTH = max([len(m) for l in measure_list for m in l])\n",
    "Max_CHUNK_NO = max([len(l) for l in measure_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from spacy.symbols import nsubj, VERB\n",
    "\n",
    "sentence = list(doc.sents)[5]\n",
    "for token in sentence:\n",
    "    #print(token, list(token.children), token.head, token.dep_, list(token.lefts), list(token.rights), list(token.subtree))\n",
    "    #print(token, list(token.subtree)) #,list(token.lefts),list(token.rights),token.left_edge, token.right_edge)\n",
    "    print((token, list(token.subtree)))\n",
    "    #print(token, list(token.ancestors))\n",
    "    #print(token, token.vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_iterator(data_path):\n",
    "    with gzip.open(data_path, 'rt') as f:\n",
    "        for line in f:\n",
    "            target, sep, words = line.partition(\"\\t\")\n",
    "            words, target = words.split(), target.split()\n",
    "            if len(words):\n",
    "                target = torch.Tensor([float(v) for v in target])\n",
    "                yield words, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "n = 0\n",
    "maxi = 0\n",
    "mean = M2 = 0.0\n",
    "\n",
    "with gzip.open(val_path, 'rt') as f:\n",
    "    for line in f:\n",
    "        target, sep, review = line.partition(\"\\t\")\n",
    "        n_sentences = len(nltk.sent_tokenize(review))\n",
    "        x = n_sentences\n",
    "        \n",
    "        n += 1\n",
    "        if x == 101: \n",
    "            break\n",
    "        maxi = max(x, maxi)\n",
    "        delta = x - mean\n",
    "        mean += delta/n\n",
    "        delta2 = x - mean\n",
    "        M2 += delta*delta2\n",
    "\n",
    "        #i += 1\n",
    "        #if i > 21:\n",
    "         #   break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# How to create sentences from a review\n",
    "sentences = nltk.sent_tokenize(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# How to create words from a sentence\n",
    "words = nltk.word_tokenize(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# How to remove stop words from a sentence\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "operators = set(('no', 'not'))\n",
    "punctuation = set(string.punctuation)\n",
    "stop = (set(stopwords.words('english')) | punctuation) - operators \n",
    "fwords = [[word for word in words if word not in stop] for words in [nltk.word_tokenize(sen) for sen in sentences]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# How to create bigrams (after stop-word removal? - would do so)\n",
    "\n",
    "print(list(nltk.bigrams(words)))\n",
    "print(30*'-')\n",
    "list(nltk.bigrams(fwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "anno_path = os.path.join(args.data_path_local, 'annotations.json')\n",
    "def read_rationales(path):\n",
    "    \"\"\"\n",
    "    This reads the json.annotations file. \n",
    "    Creates a list of dictionaries, which holds the 994 reviews for which\n",
    "    sentence-level annotations are available. \n",
    "    \"\"\"\n",
    "    data = []\n",
    "    fopen = gzip.open if path.endswith(\".gz\") else open\n",
    "    with fopen(path) as fin:\n",
    "        for line in fin:\n",
    "            item = json.loads(line)\n",
    "            data.append(item)\n",
    "    return data\n",
    "\n",
    "rationales = read_rationales(anno_path)\n",
    "\n",
    "rationales[0].keys()\n",
    "\n",
    "nltk.sent_tokenize(rationales[1]['raw'])[10]\n",
    "\n",
    "import string\n",
    "raw = \"\".join([\" \"+i if not i.startswith(\"'\") and i not in string.punctuation else i for i in rationales[2]['x']]).strip()\n",
    "nltk.sent_tokenize(raw)\n",
    "\n",
    "print(rationales[2]['2'][0])\n",
    "rationales[2]['x'][49:63]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "doc = nlp(ast.literal_eval(data[0]['raw']) ['review/text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data[0]['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import tqdm\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "path = os.path.join(args.data_path_local, str.join(\".\",['reviews', args.aspect, 'heldout.txt.gz']))\n",
    "def simple_iterator(path):\n",
    "    with gzip.open(path, 'rt') as f:\n",
    "        for line in f:\n",
    "            target, sep, review = line.partition(\"\\t\")\n",
    "            yield review, target\n",
    "            \n",
    "# Create set of filtered sentences for each review:\n",
    "operators = set(('no', 'not'))\n",
    "punctuation = set(string.punctuation)\n",
    "stop = set(stopwords.words('english')) | punctuation | set('...')  - operators \n",
    "\n",
    "for review, target in tqdm.tqdm(simple_iterator(path)):\n",
    "\n",
    "    # Split review into sentences\n",
    "    sens = nltk.sent_tokenize(review)\n",
    "    \n",
    "    # Split each sentence into words\n",
    "    sens = [nltk.word_tokenize(sen) for sen in sens]\n",
    "    \n",
    "    # Remove stop words\n",
    "    fwords = [[word for word in sen if word not in stop] for sen in sens]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Noun-Chunks\n",
    "def noun_chunks(doc):\n",
    "    my_processed_review = []\n",
    "    for chunk in doc.noun_chunks:\n",
    "        chunk = tuple(filter(lambda token: not token.is_stop, chunk))\n",
    "        chunk = tuple(word.text for word in chunk)\n",
    "        my_processed_review.append(chunk)\n",
    "    my_processed_review = list(filter(None, my_processed_review))\n",
    "    return my_processed_review\n",
    "\n",
    "# Sub-Trees\n",
    "def sub_trees(doc):\n",
    "    my_processed_review = []\n",
    "    for sen in doc.sents:\n",
    "        for token in sen:\n",
    "            chunk = token.subtree\n",
    "            chunk = tuple(filter(lambda token: not token.is_stop, chunk))\n",
    "            #chunk = tuple(word.text for word in chunk)\n",
    "            my_processed_review.append(chunk)\n",
    "    my_processed_review = list(filter(None, my_processed_review))\n",
    "    my_processed_review = list(set(my_processed_review))\n",
    "    return my_processed_review\n",
    "\n",
    "# Sentence-Level processing. \n",
    "def sentences(doc):\n",
    "    my_processed_review = []\n",
    "    for sen in doc.sents:\n",
    "        chunk = tuple(token for token in sen if not token.is_stop)\n",
    "        chunk = tuple(word.text for word in chunk)\n",
    "        my_processed_review.append(chunk)\n",
    "    my_processed_review = list(filter(None, my_processed_review))\n",
    "    return my_processed_review\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
