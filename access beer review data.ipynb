{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "994\n"
     ]
    }
   ],
   "source": [
    "# annotations contains the rationals. Roughly 1000, 994 to be precise.\n",
    "# heldouts seem to be the validation sets\n",
    "# what is the test set? just the 994 reviews that are also annotated? or all data?\n",
    "# review + wiki_filtered-200 could be the word embeddings. \n",
    "\n",
    "filepath = \"/Users/Max/data/beer_reviews/Beeradvocate.txt\" # full-dataset\n",
    "filtered =  \"/Users/Max/data/beer_reviews/review+wiki.filtered.200.txt.gz\" # review+wiki.filtered.200.txt.gz\n",
    "anno = \"/Users/Max/data/beer_reviews/annotations.json\" # annotated commentaries\n",
    "heldout = \"/Users/Max/data/beer_reviews/reviews.aspect1.heldout.txt.gz\" # selection of commentaries\n",
    "count = 0\n",
    "\n",
    "with open(anno, 'r') as f:\n",
    "    for line in f:\n",
    "        count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Firstly, I need a PyTorch Dataset.\n",
    "# Secondly, I can simply pass this PyTorch Dataset to DataLoader, to get batch functionality etc. \n",
    "\n",
    "###\n",
    "# Let's have one embedding layer, which maps word incides to the corresponding embeddings. \n",
    "# I need one padding index!\n",
    "# My data set consists of x, y.\n",
    "# x is a list of indices, y is the sentinment vector\n",
    "# Input: file_path to txt file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build my custom embedding\n",
    "# i.e. transfer the word embeddings to a PyTorch word embedding\n",
    "# This works!\n",
    "# Wrap into two functions!\n",
    "# 1) Function to actually create the word embedding pytorch file, takes 10 sec., need only once forever\n",
    "# 2) Function that instantiates and loads word embeddings and set requires_grad = False\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "import gzip\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from dpp_nets.utils.io import embd_iterator, data_iterator\n",
    "\n",
    "root = '/Users/Max/data/beer_reviews'\n",
    "data_file = 'reviews.aspect1.heldout.txt.gz'\n",
    "embd_file = 'review+wiki.filtered.200.txt.gz'\n",
    "\n",
    "def make_embd(embd_path, word_to_ix=False, save_path=None):\n",
    "    \n",
    "    # Create dictionaries\n",
    "    ix_to_word = {}\n",
    "    ix_to_vecs = {}\n",
    "    word_to_ix = {}\n",
    "    \n",
    "    for ix, (word, vecs) in enumerate(embd_iterator(embd_path)):\n",
    "        ix_to_word[ix] = word\n",
    "        ix_to_vecs[ix] = vecs\n",
    "        word_to_ix = {word: ix}\n",
    "    \n",
    "    vocab_size, embd_dim = len(ix_to_word), len(ix_to_vecs[0])\n",
    "    \n",
    "    if word_to_ix:\n",
    "        return word_to_ix\n",
    "    \n",
    "    embd = torch.zeros(1 + vocab_size, embd_dim)\n",
    "    for i, vec in enumerate(ix_to_vecs.values(), 1): \n",
    "        embd[i] = vec\n",
    "\n",
    "    embd_weight_dict = OrderedDict([('weight', embd)])\n",
    "\n",
    "    if save:\n",
    "        torch.save(embd_weight_dict, 'embeddings.pt')    \n",
    "    else:\n",
    "        embd_layer = nn.Embedding(1 + vocab_size, embd_dim, padding_idx=0)\n",
    "        embd_layer.load_state_dict(embd_weight_dict)\n",
    "        embd.weight.requires_grad = False\n",
    "        return embd_layer\n",
    "\n",
    "def load_embd(embd_dict_path):\n",
    "    \n",
    "    embd_weight_dict = torch.load(embd_dict_path)\n",
    "    vocab_size, embd_dim = embd_weight_dict['weight'].size()\n",
    "    embd_layer = nn.Embedding(1 + vocab_size, embd_dim, padding_idx=0)\n",
    "    embd_layer.load_state_dict(embd_weight_dict)\n",
    "    embd.weight.requires_grad = False\n",
    "\n",
    "    return embd_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_tensor_dataset(data_path, word_to_ix):\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_embd(embd_dict_path):\n",
    "    \n",
    "    embd_weight_dict = torch.load(embd_dict_path)\n",
    "    vocab_size, embd_dim = embd_weight_dict['weight'].size()\n",
    "    embd_layer = nn.Embedding(1 + vocab_size, embd_dim, padding_idx=0)\n",
    "    embd_layer.load_state_dict(embd_weight_dict)\n",
    "    embd.weight.requires_grad = False\n",
    "\n",
    "    return embd_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "   \n",
    "\n",
    "\n",
    "\n",
    "def make_pytorch_ready(root, data_file, embd_file, max_set_size=None):  \n",
    "    \n",
    "    # Create data paths\n",
    "    data_path = os.path.join(root, data_file)\n",
    "    embd_path = os.path.join(root, embd_file)\n",
    "    \n",
    "    # Create dictionaries\n",
    "    ix_to_word = {}\n",
    "    ix_to_vals = {}\n",
    "    \n",
    "    for ix, (word, vals) in enumerate(embd_iterator(embd_path)):\n",
    "        ix_to_word[ix] = word\n",
    "        ix_to_vals[ix] = vals\n",
    "    \n",
    "    word_to_ix = {v: k for k,v in ix_to_word.items()}\n",
    "    \n",
    "    # Create stats\n",
    "    vocab_size = len(ix_to_word)\n",
    "    embd_dim = len(ix_to_vals[0])\n",
    "    \n",
    "    # Create word embedding\n",
    "    my_embd = np.zeros([1 + vocab_size, embd_dim])\n",
    "    for i, val in enumerate(ix_to_vals.values(),1): \n",
    "        my_embd[i] = val\n",
    "    my_state_dict = OrderedDict([('weight',torch.Tensor(my_embd))])\n",
    "    torch.save(my_state_dict, 'embeddings.pt')\n",
    "    \n",
    "    if not max_set_size:\n",
    "        max_set_size = 915\n",
    "        # calculate the maximum_set_size\n",
    "        \n",
    "    # Create Dataset\n",
    "    data_x, data_y = [ ], [ ]\n",
    "    for (words, target) in data_iterator(data_path):\n",
    "        data_x.append(words)\n",
    "        data_y.append(target)\n",
    "        \n",
    "    targets = torch.stack([torch.Tensor(i) for i in data_y])\n",
    "\n",
    "    # one-loop too many\n",
    "    data_x_enc = []\n",
    "    errors = []\n",
    "    for words in data_x:\n",
    "        temp = []\n",
    "        temp2 = []\n",
    "        for word in words:\n",
    "            if word in word_to_ix:\n",
    "                temp.append(word_to_ix[word])\n",
    "            else:\n",
    "                temp2.append(word)\n",
    "        temp = np.pad(np.array(temp),[0,max_set_size - len(temp)], 'constant', constant_values=(0, 0))\n",
    "        data_x_enc.append(temp)\n",
    "        errors.append(temp2)\n",
    "    reviews = torch.stack([torch.LongTensor(i) for i in data_x_enc])\n",
    "    \n",
    "    return reviews, targets\n",
    "# Consider changing the word-processing, i.e. split at -, split at /, etc. !!\n",
    "# Right now "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root = '/Users/Max/data/beer_reviews'\n",
    "data_file = 'reviews.aspect1.heldout.txt.gz'\n",
    "embd_file = 'review+wiki.filtered.200.txt.gz'\n",
    "\n",
    "reviews, targets = make_pytorch_ready(root, data_file, embd_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_data_set = TensorDataset(reviews, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(my_data_set, 'my_data_set.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loaded_data_set = torch.load('my_data_set.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 1.2000e+01  6.4300e+02  2.6700e+02  ...   0.0000e+00  0.0000e+00  0.0000e+00\n",
       " 2.1037e+04  2.5109e+04  1.0740e+03  ...   0.0000e+00  0.0000e+00  0.0000e+00\n",
       " 7.4300e+02  8.9000e+01  3.4000e+02  ...   0.0000e+00  0.0000e+00  0.0000e+00\n",
       "                ...                   ⋱                   ...                \n",
       " 6.6000e+01  1.4000e+01  2.2000e+01  ...   0.0000e+00  0.0000e+00  0.0000e+00\n",
       " 1.1430e+03  3.5800e+02  1.0000e+00  ...   0.0000e+00  0.0000e+00  0.0000e+00\n",
       " 2.6959e+04  1.1200e+02  6.1390e+03  ...   0.0000e+00  0.0000e+00  0.0000e+00\n",
       "[torch.LongTensor of size 10000x915]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_data_set.data_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "( 0 ,.,.) = \n",
       " -0.0813 -0.0336  0.0406  ...  -0.0041 -0.0135 -0.0813\n",
       "  0.1185  0.0688 -0.0062  ...   0.0312 -0.0398 -0.0726\n",
       " -0.1108 -0.0834  0.0851  ...   0.0742  0.0198  0.0375\n",
       "           ...             ⋱             ...          \n",
       "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       "\n",
       "( 1 ,.,.) = \n",
       " -0.0072 -0.0024  0.0411  ...  -0.0923 -0.0001 -0.0169\n",
       " -0.0515 -0.1156  0.1176  ...   0.0355  0.0461 -0.0511\n",
       " -0.0287 -0.0775  0.1195  ...   0.0786  0.0693  0.0316\n",
       "           ...             ⋱             ...          \n",
       "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       "\n",
       "( 2 ,.,.) = \n",
       " -0.0105 -0.0856  0.0691  ...   0.0508  0.0344 -0.0080\n",
       "  0.0362  0.0103  0.0808  ...  -0.0468  0.0393  0.0518\n",
       " -0.1188  0.0190 -0.0323  ...  -0.0478 -0.1224  0.0610\n",
       "           ...             ⋱             ...          \n",
       "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       "\n",
       "( 3 ,.,.) = \n",
       "  0.0779 -0.0145 -0.1343  ...   0.0231 -0.0424 -0.1053\n",
       "  0.0637 -0.0729 -0.0013  ...  -0.0112 -0.0029 -0.1075\n",
       "  0.0740  0.0255  0.0590  ...  -0.0621 -0.1416 -0.0944\n",
       "           ...             ⋱             ...          \n",
       "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       "\n",
       "( 4 ,.,.) = \n",
       "  0.0671 -0.0601 -0.0625  ...  -0.0142  0.0119  0.0019\n",
       "  0.0367 -0.0642 -0.1111  ...  -0.0301 -0.1359 -0.1109\n",
       "  0.0319 -0.0134 -0.0357  ...   0.0427 -0.0499 -0.1087\n",
       "           ...             ⋱             ...          \n",
       "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       "\n",
       "( 5 ,.,.) = \n",
       " -0.1518  0.0251  0.0050  ...  -0.0179 -0.0713  0.0152\n",
       " -0.0816 -0.0083 -0.0058  ...  -0.0056 -0.0061 -0.0208\n",
       " -0.0103  0.0417  0.0877  ...  -0.0463 -0.0232  0.0066\n",
       "           ...             ⋱             ...          \n",
       "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       "[torch.FloatTensor of size 6x915x200]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "embd(Variable(reviews[1:7]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hey', 'you', 'what', 'are', 'you', 'doing', 'here']"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "# Splitting on: , <space> - ! ? :\n",
    "list(filter(None, re.split(\"[, \\-!?:]+\", \"Hey, you - what are you doing here!?\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = os.path.join(root, data_file)\n",
    "embd_path = os.path.join(root, embd_file)\n",
    "gen = create_embd_iterator(embd_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "gen = create_data_iterator(data_path)\n",
    "for instance in gen:\n",
    "    count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import gzip\n",
    "import numpy as np\n",
    "\n",
    "def read_annotations(path):\n",
    "    \"\"\"\n",
    "    This reads in the original data set.  \n",
    "    \"\"\"\n",
    "    data_x, data_y = [ ], [ ]\n",
    "    fopen = gzip.open if path.endswith(\".gz\") else open\n",
    "    with fopen(path, 'rt') as fin:\n",
    "        for line in fin:\n",
    "            y, sep, x = line.partition(\"\\t\")\n",
    "            x, y = x.split(), y.split()\n",
    "            if len(x) == 0: continue\n",
    "            y = np.asarray([ float(v) for v in y ])\n",
    "            data_x.append(x)\n",
    "            data_y.append(y)\n",
    "    say(\"{} examples loaded from {}\\n\".format(\n",
    "            len(data_x), path\n",
    "        ))\n",
    "    say(\"max text length: {}\\n\".format(\n",
    "        max(len(x) for x in data_x)\n",
    "    ))\n",
    "    return data_x, data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build word embedding\n",
    "embd_path = \"/Users/Max/data/beer_reviews/review+wiki.filtered.200.txt.gz\"\n",
    "\n",
    "def load_embedding_iterator(path):\n",
    "    file_open = gzip.open if path.endswith(\".gz\") else open\n",
    "    with file_open(path, 'rt') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                parts = line.split()\n",
    "                word = parts[0]\n",
    "                vals = np.array([float(x) for x in parts[1:]])\n",
    "                yield word, vals\n",
    "\n",
    "ix_to_word = {}\n",
    "ix_to_vals = {}\n",
    "\n",
    "for ix, (word, vals) in enumerate(load_embedding_iterator(embd_path)):\n",
    "    ix_to_word[ix] = word\n",
    "    ix_to_vals[ix] = vals\n",
    "\n",
    "word_to_ix = {v: k for k,v in ix_to_word.items()}\n",
    "\n",
    "assert(len(ix_to_vals) == len(ix_to_word))\n",
    "vocab_size = len(ix_to_word)\n",
    "embd_dim = len(ix_to_vals[0])\n",
    "    \n",
    "my_embd = np.zeros([1 + vocab_size, embd_dim])\n",
    "for i, val in enumerate(ix_to_vals.values(),1): \n",
    "    my_embd[i] = val\n",
    "        \n",
    "my_state_dict = OrderedDict([('weight',torch.Tensor(my_embd))])\n",
    "torch.save(my_state_dict, 'embeddings.pt')\n",
    "\n",
    "#########\n",
    "embd = nn.Embedding(1 + vocab_size, embd_dim, padding_idx=0)\n",
    "embd.load_state_dict(torch.load('embeddings.pt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import gzip\n",
    "import numpy as np\n",
    "\n",
    "def read_annotations(path):\n",
    "    \"\"\"\n",
    "    This reads in the original data set.  \n",
    "    \"\"\"\n",
    "    data_x, data_y = [ ], [ ]\n",
    "    fopen = gzip.open if path.endswith(\".gz\") else open\n",
    "    with fopen(path, 'rt') as fin:\n",
    "        for line in fin:\n",
    "            y, sep, x = line.partition(\"\\t\")\n",
    "            x, y = x.split(), y.split()\n",
    "            if len(x) == 0: continue\n",
    "            y = np.asarray([ float(v) for v in y ])\n",
    "            data_x.append(x)\n",
    "            data_y.append(y)\n",
    "    say(\"{} examples loaded from {}\\n\".format(\n",
    "            len(data_x), path\n",
    "        ))\n",
    "    say(\"max text length: {}\\n\".format(\n",
    "        max(len(x) for x in data_x)\n",
    "    ))\n",
    "    return data_x, data_y\n",
    "\n",
    "def read_rationales(path):\n",
    "    \"\"\"\n",
    "    This reads the json.annotations file. \n",
    "    Creates a list of dictionaries, which holds the 994 reviews for which\n",
    "    sentence-level annotations are available. \n",
    "    \"\"\"\n",
    "    data = [ ]\n",
    "    fopen = gzip.open if path.endswith(\".gz\") else open\n",
    "    with fopen(path) as fin:\n",
    "        for line in fin:\n",
    "            item = json.loads(line)\n",
    "            data.append(item)\n",
    "    return data\n",
    "\n",
    "def read_corpus(path):\n",
    "    with open(path) as fin:\n",
    "        lines = fin.readlines()\n",
    "    lines = [ x.strip().split() for x in lines ]\n",
    "    lines = [ x for x in lines if x ]\n",
    "    corpus_x = [ x[1:] for x in lines ]\n",
    "    corpus_y = [ int(x[0]) for x in lines ]\n",
    "    return corpus_x, corpus_y\n",
    "\n",
    "def say(s, stream=sys.stdout):\n",
    "    stream.write(\"{}\".format(s))\n",
    "    stream.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 examples loaded from /Users/Max/data/beer_reviews/reviews.aspect1.heldout.txt.gz\n",
      "max text length: 915\n"
     ]
    }
   ],
   "source": [
    "#annotated_rationales = read_rationales(anno)\n",
    "validation_x, validation_y = read_annotations(heldout)\n",
    "targets = torch.stack([torch.Tensor(i) for i in validation_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "errors = []\n",
    "data_x_enc =[]\n",
    "\n",
    "for words in validation_x:\n",
    "    temp = []\n",
    "    temp2 = []\n",
    "    for word in words:\n",
    "        if word in word_to_ix:\n",
    "            temp.append(word_to_ix[word])\n",
    "        else:\n",
    "            temp2.append(word)\n",
    "    data_x_enc.append(temp)\n",
    "    errors.append(temp2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
