{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The old notebook became too confusing!\n",
    "\n",
    "# Trying to check Singular Values and Singular Vectors in Numpy\n",
    " \n",
    "import numpy as np\n",
    "\n",
    "# General Set-up\n",
    "N = 2\n",
    "A = 0.1 * np.random.randn(N, N) + np.diag(np.arange(1, N+1))\n",
    "B = np.random.randn(N, N)\n",
    "I = np.eye(N)\n",
    "dA = np.random.randn(N, N)\n",
    "dB = np.random.randn(N, N)\n",
    "bC = np.random.randn(N, N)\n",
    "eps = 1e-20\n",
    "epsi = 1 / eps\n",
    "Ae = A + 1j*eps*dA\n",
    "Be = B + 1j*eps*dB # don't need it, I'm working with single input.\n",
    "\n",
    "# SVD Set-up\n",
    "u, s, vT = np.linalg.svd(A)\n",
    "\n",
    "# Complex part\n",
    "De, Du = np.linalg.eig(Ae.dot(Ae.T))\n",
    "Du, _, _ = np.linalg.svd(Ae)\n",
    "idx = De.argsort()[::-1]   \n",
    "De = De[idx]\n",
    "Du = u[:,idx]\n",
    "D = np.real(De) # only needed for what? \n",
    "\n",
    "# forward propagation \n",
    "dA = dA\n",
    "dS = np.diag(I * u.T.dot(dA).dot(vT.T))\n",
    "\n",
    "E = np.outer(np.ones(N), s) - np.outer(s, np.ones(N))\n",
    "F = 1 / (E + np.eye(N)) - np.eye(N)\n",
    "dU = u.dot(F * (u.T.dot(dA).dot(vT.T).dot(np.diag(s)) + np.diag(s).dot(vT).dot(dA.T).dot(u)))\n",
    "\n",
    "# backward gradients\n",
    "bS = np.random.randn(N) # gradients wrt singular values\n",
    "bA = u.dot(np.diag(bS)).dot(vT) # backpropagated gradient wrt to matrix A\n",
    "\n",
    "print('singular value')\n",
    "print('svd error: ', (np.linalg.norm(s-np.sqrt(D))))\n",
    "\n",
    "# Forward Check based on complex matrices\n",
    "print('CVT error (vals): ', (np.linalg.norm(2*s*dS - epsi*np.imag(De))))\n",
    "print('CVT error (vecs): ', (np.linalg.norm(dU - epsi*np.imag(Du))))\n",
    "\n",
    "# Backward Check, these are essentially two traces!!\n",
    "print('adj error: ',np.sum(dA*bA)-np.sum(dS*bS))\n",
    "\n",
    "# I should be able to use the same identity for my testing purposes!!!\n",
    "# trace(bC.T * dC) = trace(bA.T * dA) + trace(bB.T * dB)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Checking eigenvectors and eigenvalues in Numpy\n",
    "np.random.seed(10)\n",
    "# General Set-up\n",
    "N = 5\n",
    "A = 0.1 * np.random.randn(N, N) + np.diag(np.arange(1, N+1))\n",
    "#A = A.dot(A.T)\n",
    "B = np.random.randn(N, N)\n",
    "I = np.eye(N)\n",
    "dA = np.random.randn(N, N)\n",
    "dB = np.random.randn(N, N)\n",
    "bC = np.random.randn(N, N)\n",
    "eps = 1e-20\n",
    "epsi = 1 / eps\n",
    "Ae = A + 1j*eps*dA\n",
    "Be = B + 1j*eps*dB\n",
    "\n",
    "# EIGEN Set-up\n",
    "De, Ue = np.linalg.eig(Ae)\n",
    "D = np.real(De)\n",
    "U = np.real(Ue)\n",
    "\n",
    "# make dC diagonal equal to zero\n",
    "Ue = Ue.dot(np.diag(1 / np.diag(np.linalg.inv(U).dot(Ue))))\n",
    "E = np.outer(np.ones(N), D) - np.outer(D, np.ones(N))\n",
    "F = 1 / (E + np.eye(N)) - np.eye(N)\n",
    "P = np.linalg.inv(U).dot(dA.dot(U))\n",
    "dD = np.eye(N) * P\n",
    "dU = U.dot(F*P)\n",
    "#dD, dU = forward(A, dA)\n",
    "\n",
    "bD = np.diag(np.random.randn(N)) # random perturbation of eigenvalues\n",
    "bU = np.random.randn(N,N) # random perturbation of eigenvectors\n",
    "bD = bD + F * (U.T.dot(bU))\n",
    "bA = np.linalg.inv(U.T).dot(bD.dot(U.T))\n",
    "print('eigenvalues and eigenvectors')\n",
    "print('CVT error (vals): ', np.linalg.norm(np.diag(dD) - epsi*np.imag(De)))\n",
    "print('CVT error (vecs): ', np.linalg.norm(dU - epsi*np.imag(Ue)))\n",
    "print('adj error (backward): ', np.sum(dA*bA)-np.sum(dD*bD)-np.sum(dU*bU))\n",
    "#print('CVT error (vecs): ', np.linalg.norm(np.abs(dU) - np.abs(epsi*np.imag(Ue))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# essentials for eig in PyTorch\n",
    "import torch\n",
    "\n",
    "N = 5\n",
    "A = torch.randn(N,N).double()\n",
    "L = A.mm(A.t())\n",
    "I = torch.eye(N).double()\n",
    "dA = torch.randn(N, N).double() \n",
    "dL = dA.mm(A.t()) + A.mm(dA.t())\n",
    "\n",
    "vals, vecs = torch.eig(L, eigenvectors=True)\n",
    "vals = vals[:,0]\n",
    "E = vals.expand(N,N) - vals.expand(N,N).t()\n",
    "F = 1 / (E + I) - I\n",
    "P = torch.inverse(vecs).mm(dL.mm(vecs))\n",
    "dvals = I * P\n",
    "dvecs = vecs.mm(F*P)\n",
    "\n",
    "# backward pass\n",
    "bvals = torch.randn(N).diag().double()\n",
    "bvecs = torch.randn(N,N).double()\n",
    "med = bvals + F * (vecs.t().mm(bvecs))\n",
    "bL = torch.inverse(vecs.t()).mm(med.mm(vecs.t()))\n",
    "bA = bL.mm(A) + bL.t().mm(A)\n",
    "\n",
    "# check\n",
    "torch.sum(bA * dA) - torch.sum(dvals * bvals) - torch.sum(dvecs * bvecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.552713678800501e-15"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# essentials for svd in PyTorch-\n",
    "\n",
    "import torch\n",
    "\n",
    "M = 6\n",
    "N = 5\n",
    "A = torch.randn(M,N).double()\n",
    "dA = torch.randn(M, N).double() \n",
    "dL = dA.mm(A.t()) + A.mm(dA.t())\n",
    "\n",
    "vecs, vals, v = torch.svd(A)\n",
    "vals = vals**2\n",
    "s = vals.size(0)\n",
    "I = torch.eye(s).double()\n",
    "E = vals.expand(s,s) - vals.expand(s,s).t()\n",
    "F = 1 / (E + I) - I\n",
    "P = vecs.t().mm(dL.mm(vecs))\n",
    "dvals = I * P\n",
    "dvecs = vecs.mm(F*P)\n",
    "\n",
    "# backward pass\n",
    "bvals = torch.randn(s).diag().double()\n",
    "bvecs = torch.randn(vecs.size()).double()\n",
    "med = bvals + F * (vecs.t().mm(bvecs))\n",
    "bL = vecs.mm(med.mm(vecs.t()))\n",
    "bA = bL.mm(A) + bL.t().mm(A)\n",
    "\n",
    "# check\n",
    "torch.sum(bA * dA) - torch.sum(dvals * bvals) - torch.sum(dvecs * bvecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "from dpp_nets.my_torch.linalg import custom_decomp\n",
    "\n",
    "A = Variable(A, requires_grad=True)\n",
    "vals, vecs = custom_decomp()(A)\n",
    "torch.autograd.backward([vals, vecs],[bvals.diag(), bvecs])\n",
    "torch.sum(A.grad.data == bA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is an important comparison of my three gradient methods\n",
    "# DON'T DELETE!!\n",
    "# 1) Numpy function (Low-rank DPP-Factorization)\n",
    "# 2) Allinone = Numpy gradient in Pytorch\n",
    "# 3) Back-propagating through singular value decomposition\n",
    "\n",
    "import numpy as np\n",
    "from dpp_nets.dpp.score_dpp import score_dpp\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# 1) NUMPY GRADIENT (THIS IS CONFIRMED TO BE THE SAME AS LOW-RANK PAPER)\n",
    "embd = np.random.randn(10,10)\n",
    "subset = np.array([1,1,1,0,0,1,0,1,0,0], dtype=float)\n",
    "grad_numpy = score_dpp(embd, subset)\n",
    "print(grad_numpy)\n",
    "\n",
    "# 2) IN PYTORCH\n",
    "embd = torch.DoubleTensor(embd)\n",
    "vecs, vals, _ = torch.svd(embd)\n",
    "vals.pow_(2)\n",
    "subset = torch.DoubleTensor(subset)\n",
    "\n",
    "def backward(kernel, subset):\n",
    "    dtype = kernel.type()\n",
    "    n, kernel_dim = kernel.size()\n",
    "    subset_sum = subset.long().sum()   \n",
    "    grad_kernel = torch.zeros(kernel.size()).type(dtype)\n",
    "    \n",
    "    if subset_sum:\n",
    "        # auxillary\n",
    "        P = torch.eye(n).masked_select(subset.expand(n,n).t().byte()).view(subset_sum, -1).type(dtype)\n",
    "        subembd = P.mm(kernel)\n",
    "        submatrix = subembd.mm(subembd.t())\n",
    "        submatinv = torch.inverse(submatrix)\n",
    "        subgrad = 2 * submatinv.mm(subembd)\n",
    "        subgrad = P.t().mm(subgrad)\n",
    "        grad_kernel.add_(subgrad)\n",
    "\n",
    "        # Gradient from whole L matrix\n",
    "        K = kernel.t().mm(kernel) # not L!\n",
    "        I_k = torch.eye(kernel_dim).type(dtype)\n",
    "        I = torch.eye(n).type(dtype)\n",
    "        inv = torch.inverse(I_k + K)\n",
    "        B = I - kernel.mm(inv).mm(kernel.t())\n",
    "        grad_from_full = 2 * B.mm(kernel)\n",
    "        grad_kernel.sub_(grad_from_full)\n",
    "\n",
    "        return grad_kernel\n",
    "    \n",
    "grad_pytorch = backward(embd, subset)\n",
    "print(grad_pytorch)\n",
    "grad_numpy - grad_pytorch.numpy()\n",
    "\n",
    "# 3) GRADIENT WITH SVD BACKPROPGATION\n",
    "def backward_1(vals, vecs, subset):\n",
    "    # Set-up\n",
    "    dtype = vals.type()\n",
    "    n = vecs.size(0)\n",
    "    n_vals = vals.size(0)\n",
    "    subset_sum = subset.long().sum()\n",
    "\n",
    "    grad_vals = 1 / vals\n",
    "    grad_vecs = torch.zeros(n, n_vals).type(dtype)\n",
    "\n",
    "    if subset_sum:\n",
    "        # auxillary\n",
    "        matrix = vecs.mm(vals.diag()).mm(vecs.t())\n",
    "        P = torch.eye(n).masked_select(subset.expand(n,n).t().byte()).view(subset_sum, -1).type(dtype)\n",
    "        submatrix = P.mm(matrix).mm(P.t())\n",
    "        subinv = torch.inverse(submatrix)\n",
    "        Pvecs = P.mm(vecs)\n",
    "\n",
    "        grad_vals += Pvecs.t().mm(subinv).mm(Pvecs).diag()\n",
    "        grad_vecs += P.t().mm(subinv).mm(Pvecs).mm(vals.diag())    \n",
    "\n",
    "    return grad_vals, grad_vecs\n",
    "\n",
    "def backward_2(grad_vals, grad_vecs, mat, vals, vecs):\n",
    "    \n",
    "    # unpack\n",
    "    N = grad_vals.size(0)\n",
    "\n",
    "    # auxillary\n",
    "    I = grad_vecs.new(N,N).copy_(torch.eye(N))\n",
    "    F = vals.expand(N,N) - vals.expand(N,N).t()\n",
    "    F.add_(I).reciprocal_().sub_(I)\n",
    "\n",
    "    # gradient\n",
    "    grad_mat = grad_vals.diag() + F * (vecs.t().mm(grad_vecs)) # intermediate variable \n",
    "    grad_mat = vecs.mm(grad_mat.mm(vecs.t())) # this is the gradient wrt L\n",
    "    grad_mat = grad_mat.mm(mat) + grad_mat.t().mm(mat) # this is the grad wrt A\n",
    "\n",
    "    return grad_mat\n",
    "\n",
    "grad_vals, grad_vecs = backward_1(vals, vecs, subset)\n",
    "backward_2(grad_vals, grad_vecs, embd, vals, vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dpp_nets.my_torch.DPP import AllInOne\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "A = Variable(torch.randn(10,10).double(), requires_grad=True)\n",
    "transform = AllInOne()\n",
    "subset = transform(A)\n",
    "subset.reinforce(1)\n",
    "subset.sum().backward()\n",
    "print(A.grad.data)\n",
    "print(backward(A.data, subset.data)) # yes it works correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Marginal Trainer\n",
    "# Implement Gradient for Inverse\n",
    "# in order to check if my gradient is working correctly \n",
    "import torch\n",
    "from torch.autograd import Variable, Function\n",
    "from dpp_nets.my_torch.linalg import custom_decomp\n",
    "\n",
    "class var_inverse(Function):\n",
    "    \n",
    "    def forward(self, A):\n",
    "        C = torch.inverse(A)\n",
    "        self.save_for_backward(C)\n",
    "        return C\n",
    "    \n",
    "    def backward(self, grad_C):\n",
    "        C, = self.saved_tensors\n",
    "        grad_A = -C.t().mm(grad_C).mm(C.t())\n",
    "        return grad_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K is : Variable containing:\n",
      " 0.6022 -0.2172  0.0162 -0.1169\n",
      "-0.2172  0.5719 -0.0465 -0.0628\n",
      " 0.0162 -0.0465  0.8163  0.0278\n",
      "-0.1169 -0.0628  0.0278  0.7891\n",
      "[torch.DoubleTensor of size 4x4]\n",
      "\n",
      "K is:  Variable containing:\n",
      " 0.6022 -0.2172  0.0162 -0.1169\n",
      "-0.2172  0.5719 -0.0465 -0.0628\n",
      " 0.0162 -0.0465  0.8163  0.0278\n",
      "-0.1169 -0.0628  0.0278  0.7891\n",
      "[torch.DoubleTensor of size 4x4]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set-up \n",
    "# backpropgation through singular values works!\n",
    "\n",
    "M = 4\n",
    "N = 6\n",
    "data = torch.randn(M, N).double()\n",
    "identity = Variable(torch.eye(M).double())\n",
    "\n",
    "# short-cut (using SVD)\n",
    "Vs = Variable(data, requires_grad=True)\n",
    "vals, vecs = custom_decomp()(Vs)\n",
    "\n",
    "Ltemp = vecs.mm(vals.diag()).mm(vecs.t())\n",
    "K = identity - vecs.mm((1 / (vals +1)).diag()).mm(vecs.t())\n",
    "print(\"K is :\", K)\n",
    "#K.diag().backward(torch.arange(0,max(M,N)).double())\n",
    "#print(\"Grad is: \", Vs.grad.data)\n",
    "\n",
    "# conventional (long-way)\n",
    "Vl = Variable(data, requires_grad=True)\n",
    "L = Vl.mm(Vl.t())\n",
    "K = identity - var_inverse()((L + identity))\n",
    "print(\"K is: \", K)\n",
    "#K.diag().backward(torch.arange(0,max(M,N)).double())\n",
    "#print(\"Grad is: \", Vl.grad.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# even better short-cut (when only interested in marginals)\n",
    "Vss = Variable(data, requires_grad=True)\n",
    "vals, vecs = custom_decomp()(Vss)\n",
    "K = vecs.mm((1 / (vals +1)).diag()).mm(vecs.t()) # not really K\n",
    "marginals = (1 - K.diag()).diag()\n",
    "print(\"Marginals are: \", marginals)\n",
    "marginals.backward(torch.arange(0,max(M,N)).diag().double())\n",
    "print(\"Grad is:\", Vss.grad.data)\n",
    "\n",
    "# Deviation\n",
    "(Vss.grad.data - Vl.grad.data).abs().sum() // N**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable, Function\n",
    "from dpp_nets.my_torch.linalg import custom_decomp\n",
    "\n",
    "\n",
    "class new_custom_decomp(Function):\n",
    "    \"\"\"\n",
    "    This is a \"strange\" but useful decomposition. \n",
    "    It takes in a matrix A of size m x n. \n",
    "    It will return the eigenvectors (vecs) \n",
    "    and eigenvalues (vals) of the matrix \n",
    "    L = A.mm(A.t()), but without actually\n",
    "    computing L. \n",
    "    \n",
    "    Any square or rectangular shapes are allowed\n",
    "    for A. :-)\n",
    "    \"\"\"\n",
    "    def __init__(self, some=False):\n",
    "        self.some = some\n",
    "        \n",
    "    def forward(self, mat):\n",
    "        vecs, vals, _ = torch.svd(mat, self.some)\n",
    "        vals.pow_(2)\n",
    "        self.save_for_backward(mat, vals, vecs)\n",
    "        \n",
    "        return vals, vecs\n",
    "    \n",
    "    def backward(self, grad_vals, grad_vecs):\n",
    "        # unpack\n",
    "        mat, vals, vecs = self.saved_tensors \n",
    "        N = vals.size(0)\n",
    "        \n",
    "        # auxillary\n",
    "        I = grad_vecs.new(N,N).copy_(torch.eye(N))\n",
    "        F = vals.expand(N,N) - vals.expand(N,N).t()\n",
    "        F.add_(I).reciprocal_().sub_(I)\n",
    "\n",
    "        # gradient\n",
    "        grad_mat = grad_vals.diag() + F * (vecs.t().mm(grad_vecs)) # intermediate variable \n",
    "        grad_mat = vecs.mm(grad_mat.mm(vecs.t())) # this is the gradient wrt L\n",
    "        grad_mat = grad_mat.mm(mat) + grad_mat.t().mm(mat) # this is the grad wrt A\n",
    "        \n",
    "        return grad_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 6]) torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "M = 6\n",
    "N = 4\n",
    "A = torch.randn(M, N)\n",
    "L = A.mm(A.t())\n",
    "vecs, vals, _ = torch.svd(A, some=False)\n",
    "print(vecs.size(), vals.size())\n",
    "vals = vals**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 10.1708  -5.4160   0.0345   2.8779   1.4180   0.3497\n",
      " -5.4160   7.3209   3.2153  -4.6769  -1.2542  -0.5570\n",
      "  0.0345   3.2153   2.5564  -2.0274  -0.2828  -0.0499\n",
      "  2.8779  -4.6769  -2.0274   3.4030   0.9662   0.3621\n",
      "  1.4180  -1.2542  -0.2828   0.9662   0.6507  -0.7268\n",
      "  0.3497  -0.5570  -0.0499   0.3621  -0.7268   2.4910\n",
      "[torch.FloatTensor of size 6x6]\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [6 x 6], m2: [4 x 4] at /Users/soumith/miniconda2/conda-bld/pytorch_1493757319118/work/torch/lib/TH/generic/THTensorMath.c:1237",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-678005158231>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [6 x 6], m2: [4 x 4] at /Users/soumith/miniconda2/conda-bld/pytorch_1493757319118/work/torch/lib/TH/generic/THTensorMath.c:1237"
     ]
    }
   ],
   "source": [
    "print(L)\n",
    "print(vecs.mm(vals.diag()).mm(vecs.t()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
