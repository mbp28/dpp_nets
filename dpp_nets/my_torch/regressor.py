import dpp_nets.my_torch
import numpy as np 
import torch
import torch.nn as nn
import torch.optim as optim
from torch.autograd import Variable
from dpp_nets.my_torch.controlvar import compute_alpha
from dpp_nets.my_torch.linalg import my_svd

from collections import defaultdict

class DPPRegressor(nn.Module):
    
    def __init__(self, network_params, dtype):
        """
        Arguments:
        - network_params: see below for which parameters must specify
        - dtype: torch.DoubleTensor or torch.FloatTensor
        """
        super(DPPRegressor, self).__init__()
        
        # Read in parameters
        self.set_size = network_params['set_size'] # 40
        self.emb_in = network_params['emb_in']
        self.emb_h = network_params['emb_h']
        self.emb_out = network_params['emb_out']
        self.pred_in = network_params['pred_in'] 
        self.pred_h = network_params['pred_h']
        self.pred_out = network_params['pred_out']
        assert int(self.emb_in / 2) == self.pred_in
        self.dtype = dtype

        # Initialize Network
        self.emb_layer = torch.nn.Sequential(nn.Linear(self.emb_in, self.emb_h), nn.ELU(),
                                             nn.Linear(self.emb_h, self.emb_out))
        self.dpp_layer = dpp_nets.my_torch.DPPLayer(self.dtype)
        self.pred_layer = torch.nn.Sequential(nn.Linear(self.pred_in, self.pred_h), nn.ReLU(), 
                                                    nn.Linear(self.pred_h, self.pred_out))
        # Choose MSELoss as training criterion
        self.criterion = nn.MSELoss()

        # Useful intermediate variables 
        self.embedding = None
        self.subset = None
        self.pick = None
        self.pred = None

        self.type(self.dtype)

        # A varierty of convenience dictionaries
        # For alpha iteration
        self.alpha_dict = defaultdict(list)
        self.a_score_dict = defaultdict(list) # never delete
        self.a_reinforce_dict = defaultdict(list) # never delete
        self.a_loss_dict = defaultdict(list)

        # Gradients
        self.score_dict = defaultdict(list)
        self.reinforce_dict = defaultdict(list)
        self.control_dict = defaultdict(list)

        # Prediction & Loss
        self.embedding_dict = defaultdict(list)
        self.subset_dict = defaultdict(list)
        self.pred_dict = defaultdict(list)
        self.loss_dict = defaultdict(list)
        self.total_loss = defaultdict(list)

        # Network weights
        self.emb_w1_max = defaultdict(list)
        self.emb_w1_mean = defaultdict(list)
        self.emb_w2_max = defaultdict(list)
        self.emb_w2_mean = defaultdict(list)

        self.pred_w1_max = defaultdict(list)
        self.pred_w1_mean = defaultdict(list)
        self.pred_w2_max = defaultdict(list)
        self.pred_w2_mean = defaultdict(list)


    def forward(self, words, context):
        """
        words: Tensor of dimension [set_size, word_emb_dim]
        contexts: Tensor of dimension [set_size, word_emb_dim]
        The rows of x2 are all identical and equal the sum
        across rows of x1 (context to predict DPP embedding)
        self.emb_dim must be 2 * word_emb_dim
        """
        # Concatenate individual words and set context
        x = torch.cat([words, context], dim = 1)

        # Compute embedding of DPP kernel
        self.embedding = self.emb_layer(x)

        # Sample a subset of words from the DPP
        self.subset = torch.diag(self.dpp_layer(self.embedding))
        
        # Filter out the selected words and combine them
        self.pick = self.subset.mm(words).sum(0)

        # Compute a prediction based on the sampled words
        self.pred = self.pred_layer(self.pick)

        return self.pred

    def generate(self):
        """
        Each training instance consists of a set of words (2D array) whose words come from a random
        number (between 1 and 20) of different clusters. In total, there exist self.pred_in / GLUE 
        different clusters. Each cluster contains standard random normal noise in most dimensions,
        except for GLUE dimensions, in which its entries are generated by a normal distribution around
        50. These signal dimensions differ across all clusters. 
        """
        GLUE = 5
        SIGNAL = 50

        words = np.random.randn(self.set_size, self.pred_in)

        # Sample a number of clusters (between 1 and 20) present in training instance
        n_clusters = 1 + np.random.choice(20,1) 
        # Will repeat cluster indices to fill upto set_size
        rep = (self.set_size // n_clusters) + 1 

        # Sample cluster indices 
        clusters = np.random.choice(self.pred_in // GLUE, n_clusters, replace=False)

        # Find column indices associated with cluster indices
        col_idx = np.array([np.arange(i*GLUE, i*GLUE + GLUE) for i in clusters]).flatten()
        # Repeat indices to fill upto set_size 
        col_idx = np.tile(col_idx, rep)[:(self.set_size * GLUE)]

        # Overwrite training data with signal according to column indices
        words[np.repeat(np.arange(40), GLUE), col_idx] = np.random.normal(SIGNAL, 1, (self.set_size * GLUE))

        # Create context 
        context = np.tile(np.sum(words, axis=0), (self.set_size, 1))

        # Shuffle, so it doesn't learn to always choose the first item for example.
        np.random.shuffle(words)

        # Wrap into Variables 
        words = Variable(self.dtype(words))
        context = Variable(self.dtype(context))
        target = Variable(self.dtype(n_clusters.astype(np.float64)))

        return words, context, target 

    def sample(self):
        """
        Demonstrates how Network is currently performing
        by classifying a random instance 
        """

        # Sample
        words, context, target = self.generate()

        # Assigns prediction to self.pred
        self.forward(words, context)

        # Print a couple of messages
        print("Target sampled was: ", target.data[0])
        print("Network predicted: ", self.pred.data[0])
        print("Resulting loss is: ", self.criterion(self.pred, target))
        print("Prediction was based on ", self.subset.data.sum(), "observations.")

        return (self.pred, target), (words, context)
            
    def train_with_baseline(self, train_iter, batch_size, sample_iter, alpha_iter, lr, weight_decay, reg_exp, reg_var, overwrite=0):
        
        # Clear dictionaries
        # For alpha iteration
        self.alpha_dict.clear()
        self.a_score_dict.clear()
        self.a_reinforce_dict.clear()
        self.a_loss_dict.clear()

        # Gradients
        self.score_dict.clear()
        self.reinforce_dict.clear()
        self.control_dict.clear()

        # Prediction & Loss
        self.embedding_dict.clear()
        self.subset_dict.clear()
        self.pred_dict.clear()
        self.loss_dict.clear()
        self.total_loss.clear()

        # Network weights
        self.emb_w1_max.clear()
        self.emb_w1_mean.clear()
        self.emb_w2_max.clear()
        self.emb_w2_mean.clear()

        self.pred_w1_max.clear()
        self.pred_w1_mean.clear()
        self.pred_w2_max.clear()
        self.pred_w2_mean.clear()

        # Prepare Optimizer
        optimizer = optim.SGD([{'params': self.emb_layer.parameters()},
                               {'params': self.pred_layer.parameters(),
                               'weight_decay': weight_decay * batch_size * sample_iter}], 
                              lr = lr / (batch_size * sample_iter))
        

        for t in range(train_iter):

            # Draw a Training Sample
            words, context, target = self.generate()
            
            # Save current weights
            self.emb_w1_max[t].append(self.dtype([torch.max(torch.abs(self.emb_layer[0].weight.data))]))
            self.emb_w1_mean[t].append(self.dtype([torch.mean(torch.abs(self.emb_layer[0].weight.data))]))
            self.emb_w2_max[t].append(self.dtype([torch.max(torch.abs(self.emb_layer[2].weight.data))]))
            self.emb_w2_mean[t].append(self.dtype([torch.mean(torch.abs(self.emb_layer[2].weight.data))]))

            self.pred_w1_max[t].append(self.dtype([torch.max(torch.abs(self.pred_layer[0].weight.data))]))
            self.pred_w1_mean[t].append(self.dtype([torch.mean(torch.abs(self.pred_layer[0].weight.data))]))
            self.pred_w2_max[t].append(self.dtype([torch.max(torch.abs(self.pred_layer[2].weight.data))]))
            self.pred_w2_mean[t].append(self.dtype([torch.mean(torch.abs(self.pred_layer[2].weight.data))]))

            # Estimate alpha
            # Save score, build reinforce gradient, save reinforce gradient
            save_score = self.dpp_layer.register_backward_hook(lambda module, grad_in, grad_out: self.a_score_dict[t].append(grad_in[0].data))
            reinforce_grad = self.dpp_layer.register_backward_hook(lambda module, grad_in, grad_out: (grad_in[0] * (loss.data[0]),))
            save_reinforce = self.dpp_layer.register_backward_hook(lambda module, grad_in, grad_out: self.a_reinforce_dict[t].append(grad_in[0].data))
            
            if alpha_iter:
                for i in range(alpha_iter):                                      
                    self.forward(words, context)
                    loss = self.criterion(self.pred, target)
                    self.a_loss_dict[t].append(loss.data)
                    loss.backward()

                self.alpha = compute_alpha(self.a_reinforce_dict[t], self.a_score_dict[t], False, False, False).type(self.dtype)
                self.zero_grad()
                self.alpha_dict[t].append(self.alpha)
                
            else:
                self.alpha = overwrite * torch.ones(self.embedding.size()).type(self.dtype)
                    
            save_score.remove()
            reinforce_grad.remove()
            save_reinforce.remove()
            
            # now actual training
            # save scores, reinforce, implement baseline gradient, save baseline gradient
            save_score = self.dpp_layer.register_backward_hook(lambda module, grad_in, grad_out: self.score_dict[t].append(grad_in[0].data))
            save_reinforce = self.dpp_layer.register_backward_hook(lambda module, grad_in, grad_out: self.reinforce_dict[t].append(grad_in[0].data *  loss.data[0]))
            modify_grad = self.dpp_layer.register_backward_hook(lambda module, grad_in, grad_out: (Variable(grad_in[0].data * (loss.data[0] - self.alpha)),))
            save_control = self.dpp_layer.register_backward_hook(lambda module, grad_in, grad_out: self.control_dict[t].append(grad_in[0].data))

            # sample multiple times from the DPP and backpropagate associated gradients!
            for i in range(sample_iter):
                self.forward(words, context)
                loss = self.criterion(self.pred, target)

                self.embedding_dict[t].append(self.embedding.data)
                self.subset_dict[t].append(self.subset.data)
                self.pred_dict[t].append(self.pred.data)
                self.loss_dict[t].append(loss.data) # save_loss

                if not reg_exp and not reg_var:
                    loss.backward()

                else: 
                    TRUE_MEAN = 10.5
                    _, s, _ = my_svd()(self.embedding)
                    exp = torch.sum(s**2 / (s**2 + 1))
                    var = exp - torch.sum(s**4 / ((s**2 + 1) **2))
                    reg_loss = reg_exp * ((exp - TRUE_MEAN)**2) + reg_var * var
                    total_loss = loss + reg_loss
                    self.total_loss[t].append(total_loss)
                    total_loss.backward()

            # update parameters after processing a batch
            if (t + 1) % batch_size == 0:
                optimizer.step()
                optimizer.zero_grad()

            # print loss
            if (t + 1) % 50 == 0:
                print(t + 1, loss.data[0])
                
            save_score.remove()
            save_reinforce.remove()
            modify_grad.remove()
            save_control.remove()

    def evaluate(self, test_iter):

        loss_sum = 0.0
        subset_mean = 0.0
        temp = 0.0
        
        for t in range(test_iter):
            words, context, target = self.generate()
            self.forward(words, context)
            loss_sum += self.criterion(self.pred, target)

            # Subset Statistics
            delta = self.subset.data.sum() - subset_mean
            subset_mean += delta / (t+1)
            delta2 = delta / (t+1)
            temp += delta * delta2
            
        subset_var = temp / (test_iter - 1)
        loss_av = loss_sum / test_iter

        print("Average Loss is: ", loss_av) 
        print("Average Subset Size is: ", subset_mean)
        print("Subset Variance is: ", subset_var)

        return loss_av, subset_mean, subset_var
        
    def reset_parameter(self):
        self.emb_layer[0].reset_parameters()
        self.emb_layer[2].reset_parameters()
        self.pred_layer[0].reset_parameters()
        self.pred_layer[2].reset_parameters()
        
        self.optimizer = optim.SGD([{'params': self.emb_layer.parameters(), 'weight_decay': self.reg_kern},
                                    {'params': self.pred_layer.parameters()}],
                                    lr = self.lr / self.batch_size)
