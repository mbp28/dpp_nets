{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The old notebook became too confusing!\n",
    "\n",
    "# Trying to check Singular Values and Singular Vectors in Numpy\n",
    " \n",
    "import numpy as np\n",
    "\n",
    "# General Set-up\n",
    "N = 2\n",
    "A = 0.1 * np.random.randn(N, N) + np.diag(np.arange(1, N+1))\n",
    "B = np.random.randn(N, N)\n",
    "I = np.eye(N)\n",
    "dA = np.random.randn(N, N)\n",
    "dB = np.random.randn(N, N)\n",
    "bC = np.random.randn(N, N)\n",
    "eps = 1e-20\n",
    "epsi = 1 / eps\n",
    "Ae = A + 1j*eps*dA\n",
    "Be = B + 1j*eps*dB # don't need it, I'm working with single input.\n",
    "\n",
    "# SVD Set-up\n",
    "u, s, vT = np.linalg.svd(A)\n",
    "\n",
    "# Complex part\n",
    "De, Du = np.linalg.eig(Ae.dot(Ae.T))\n",
    "Du, _, _ = np.linalg.svd(Ae)\n",
    "idx = De.argsort()[::-1]   \n",
    "De = De[idx]\n",
    "Du = u[:,idx]\n",
    "D = np.real(De) # only needed for what? \n",
    "\n",
    "# forward propagation \n",
    "dA = dA\n",
    "dS = np.diag(I * u.T.dot(dA).dot(vT.T))\n",
    "\n",
    "E = np.outer(np.ones(N), s) - np.outer(s, np.ones(N))\n",
    "F = 1 / (E + np.eye(N)) - np.eye(N)\n",
    "dU = u.dot(F * (u.T.dot(dA).dot(vT.T).dot(np.diag(s)) + np.diag(s).dot(vT).dot(dA.T).dot(u)))\n",
    "\n",
    "# backward gradients\n",
    "bS = np.random.randn(N) # gradients wrt singular values\n",
    "bA = u.dot(np.diag(bS)).dot(vT) # backpropagated gradient wrt to matrix A\n",
    "\n",
    "print('singular value')\n",
    "print('svd error: ', (np.linalg.norm(s-np.sqrt(D))))\n",
    "\n",
    "# Forward Check based on complex matrices\n",
    "print('CVT error (vals): ', (np.linalg.norm(2*s*dS - epsi*np.imag(De))))\n",
    "print('CVT error (vecs): ', (np.linalg.norm(dU - epsi*np.imag(Du))))\n",
    "\n",
    "# Backward Check, these are essentially two traces!!\n",
    "print('adj error: ',np.sum(dA*bA)-np.sum(dS*bS))\n",
    "\n",
    "# I should be able to use the same identity for my testing purposes!!!\n",
    "# trace(bC.T * dC) = trace(bA.T * dA) + trace(bB.T * dB)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Checking eigenvectors and eigenvalues in Numpy\n",
    "np.random.seed(10)\n",
    "# General Set-up\n",
    "N = 5\n",
    "A = 0.1 * np.random.randn(N, N) + np.diag(np.arange(1, N+1))\n",
    "#A = A.dot(A.T)\n",
    "B = np.random.randn(N, N)\n",
    "I = np.eye(N)\n",
    "dA = np.random.randn(N, N)\n",
    "dB = np.random.randn(N, N)\n",
    "bC = np.random.randn(N, N)\n",
    "eps = 1e-20\n",
    "epsi = 1 / eps\n",
    "Ae = A + 1j*eps*dA\n",
    "Be = B + 1j*eps*dB\n",
    "\n",
    "# EIGEN Set-up\n",
    "De, Ue = np.linalg.eig(Ae)\n",
    "D = np.real(De)\n",
    "U = np.real(Ue)\n",
    "\n",
    "# make dC diagonal equal to zero\n",
    "Ue = Ue.dot(np.diag(1 / np.diag(np.linalg.inv(U).dot(Ue))))\n",
    "E = np.outer(np.ones(N), D) - np.outer(D, np.ones(N))\n",
    "F = 1 / (E + np.eye(N)) - np.eye(N)\n",
    "P = np.linalg.inv(U).dot(dA.dot(U))\n",
    "dD = np.eye(N) * P\n",
    "dU = U.dot(F*P)\n",
    "#dD, dU = forward(A, dA)\n",
    "\n",
    "bD = np.diag(np.random.randn(N)) # random perturbation of eigenvalues\n",
    "bU = np.random.randn(N,N) # random perturbation of eigenvectors\n",
    "bD = bD + F * (U.T.dot(bU))\n",
    "bA = np.linalg.inv(U.T).dot(bD.dot(U.T))\n",
    "print('eigenvalues and eigenvectors')\n",
    "print('CVT error (vals): ', np.linalg.norm(np.diag(dD) - epsi*np.imag(De)))\n",
    "print('CVT error (vecs): ', np.linalg.norm(dU - epsi*np.imag(Ue)))\n",
    "print('adj error (backward): ', np.sum(dA*bA)-np.sum(dD*bD)-np.sum(dU*bU))\n",
    "#print('CVT error (vecs): ', np.linalg.norm(np.abs(dU) - np.abs(epsi*np.imag(Ue))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# essentials for eig in PyTorch\n",
    "import torch\n",
    "\n",
    "N = 5\n",
    "A = torch.randn(N,N).double()\n",
    "L = A.mm(A.t())\n",
    "I = torch.eye(N).double()\n",
    "dA = torch.randn(N, N).double() \n",
    "dL = dA.mm(A.t()) + A.mm(dA.t())\n",
    "\n",
    "vals, vecs = torch.eig(L, eigenvectors=True)\n",
    "vals = vals[:,0]\n",
    "E = vals.expand(N,N) - vals.expand(N,N).t()\n",
    "F = 1 / (E + I) - I\n",
    "P = torch.inverse(vecs).mm(dL.mm(vecs))\n",
    "dvals = I * P\n",
    "dvecs = vecs.mm(F*P)\n",
    "\n",
    "# backward pass\n",
    "bvals = torch.randn(N).diag().double()\n",
    "bvecs = torch.randn(N,N).double()\n",
    "med = bvals + F * (vecs.t().mm(bvecs))\n",
    "bL = torch.inverse(vecs.t()).mm(med.mm(vecs.t()))\n",
    "bA = bL.mm(A) + bL.t().mm(A)\n",
    "\n",
    "# check\n",
    "torch.sum(bA * dA) - torch.sum(dvals * bvals) - torch.sum(dvecs * bvecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.881784197001252e-16"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# essentials for svd in PyTorch\n",
    "\n",
    "import torch\n",
    "\n",
    "M = 6\n",
    "N = 5\n",
    "A = torch.randn(M,N).double()\n",
    "dA = torch.randn(M, N).double() \n",
    "dL = dA.mm(A.t()) + A.mm(dA.t())\n",
    "\n",
    "vecs, vals, v = torch.svd(A)\n",
    "vals = vals**2\n",
    "s = vals.size(0)\n",
    "I = torch.eye(s).double()\n",
    "E = vals.expand(s,s) - vals.expand(s,s).t()\n",
    "F = 1 / (E + I) - I\n",
    "P = vecs.t().mm(dL.mm(vecs))\n",
    "dvals = I * P\n",
    "dvecs = vecs.mm(F*P)\n",
    "\n",
    "# backward pass\n",
    "bvals = torch.randn(s).diag().double()\n",
    "bvecs = torch.randn(vecs.size()).double()\n",
    "med = bvals + F * (vecs.t().mm(bvecs))\n",
    "bL = vecs.mm(med.mm(vecs.t()))\n",
    "bA = bL.mm(A) + bL.t().mm(A)\n",
    "\n",
    "# check\n",
    "torch.sum(bA * dA) - torch.sum(dvals * bvals) - torch.sum(dvecs * bvecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "from dpp_nets.my_torch.linalg import custom_decomp\n",
    "\n",
    "A = Variable(A, requires_grad=True)\n",
    "vals, vecs = custom_decomp()(A)\n",
    "torch.autograd.backward([vals, vecs],[bvals.diag(), bvecs])\n",
    "torch.sum(A.grad.data == bA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.07959999  0.25817122 -0.19481387 -0.15406223 -0.16289138  0.01301727\n",
      "   0.16096651  0.0220506  -0.08959448 -0.00648537]\n",
      " [ 0.02754139 -0.28771102 -0.1345081   0.14588997  0.09026412  0.1621723\n",
      "  -0.09410064 -0.20756491  0.12806961  0.05571745]\n",
      " [ 0.07758697 -0.00868127 -0.04071899  0.03899944  0.05223629  0.10904184\n",
      "   0.06110153 -0.04770102 -0.04257     0.01801158]\n",
      " [ 0.09265008  0.44131825 -0.09311421  0.21475978 -0.04605407  0.29581014\n",
      "   0.18095795  0.01527339 -0.41104857  0.11346841]\n",
      " [-0.28859447 -0.04535395  0.17998769  0.00837501 -0.10116249  0.07497653\n",
      "  -0.21610942 -0.08198101  0.14867388  0.06233488]\n",
      " [ 0.10931137 -0.23823863 -0.01191077  0.13401147  0.21430319 -0.14084492\n",
      "  -0.00454534 -0.0875937   0.01457272  0.07468935]\n",
      " [-0.13818105  0.35771056  0.30251379 -0.34316076 -0.01488551 -0.10757703\n",
      "   0.13376659  0.19863658 -0.05054521 -0.12329977]\n",
      " [-0.47896153  0.24615582  0.4477023   0.11222805 -0.27169548  0.73876828\n",
      "  -0.27767155 -0.14787817  0.11213935 -0.05816795]\n",
      " [-0.06811473 -0.17751073 -0.05023657 -0.30421004 -0.18561715  0.29518447\n",
      "  -0.12190253 -0.14345931  0.26646354 -0.18092848]\n",
      " [ 0.19522751 -0.05706623  0.20862322  0.09499316  0.19747271  0.29927807\n",
      "   0.05267934 -0.11260283 -0.047591   -0.09572257]]\n",
      "\n",
      " 0.0796  0.2582 -0.1948 -0.1541 -0.1629  0.0130  0.1610  0.0221 -0.0896 -0.0065\n",
      " 0.0275 -0.2877 -0.1345  0.1459  0.0903  0.1622 -0.0941 -0.2076  0.1281  0.0557\n",
      " 0.0776 -0.0087 -0.0407  0.0390  0.0522  0.1090  0.0611 -0.0477 -0.0426  0.0180\n",
      " 0.0927  0.4413 -0.0931  0.2148 -0.0461  0.2958  0.1810  0.0153 -0.4110  0.1135\n",
      "-0.2886 -0.0454  0.1800  0.0084 -0.1012  0.0750 -0.2161 -0.0820  0.1487  0.0623\n",
      " 0.1093 -0.2382 -0.0119  0.1340  0.2143 -0.1408 -0.0045 -0.0876  0.0146  0.0747\n",
      "-0.1382  0.3577  0.3025 -0.3432 -0.0149 -0.1076  0.1338  0.1986 -0.0505 -0.1233\n",
      "-0.4790  0.2462  0.4477  0.1122 -0.2717  0.7388 -0.2777 -0.1479  0.1121 -0.0582\n",
      "-0.0681 -0.1775 -0.0502 -0.3042 -0.1856  0.2952 -0.1219 -0.1435  0.2665 -0.1809\n",
      " 0.1952 -0.0571  0.2086  0.0950  0.1975  0.2993  0.0527 -0.1126 -0.0476 -0.0957\n",
      "[torch.DoubleTensor of size 10x10]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       " 4.7776  2.8190  2.1331 -0.8737  0.8512 -1.1663 -2.4844 -1.5478  3.3371  4.3282\n",
       "-0.0652  1.0424  0.2123 -0.1905  1.2930 -0.2799  0.3043 -0.0448  0.6142  0.5766\n",
       "-0.2118 -0.2637 -0.0110 -0.1900  0.0306 -0.0095  0.5831  0.2051  0.2713  0.6186\n",
       " 2.0373  0.5967  1.6130 -0.7798  0.7193 -0.7092 -1.7605 -0.5905  1.9940  1.8754\n",
       "-4.0989 -2.7947 -3.0468  2.3234 -0.9062 -0.1409  4.1750  1.9738 -2.5007 -5.2712\n",
       "-3.5963 -2.0724 -2.6797 -0.2316 -1.1396  0.2749  2.4470 -0.1368 -2.7777 -2.7117\n",
       "-3.9442 -3.3244 -3.3512  1.8701 -1.3596  0.4961  2.7915  0.9186 -2.2602 -4.1464\n",
       "-6.1477 -3.7241 -2.1425  5.2741 -0.7764 -0.3859  6.0467  3.5210 -1.6018 -8.4349\n",
       " 3.3071  2.4413  2.8323  0.9707  1.9313 -1.4308 -1.1220  0.1965  2.0630  2.5706\n",
       "-7.4335 -4.5150 -5.2891  2.1042 -2.5529  0.1610  5.1171  2.2295 -4.0832 -7.1519\n",
       "[torch.DoubleTensor of size 10x10]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is an important comparison of my three gradient methods\n",
    "# DON'T DELETE!!\n",
    "# 1) Numpy function (Low-rank DPP-Factorization)\n",
    "# 2) Allinone = Numpy gradient in Pytorch\n",
    "# 3) Back-propagating through singular value decomposition\n",
    "\n",
    "import numpy as np\n",
    "from dpp_nets.dpp.score_dpp import score_dpp\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# 1) NUMPY GRADIENT (THIS IS CONFIRMED TO BE THE SAME AS LOW-RANK PAPER)\n",
    "embd = np.random.randn(10,10)\n",
    "subset = np.array([1,1,1,0,0,1,0,1,0,0], dtype=float)\n",
    "grad_numpy = score_dpp(embd, subset)\n",
    "print(grad_numpy)\n",
    "\n",
    "# 2) IN PYTORCH\n",
    "embd = torch.DoubleTensor(embd)\n",
    "vecs, vals, _ = torch.svd(embd)\n",
    "vals.pow_(2)\n",
    "subset = torch.DoubleTensor(subset)\n",
    "\n",
    "def backward(kernel, subset):\n",
    "    dtype = kernel.type()\n",
    "    n, kernel_dim = kernel.size()\n",
    "    subset_sum = subset.long().sum()   \n",
    "    grad_kernel = torch.zeros(kernel.size()).type(dtype)\n",
    "    \n",
    "    if subset_sum:\n",
    "        # auxillary\n",
    "        P = torch.eye(n).masked_select(subset.expand(n,n).t().byte()).view(subset_sum, -1).type(dtype)\n",
    "        subembd = P.mm(kernel)\n",
    "        submatrix = subembd.mm(subembd.t())\n",
    "        submatinv = torch.inverse(submatrix)\n",
    "        subgrad = 2 * submatinv.mm(subembd)\n",
    "        subgrad = P.t().mm(subgrad)\n",
    "        grad_kernel.add_(subgrad)\n",
    "\n",
    "        # Gradient from whole L matrix\n",
    "        K = kernel.t().mm(kernel) # not L!\n",
    "        I_k = torch.eye(kernel_dim).type(dtype)\n",
    "        I = torch.eye(n).type(dtype)\n",
    "        inv = torch.inverse(I_k + K)\n",
    "        B = I - kernel.mm(inv).mm(kernel.t())\n",
    "        grad_from_full = 2 * B.mm(kernel)\n",
    "        grad_kernel.sub_(grad_from_full)\n",
    "\n",
    "        return grad_kernel\n",
    "    \n",
    "grad_pytorch = backward(embd, subset)\n",
    "print(grad_pytorch)\n",
    "grad_numpy - grad_pytorch.numpy()\n",
    "\n",
    "# 3) GRADIENT WITH SVD BACKPROPGATION\n",
    "def backward_1(vals, vecs, subset):\n",
    "    # Set-up\n",
    "    dtype = vals.type()\n",
    "    n = vecs.size(0)\n",
    "    n_vals = vals.size(0)\n",
    "    subset_sum = subset.long().sum()\n",
    "\n",
    "    grad_vals = 1 / vals\n",
    "    grad_vecs = torch.zeros(n, n_vals).type(dtype)\n",
    "\n",
    "    if subset_sum:\n",
    "        # auxillary\n",
    "        matrix = vecs.mm(vals.diag()).mm(vecs.t())\n",
    "        P = torch.eye(n).masked_select(subset.expand(n,n).t().byte()).view(subset_sum, -1).type(dtype)\n",
    "        submatrix = P.mm(matrix).mm(P.t())\n",
    "        subinv = torch.inverse(submatrix)\n",
    "        Pvecs = P.mm(vecs)\n",
    "\n",
    "        grad_vals += Pvecs.t().mm(subinv).mm(Pvecs).diag()\n",
    "        grad_vecs += P.t().mm(subinv).mm(Pvecs).mm(vals.diag())    \n",
    "\n",
    "    return grad_vals, grad_vecs\n",
    "\n",
    "def backward_2(grad_vals, grad_vecs, mat, vals, vecs):\n",
    "    \n",
    "    # unpack\n",
    "    N = grad_vals.size(0)\n",
    "\n",
    "    # auxillary\n",
    "    I = grad_vecs.new(N,N).copy_(torch.eye(N))\n",
    "    F = vals.expand(N,N) - vals.expand(N,N).t()\n",
    "    F.add_(I).reciprocal_().sub_(I)\n",
    "\n",
    "    # gradient\n",
    "    grad_mat = grad_vals.diag() + F * (vecs.t().mm(grad_vecs)) # intermediate variable \n",
    "    grad_mat = vecs.mm(grad_mat.mm(vecs.t())) # this is the gradient wrt L\n",
    "    grad_mat = grad_mat.mm(mat) + grad_mat.t().mm(mat) # this is the grad wrt A\n",
    "\n",
    "    return grad_mat\n",
    "\n",
    "grad_vals, grad_vecs = backward_1(vals, vecs, subset)\n",
    "backward_2(grad_vals, grad_vecs, embd, vals, vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-0.1404 -0.2140 -0.6279 -0.2307 -0.3461 -0.1435 -0.2747 -0.2016  0.0810  0.1691\n",
      " 0.0710  0.1794 -0.0790  0.4298  0.2080 -0.0452 -0.0815 -0.4112 -0.0169  0.0369\n",
      " 0.4549  0.0554  0.0674  0.3554  0.0527 -0.3100 -0.2111  0.0662  0.3635  0.1317\n",
      " 0.2941  0.2242 -0.1612  0.1711  0.1192 -0.0345 -0.1886 -0.0133  0.0559  0.3058\n",
      " 0.5857  0.2459 -0.0550  0.2074 -0.0974 -0.3987 -0.2606  0.3966  0.3680  0.1667\n",
      "-0.2933 -0.0418 -0.1523  0.0903  0.1544  0.1999  0.1415 -0.4876 -0.3196  0.0456\n",
      " 0.4238  0.2725  0.0835 -0.0598 -0.1698 -0.2233 -0.2440  0.5445  0.2523  0.1888\n",
      "-0.8116 -0.1312 -0.4818 -0.0871  0.2688  0.6591  0.2495 -1.1016 -0.6981  0.0620\n",
      " 0.5141  0.1618 -0.0368 -0.1134 -0.2043 -0.3357 -0.3380  0.6602  0.4206  0.2106\n",
      "-0.3572 -0.2604  0.0867 -0.2916  0.2837  0.3730  0.1291 -0.2512 -0.2077 -0.0230\n",
      "[torch.DoubleTensor of size 10x10]\n",
      "\n",
      "\n",
      "-0.1404 -0.2140 -0.6279 -0.2307 -0.3461 -0.1435 -0.2747 -0.2016  0.0810  0.1691\n",
      " 0.0710  0.1794 -0.0790  0.4298  0.2080 -0.0452 -0.0815 -0.4112 -0.0169  0.0369\n",
      " 0.4549  0.0554  0.0674  0.3554  0.0527 -0.3100 -0.2111  0.0662  0.3635  0.1317\n",
      " 0.2941  0.2242 -0.1612  0.1711  0.1192 -0.0345 -0.1886 -0.0133  0.0559  0.3058\n",
      " 0.5857  0.2459 -0.0550  0.2074 -0.0974 -0.3987 -0.2606  0.3966  0.3680  0.1667\n",
      "-0.2933 -0.0418 -0.1523  0.0903  0.1544  0.1999  0.1415 -0.4876 -0.3196  0.0456\n",
      " 0.4238  0.2725  0.0835 -0.0598 -0.1698 -0.2233 -0.2440  0.5445  0.2523  0.1888\n",
      "-0.8116 -0.1312 -0.4818 -0.0871  0.2688  0.6591  0.2495 -1.1016 -0.6981  0.0620\n",
      " 0.5141  0.1618 -0.0368 -0.1134 -0.2043 -0.3357 -0.3380  0.6602  0.4206  0.2106\n",
      "-0.3572 -0.2604  0.0867 -0.2916  0.2837  0.3730  0.1291 -0.2512 -0.2077 -0.0230\n",
      "[torch.DoubleTensor of size 10x10]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from dpp_nets.my_torch.DPP import AllInOne\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "A = Variable(torch.randn(10,10).double(), requires_grad=True)\n",
    "transform = AllInOne()\n",
    "subset = transform(A)\n",
    "subset.reinforce(1)\n",
    "subset.sum().backward()\n",
    "print(A.grad.data)\n",
    "print(backward(A.data, subset.data)) # yes it works correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
