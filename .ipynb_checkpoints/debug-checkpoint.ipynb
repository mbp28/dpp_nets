{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Function\n",
    "from torch.autograd import StochasticFunction\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import dpp_nets.dpp as dpp\n",
    "from dpp_nets.my_torch.utilities import omit_slice\n",
    "from dpp_nets.my_torch.utilities import orthogonalize\n",
    "\n",
    "\n",
    "class AllInOne(StochasticFunction):\n",
    "    \n",
    "    def forward(self, kernel):\n",
    "        self.dtype = kernel.type()\n",
    "        vecs, vals, _ = torch.svd(kernel)\n",
    "        vals.pow_(2)\n",
    "\n",
    "        # Sometimes orthogonalization fails (i.e. deletes vectors)\n",
    "        # In that case just retry!\n",
    "        while True:\n",
    "            try:\n",
    "                # Set-up\n",
    "                n = vecs.size(0)\n",
    "                n_vals = vals.size(0)\n",
    "\n",
    "                # Sample a set size\n",
    "                index = (vals / (vals + 1)).bernoulli().byte()\n",
    "                k = torch.sum(index)\n",
    "\n",
    "                # Check for empty set\n",
    "                if not k:\n",
    "                    subset = vals.new().resize_(n).copy_(torch.zeros(n))\n",
    "                    self.save_for_backward(kernel, subset) \n",
    "                    return subset\n",
    "                \n",
    "                # Check for full set\n",
    "                if k == n:\n",
    "                    subset =  vals.new().resize_(n).copy_(torch.ones(n))\n",
    "                    self.save_for_backward(kernel, subset) \n",
    "                    return subset\n",
    "\n",
    "                # Sample a subset\n",
    "                V = vecs[index.expand_as(vecs)].view(n, -1)\n",
    "                subset = vals.new().resize_(n).copy_(torch.zeros(n))\n",
    "                \n",
    "                while subset.sum() < k:\n",
    "\n",
    "                    # Sample an item\n",
    "                    probs = V.pow(2).sum(1).t()\n",
    "                    item = probs.multinomial(1)[0,0]\n",
    "                    subset[item] = 1\n",
    "                    \n",
    "                    # CHeck if we got k items now\n",
    "                    if subset.sum() == k:\n",
    "                        break\n",
    "\n",
    "                    # Choose eigenvector to eliminate\n",
    "                    j = V[item, ].abs().sign().unsqueeze(1).t().multinomial(1)[0,0]\n",
    "                    Vj = V[:, j]\n",
    "                    \n",
    "                    # Update vector basis\n",
    "                    V = omit_slice(V,1,j)\n",
    "                    V.sub_(Vj.ger(V[item, :] / Vj[item]))\n",
    "\n",
    "                    # Orthogonalize vector basis\n",
    "                    V, _ = torch.qr(V)\n",
    "\n",
    "            except RuntimeError:\n",
    "                print(\"RuntimeError\")\n",
    "                continue\n",
    "            break\n",
    "        \n",
    "        return subset\n",
    "        \n",
    "    def backward(self, reward):\n",
    "        #TODO: Need to check this!\n",
    "        # Checked it! Looks good.\n",
    "\n",
    "        # Set-up\n",
    "        kernel, subset = self.kernel, self.subset\n",
    "        dtype = self.dtype\n",
    "\n",
    "        n, kernel_dim = kernel.size()\n",
    "        subset_sum = subset.long().sum()   \n",
    "        grad_kernel = torch.zeros(kernel.size()).type(dtype)\n",
    "\n",
    "        if subset_sum:\n",
    "            # auxillary\n",
    "            P = torch.eye(n).masked_select(subset.expand(n,n).t().byte()).view(subset_sum, -1).type(dtype)\n",
    "            subembd = P.mm(kernel)\n",
    "            submatrix = subembd.mm(subembd.t())\n",
    "            submatinv = torch.inverse(submatrix)\n",
    "            subgrad = 2 * submatinv.mm(subembd)\n",
    "            subgrad = P.t().mm(subgrad)\n",
    "            grad_kernel.add_(subgrad)\n",
    "        \n",
    "        # Gradient from whole L matrix\n",
    "        K = kernel.t().mm(kernel) # not L!\n",
    "        I_k = torch.eye(kernel_dim).type(dtype)\n",
    "        I = torch.eye(n).type(dtype)\n",
    "        inv = torch.inverse(I_k + K)\n",
    "        B = I - kernel.mm(inv).mm(kernel.t())\n",
    "        grad_from_full = 2 * B.mm(kernel)\n",
    "        grad_kernel.sub_(grad_from_full)\n",
    "\n",
    "        grad_kernel.mul_(reward)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10000):\n",
    "    A = Variable(torch.randn(10,10), requires_grad=True)\n",
    "    AllInOne()(A)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    }
   ],
   "source": [
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
