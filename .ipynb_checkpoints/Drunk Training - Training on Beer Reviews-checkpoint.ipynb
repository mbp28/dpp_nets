{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dpp_nets.layers.layers import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "import shutil\n",
    "import time\n",
    "import gzip\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from dpp_nets.utils.io import make_embd, make_tensor_dataset, load_tensor_dataset\n",
    "from dpp_nets.utils.io import data_iterator, load_embd\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import time\n",
    "from dpp_nets.my_torch.utilities import pad_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Data Sets\n",
    "train_set = torch.load('/Users/Max/data/beer_reviews/pytorch/annotated_common.pt')\n",
    "rat_set = torch.load('/Users/Max/data/beer_reviews/pytorch/annotated.pt')\n",
    "embd = load_embd('/Users/Max/data/beer_reviews/pytorch/embeddings.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "batch_size = 25\n",
    "_, max_set_size = train_set.data_tensor.size()\n",
    "_, embd_dim = embd.weight.size()\n",
    "\n",
    "hidden_dim = 500\n",
    "enc_dim = 200\n",
    "target_dim = 3 # let's choose the first three aspects to learn!\n",
    "\n",
    "# Baseline\n",
    "baseline_nets = DeepSetBaseline(embd_dim, hidden_dim, enc_dim, target_dim)\n",
    "baseline = nn.Sequential(embd, baseline_nets, nn.Sigmoid())\n",
    "\n",
    "# Model\n",
    "kernel_dim = 200\n",
    "kernel_net = KernelVar(embd_dim, hidden_dim, kernel_dim)\n",
    "sampler = MarginalSampler()\n",
    "pred_net = PredNet(embd_dim, hidden_dim, enc_dim, target_dim)\n",
    "trainer = MarginalTrainer(kernel_net, sampler, pred_net)\n",
    "\n",
    "trainer.reg = 0.1\n",
    "trainer.reg_mean = 10\n",
    "trainer.activation = nn.Sigmoid()\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "differentiating stochastic functions requires providing a reward",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-912e53b1e926>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# Backpropagate + parameter updates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Max/Coding/anaconda2/envs/torch/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_variables)\u001b[0m\n\u001b[1;32m    144\u001b[0m                     'or with gradient w.r.t. the variable')\n\u001b[1;32m    145\u001b[0m             \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_as_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execution_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Max/Coding/anaconda2/envs/torch/lib/python3.6/site-packages/torch/autograd/stochastic_function.py\u001b[0m in \u001b[0;36m_do_backward\u001b[0;34m(self, grad_output, retain_variables)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_do_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0m_NOT_PROVIDED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             raise RuntimeError(\"differentiating stochastic functions requires \"\n\u001b[0m\u001b[1;32m     16\u001b[0m                                \"providing a reward\")\n\u001b[1;32m     17\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mStochasticFunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: differentiating stochastic functions requires providing a reward"
     ]
    }
   ],
   "source": [
    "# Actual training loop for model\n",
    "\n",
    "params = [{'params': trainer.kernel_net.parameters(), 'lr': 1e-3},\n",
    "          {'params': trainer.pred_net.parameters(), 'lr': 1e-4}]\n",
    "\n",
    "optimizer = torch.optim.Adam(params)\n",
    "trainer.reg = 0.1\n",
    "\n",
    "for epoch in range(10):\n",
    "    for t, (review, target) in enumerate(train_loader):\n",
    "        words = embd(Variable(review))\n",
    "        target = Variable(target[:,:3])\n",
    "        loss  = Rtrainer(words, target)\n",
    "        \n",
    "        # Backpropagate + parameter updates\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if not (t+1) % 10: \n",
    "            print('Loss at it :', t+1, 'is', loss.data[0])\n",
    "            \n",
    "            \n",
    "# Need also a training script for RTrainer!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at it : 10 is 0.019555579870939255\n",
      "Loss at it : 20 is 0.009959155693650246\n",
      "Loss at it : 30 is 0.02200167067348957\n",
      "Loss at it : 40 is 0.017496567219495773\n",
      "Loss at it : 10 is 0.02256540022790432\n",
      "Loss at it : 20 is 0.039579495787620544\n",
      "Loss at it : 30 is 0.024490447714924812\n",
      "Loss at it : 40 is 0.019315138459205627\n",
      "Loss at it : 10 is 0.017173100262880325\n",
      "Loss at it : 20 is 0.022935496643185616\n",
      "Loss at it : 30 is 0.014320855028927326\n",
      "Loss at it : 40 is 0.019234245643019676\n",
      "Loss at it : 10 is 0.02199249342083931\n",
      "Loss at it : 20 is 0.02136988937854767\n",
      "Loss at it : 30 is 0.024206163361668587\n",
      "Loss at it : 40 is 0.018912259489297867\n",
      "Loss at it : 10 is 0.015039907768368721\n",
      "Loss at it : 20 is 0.01443497184664011\n",
      "Loss at it : 30 is 0.01646544598042965\n",
      "Loss at it : 40 is 0.014483126811683178\n",
      "Loss at it : 10 is 0.023360947147011757\n",
      "Loss at it : 20 is 0.023667871952056885\n",
      "Loss at it : 30 is 0.016236674040555954\n",
      "Loss at it : 40 is 0.01942337676882744\n",
      "Loss at it : 10 is 0.015277279540896416\n",
      "Loss at it : 20 is 0.009650547988712788\n",
      "Loss at it : 30 is 0.01935943216085434\n",
      "Loss at it : 40 is 0.014642734080553055\n",
      "Loss at it : 10 is 0.009424908086657524\n",
      "Loss at it : 20 is 0.015709025785326958\n",
      "Loss at it : 30 is 0.01071623433381319\n",
      "Loss at it : 40 is 0.013554783537983894\n",
      "Loss at it : 10 is 0.008834625594317913\n",
      "Loss at it : 20 is 0.015352217480540276\n",
      "Loss at it : 30 is 0.013720878399908543\n",
      "Loss at it : 40 is 0.023724542930722237\n",
      "Loss at it : 10 is 0.015178239904344082\n",
      "Loss at it : 20 is 0.016915099695324898\n",
      "Loss at it : 30 is 0.01227780245244503\n",
      "Loss at it : 40 is 0.008648926392197609\n"
     ]
    }
   ],
   "source": [
    "# Actual training loop for baseline\n",
    "# Training\n",
    "criterion = nn.MSELoss()\n",
    "lr = 1e-4\n",
    "optimizer = torch.optim.Adam(baseline_nets.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "for epoch in range(10):\n",
    "    \n",
    "    for t, (review, target) in enumerate(train_loader):\n",
    "        target = Variable(target[:,:3])\n",
    "        words = Variable(review)\n",
    "        pred = baseline(words)\n",
    "        loss = criterion(pred, target)\n",
    "\n",
    "        # Backpropagate + parameter updates\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if not (t+1) % 10: \n",
    "            print('Loss at it :', t+1, 'is', loss.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validate_baseline(val_set, model, criterion):\n",
    "    x = Variable(val_set.data_tensor, volatile=True)\n",
    "    y = Variable(val_set.target_tensor[:,:3], volatile=True)\n",
    "    pred = model(x)\n",
    "    loss = criterion(pred, y)\n",
    "    print(loss.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validate_model(val_set, model):\n",
    "    model.reg = 0\n",
    "    x = Variable(val_set.data_tensor, volatile=True)\n",
    "    x = embd(x)\n",
    "    y = Variable(val_set.target_tensor[:,:3], volatile=True)\n",
    "    loss = model(x, y)\n",
    "    print(loss.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.007740424480289221\n"
     ]
    }
   ],
   "source": [
    "validate_model(train_set, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.012275410816073418\n"
     ]
    }
   ],
   "source": [
    "validate_baseline(train_set, baseline, nn.MSELoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sampler = ReinforceSampler(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rtrainer = ReinforceTrainer(trainer.kernel_net, sampler, trainer.pred_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def sample(model, sampler, embd, dataset):\n",
    "    rand = random.randint(0, len(dataset))\n",
    "    x = dataset.data_tensor[rand:rand+2]\n",
    "    x = embd(Variable(x))\n",
    "    y = dataset.target_tensor[rand:rand+2]\n",
    "    kernel = trainer.kernel_net(x)\n",
    "    sampler.s_ix = trainer.kernel_net.s_ix\n",
    "    sampler.e_ix = trainer.kernel_net.e_ix\n",
    "    sampler(kernel, x)\n",
    "    print(sampler.saved_subsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "1.00000e-02 *\n",
       "  4.5476\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand = random.randint(0, len(train_set))\n",
    "x = train_set.data_tensor[rand:rand+10]\n",
    "x = embd(Variable(x))\n",
    "y = Variable(train_set.target_tensor[rand:rand+10,:3])\n",
    "Rtrainer(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i.data.sum() for l in Rtrainer.sampler.saved_subsets for i in l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
