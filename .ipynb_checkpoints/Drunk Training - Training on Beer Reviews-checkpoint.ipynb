{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dpp_nets.layers.layers import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "import shutil\n",
    "import time\n",
    "import gzip\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from dpp_nets.utils.io import make_embd, make_tensor_dataset, load_tensor_dataset\n",
    "from dpp_nets.utils.io import data_iterator, load_embd\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import time\n",
    "from dpp_nets.my_torch.utilities import pad_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Data Sets\n",
    "train_set = torch.load('/Users/Max/data/full_beer/pytorch/annotated_common.pt')\n",
    "rat_set = torch.load('/Users/Max/data/full_beer/pytorch/annotated.pt')\n",
    "embd = load_embd('/Users/Max/data/full_beer/pytorch/embeddings.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "torch.manual_seed(12)\n",
    "batch_size = 25\n",
    "_, max_set_size = train_set.data_tensor.size()\n",
    "_, embd_dim = embd.weight.size()\n",
    "\n",
    "hidden_dim = 500\n",
    "enc_dim = 200\n",
    "target_dim = 3 # let's choose the first three aspects to learn!\n",
    "\n",
    "# Baseline\n",
    "baseline_nets = DeepSetBaseline(embd_dim, hidden_dim, enc_dim, target_dim)\n",
    "baseline = nn.Sequential(embd, baseline_nets, nn.Sigmoid())\n",
    "\n",
    "# Model\n",
    "kernel_dim = 200\n",
    "trainer = MarginalTrainer(embd, hidden_dim, kernel_dim, enc_dim, target_dim)\n",
    "\n",
    "trainer.reg = 0.1\n",
    "trainer.reg_mean = 10\n",
    "trainer.activation = nn.Sigmoid()\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Actual training loop for model\n",
    "torch.manual_seed(12)\n",
    "params = [{'params': trainer.kernel_net.parameters(), 'lr': 1e-3},\n",
    "          {'params': trainer.pred_net.parameters(), 'lr': 1e-4}]\n",
    "\n",
    "optimizer = torch.optim.Adam(params)\n",
    "trainer.reg = 0.1\n",
    "\n",
    "for epoch in range(10):\n",
    "    for t, (review, target) in enumerate(train_loader):\n",
    "        review = Variable(review)\n",
    "        target = Variable(target[:,:3])\n",
    "        loss  = trainer(review, target)\n",
    "        \n",
    "        # Backpropagate + parameter updates\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if not (t+1) % 10: \n",
    "            print('Loss at it :', t+1, 'is', loss.data[0])\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Need also a training script for RTrainer!!\n",
    "# incorporate embedding into trainer\n",
    "\n",
    "\n",
    "kernel_net = KernelVar(embd_dim, hidden_dim, kernel_dim)\n",
    "sampler = ReinforceSampler(3)\n",
    "pred_net = PredNet(embd_dim, hidden_dim, enc_dim, target_dim)\n",
    "\n",
    "Rtrainer = ReinforceTrainer(kernel_net, sampler, pred_net)\n",
    "Rtrainer.reg = 0.1\n",
    "Rtrainer.reg_mean = 10\n",
    "Rtrainer.activation = nn.Sigmoid()\n",
    "\n",
    "params = [{'params': Rtrainer.kernel_net.parameters(), 'lr': 1e-3},\n",
    "          {'params': Rtrainer.pred_net.parameters(), 'lr': 1e-4}]\n",
    "\n",
    "optimizer = torch.optim.Adam(params)\n",
    "\n",
    "Rtrainer.double()\n",
    "\n",
    "for epoch in range(20):\n",
    "    for t, (review, target) in enumerate(train_loader):\n",
    "        words = embd(Variable(review)).double()\n",
    "        target = Variable(target[:,:3]).double()\n",
    "        loss  = Rtrainer(words, target)\n",
    "        \n",
    "        # Backpropagate + parameter updates\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # print(Rtrainer.kernel_net.layer1.weight.grad)\n",
    "        optimizer.step()\n",
    "\n",
    "        if not (t+1) % 10: \n",
    "            print('Loss at it :', t+1, 'is', loss.data[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Actual training loop for baseline\n",
    "# Training\n",
    "criterion = nn.MSELoss()\n",
    "lr = 1e-4\n",
    "optimizer = torch.optim.Adam(baseline_nets.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "for epoch in range(10):\n",
    "    \n",
    "    for t, (review, target) in enumerate(train_loader):\n",
    "        target = Variable(target[:,:3])\n",
    "        words = Variable(review)\n",
    "        pred = baseline(words)\n",
    "        loss = criterion(pred, target)\n",
    "\n",
    "        # Backpropagate + parameter updates\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if not (t+1) % 10: \n",
    "            print('Loss at it :', t+1, 'is', loss.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validate_baseline(val_set, model, criterion):\n",
    "    x = Variable(val_set.data_tensor, volatile=True)\n",
    "    y = Variable(val_set.target_tensor[:,:3], volatile=True)\n",
    "    pred = model(x)\n",
    "    loss = criterion(pred, y)\n",
    "    print(loss.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validate_model(val_set, model):\n",
    "    model.reg = 0\n",
    "    x = Variable(val_set.data_tensor, volatile=True)\n",
    "    x = embd(x)\n",
    "    y = Variable(val_set.target_tensor[:,:3], volatile=True)\n",
    "    loss = model(x, y)\n",
    "    print(loss.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Rtrainer.float()\n",
    "validate_model(train_set, Rtrainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = Variable(train_set.data_tensor, volatile=True)\n",
    "x = embd(x)\n",
    "y = Variable(train_set.target_tensor[:,:3], volatile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sampler = ReinforceSampler(1)\n",
    "Rtrainer.sampler = sampler\n",
    "Rtrainer.alpha_iter = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validate_baseline(train_set, baseline, nn.MSELoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def sample(model, sampler, embd, dataset):\n",
    "    rand = random.randint(0, len(dataset))\n",
    "    x = dataset.data_tensor[rand:rand+2]\n",
    "    x = embd(Variable(x))\n",
    "    y = dataset.target_tensor[rand:rand+2]\n",
    "    kernel = trainer.kernel_net(x)\n",
    "    sampler.s_ix = trainer.kernel_net.s_ix\n",
    "    sampler.e_ix = trainer.kernel_net.e_ix\n",
    "    sampler(kernel, x)\n",
    "    print(sampler.saved_subsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rand = random.randint(0, len(train_set))\n",
    "x = train_set.data_tensor[rand:rand+10]\n",
    "x = embd(Variable(x))\n",
    "y = Variable(train_set.target_tensor[rand:rand+10,:3])\n",
    "Rtrainer(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[i.data.sum() for l in Rtrainer.sampler.saved_subsets for i in l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "help(argparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "help(argparse.ArgumentParser.add_argument)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--foo')\n",
    "parser.parse_args('--foo 1'.split())\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--foo', action='store_const', const=42)\n",
    "parser.parse_args('--foo'.split())\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--foo', action='store_true')\n",
    "parser.add_argument('--bar', action='store_false')\n",
    "args = parser.parse_args('--foo --bar'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "args.bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "int('aspect1'[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "from dpp_nets.utils.io import make_embd, make_tensor_dataset\n",
    "from dpp_nets.layers.layers import DeepSetBaseline\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Baseline (Deep Sets) Trainer')\n",
    "\n",
    "parser.add_argument('-a', '--aspect', type=str, choices=['aspect1', 'aspect2', 'aspect3', 'all'],\n",
    "                    help='what is the target?', required=True)\n",
    "parser.add_argument('--remote', type=int,\n",
    "                    help='training locally or on cluster?', required=True)\n",
    "\n",
    "parser.add_argument('--data_path_local', type=str, default='/Users/Max/data/beer_reviews',\n",
    "                    help='where is the data folder locally?')\n",
    "parser.add_argument('--data_path_remote', type=str, default='/cluster/home/paulusm/data/beer_reviews',\n",
    "                    help='where is the data folder?')\n",
    "\n",
    "parser.add_argument('--ckp_path_local', type=str, default='/Users/Max/checkpoints/beer_reviews',\n",
    "                    help='where is the data folder locally?')\n",
    "\n",
    "parser.add_argument('--ckp_path_remote', type=str, default='/cluster/home/paulusm/checkpoints/beer_reviews',\n",
    "                    help='where is the data folder?')\n",
    "\n",
    "parser.add_argument('-b', '--batch-size', default=50, type=int,\n",
    "                    metavar='N', help='mini-batch size (default: 50)')\n",
    "parser.add_argument('--epochs', default=30, type=int, metavar='N',\n",
    "                    help='number of total epochs to run')\n",
    "#parser.add_argument('--lr-k', '--learning-rate-k', default=0.1, type=float,\n",
    "#                    metavar='LRk', help='initial learning rate for kernel net')\n",
    "#parser.add_argument('--lr-p', '--learning-rate-p', default=0.1, type=float,\n",
    "#                    metavar='LRp', help='initial learning rate for pred net')\n",
    "parser.add_argument('--lr', '--learning-rate', default=0.1, type=float,\n",
    "                    metavar='LR', help='initial learning rate for baseline')\n",
    "#parser.add_argument('--reg', type=float, required=True,\n",
    "#                    metavar='reg', help='regularization constant')\n",
    "#parser.add_argument('--reg-mean', type=float, required=True,\n",
    "#                    metavar='reg_mean', help='regularization_mean')\n",
    "\n",
    "\n",
    "def train(loader, model, criterion, optimizer, aspect):\n",
    "\n",
    "    for t, (review, target) in enumerate(loader):\n",
    "        review = Variable(review)\n",
    "\n",
    "        if args.aspect == 'all':\n",
    "            target = Variable(target[:,:3]).double()\n",
    "        else:\n",
    "            target = Variable(target[:,int(args.aspect[-1])]).double()\n",
    "\n",
    "        pred = model(review)\n",
    "        loss = criterion(pred, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        print(\"Gradient in pred_net is:\", model[1].pred_net[2].weight.grad.data.sum())\n",
    "        optimizer.step()\n",
    "        print('it %d' %t, 'loss is', loss.data[0])\n",
    "\n",
    "def validate(loader, model, criterion, aspect):\n",
    "\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for i, (review, target) in enumerate(loader, 1):\n",
    "\n",
    "        review = Variable(review, volatile=True)\n",
    "\n",
    "        if args.aspect == 'all':\n",
    "            target = Variable(target[:,:3], volatile=True).double()\n",
    "        else:\n",
    "            target = Variable(target[:,int(args.aspect[-1])], volatile=True).double()\n",
    "\n",
    "        pred = model(review)\n",
    "        loss = criterion(pred, target)\n",
    "        \n",
    "        delta = loss.data[0] - total_loss\n",
    "        total_loss += (delta / i)\n",
    "\n",
    "        print(\"validated one batch\")\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "def log(epoch, loss):\n",
    "    string = str.join(\" | \", ['Epoch: %d' % (epoch), 'Validation Loss: %.5f' % (loss)])\n",
    "\n",
    "    if args.remote:\n",
    "        destination = os.path.join(args.ckp_path_remote, args.aspect + 'DeepSetBaseline_log.txt')\n",
    "    else:\n",
    "        destination = os.path.join(args.ckp_path_local, args.aspect + 'DeepSetBaseline_log.txt')\n",
    "\n",
    "    with open(destination, 'a') as log:\n",
    "        log.write(string + '\\n')\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 5 epochs\"\"\"\n",
    "    lr = args.lr * (0.1 ** (epoch // 5))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "def save_checkpoint(state, is_best, filename='baseline_checkpoint.pth.tar'):\n",
    "    \"\"\"\n",
    "    State is a dictionary that cotains valuable information to be saved.\n",
    "    \"\"\"\n",
    "    if args.remote:\n",
    "        destination = os.path.join(args.ckp_path_remote, args.aspect + filename)\n",
    "    else:\n",
    "        destination = os.path.join(args.ckp_path_local, args.aspect + filename)\n",
    "    \n",
    "    torch.save(state, destination)\n",
    "    if is_best:\n",
    "        if args.remote:\n",
    "            best_destination = os.path.join(args.ckp_path_remote, args.aspect + 'baseline_model_best.pth.tar')\n",
    "        else:\n",
    "            best_destination = os.path.join(args.ckp_path_local, args.aspect + 'baseline_model_best.pth.tar')\n",
    "        \n",
    "        shutil.copyfile(destination, best_destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded data\n",
      "loader defined\n"
     ]
    }
   ],
   "source": [
    "global args, lowest_loss\n",
    "\n",
    "args = parser.parse_args('-a aspect1 --remote 0'.split())\n",
    "\n",
    "lowest_loss = 100 # arbitrary high number as upper bound for loss\n",
    "\n",
    "### Load data\n",
    "if args.remote:\n",
    "    # print('training remotely')\n",
    "    train_path = os.path.join(args.data_path_remote, str.join(\".\",['reviews', args.aspect, 'train.txt.gz']))\n",
    "    val_path   = os.path.join(args.data_path_remote, str.join(\".\",['reviews', args.aspect, 'heldout.txt.gz']))\n",
    "    embd_path  = os.path.join(args.data_path_remote, 'review+wiki.filtered.200.txt.gz')\n",
    "\n",
    "else:\n",
    "    # print('training locally')\n",
    "    train_path = os.path.join(args.data_path_local, str.join(\".\",['reviews', args.aspect, 'train.txt.gz']))\n",
    "    val_path   = os.path.join(args.data_path_local, str.join(\".\",['reviews', args.aspect, 'heldout.txt.gz']))\n",
    "    embd_path = os.path.join(args.data_path_local, 'review+wiki.filtered.200.txt.gz')\n",
    "\n",
    "embd, word_to_ix = make_embd(embd_path)\n",
    "train_set = make_tensor_dataset(train_path, word_to_ix)\n",
    "val_set = make_tensor_dataset(val_path, word_to_ix)\n",
    "print(\"loaded data\")\n",
    "\n",
    "torch.manual_seed(0)\n",
    "train_loader = DataLoader(train_set, args.batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_set, args.batch_size)\n",
    "print(\"loader defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created model\n",
      "set up optimizer\n"
     ]
    }
   ],
   "source": [
    "### Build model\n",
    "# Network parameters\n",
    "embd_dim = embd.weight.size(1)\n",
    "hidden_dim = 500\n",
    "enc_dim = 200\n",
    "if args.aspect == 'all':\n",
    "    target_dim = 3\n",
    "else: \n",
    "    target_dim = 1\n",
    "\n",
    "# Model\n",
    "torch.manual_seed(0)\n",
    "net = DeepSetBaseline(embd_dim, hidden_dim, enc_dim, target_dim)\n",
    "activation = nn.Sigmoid()\n",
    "model = nn.Sequential(embd, net, activation)\n",
    "model.double()\n",
    "print(\"created model\")\n",
    "\n",
    "### Set-up training\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=args.lr)\n",
    "print(\"set up optimizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started loop\n",
      "Gradient in pred_net is: 0.0\n",
      "it 0 loss is 0.18479999394416835\n",
      "Gradient in pred_net is: 0.0\n",
      "it 1 loss is 0.15859999790191665\n",
      "Gradient in pred_net is: 0.0\n",
      "it 2 loss is 0.15119999508857745\n",
      "Gradient in pred_net is: 0.0\n",
      "it 3 loss is 0.14079999823570274\n",
      "Gradient in pred_net is: 0.0\n",
      "it 4 loss is 0.1901999958038332\n",
      "Gradient in pred_net is: 0.0\n",
      "it 5 loss is 0.1645999945640566\n",
      "Gradient in pred_net is: 0.0\n",
      "it 6 loss is 0.17959999456405662\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-7cac1e1f066d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0madjust_learning_rate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maspect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maspect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-5c996fde682f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(loader, model, criterion, optimizer, aspect)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Gradient in pred_net is:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpred_net\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Max/Coding/anaconda2/envs/torch/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_variables)\u001b[0m\n\u001b[1;32m    144\u001b[0m                     'or with gradient w.r.t. the variable')\n\u001b[1;32m    145\u001b[0m             \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_as_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execution_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### Loop\n",
    "torch.manual_seed(0)\n",
    "print(\"started loop\")\n",
    "for epoch in range(args.epochs):\n",
    "\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "\n",
    "    train(train_loader, model, criterion, optimizer, args.aspect)    \n",
    "    loss = validate(val_loader, model, criterion, args.aspect)\n",
    "\n",
    "    log(epoch, loss)\n",
    "    print(\"logged\")\n",
    "\n",
    "    is_best = loss < lowest_loss\n",
    "    lowest_loss = min(loss, lowest_loss)    \n",
    "    save = {'epoch:': epoch + 1, \n",
    "            'model': 'Deep Set Baseline',\n",
    "            'state_dict': model.state_dict(),\n",
    "            'lowest_loss': lowest_loss,\n",
    "            'optimizer': optimizer.state_dict()} \n",
    "\n",
    "    save_checkpoint(save, is_best)\n",
    "    print(\"saved a checkpoint\")\n",
    "\n",
    "print('*'*20, 'SUCCESS','*'*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06666073474979434\n",
      "0.00048288675536696437\n",
      "0.08519333984478093\n",
      "0.0005131492769115211\n",
      "0.09608577196008654\n",
      "0.0005612681309894671\n",
      "0.0689253644478327\n",
      "0.0005614754693065346\n",
      "0.09537681821347639\n",
      "0.000629416967287115\n",
      "0.09697931645836778\n",
      "0.0006836045003954122\n",
      "0.10159840267252736\n",
      "0.0007524704929746431\n",
      "0.06748762098283549\n",
      "0.0007751124064909559\n",
      "0.0923512621700636\n",
      "0.0008091069987244521\n",
      "0.07444130436143773\n",
      "0.0008242635549521838\n",
      "0.08201051005263757\n",
      "0.0008653144448675739\n",
      "0.10478440745237734\n",
      "0.0009609008512932914\n",
      "0.09140471416717443\n",
      "0.0009909321822885436\n",
      "0.06479244194607776\n",
      "0.001018877218078525\n",
      "0.08247154090520369\n",
      "0.001018130201372688\n",
      "0.06793033071373376\n",
      "0.0010443019639106378\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-5e31e90bb0c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mreview\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Max/Coding/anaconda2/envs/torch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Max/git/dpp_nets/dpp_nets/layers/layers.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, words)\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0;31m# Filter out zero words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbyte\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membd_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;31m# Send through encoder network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Max/Coding/anaconda2/envs/torch/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mmasked_select\u001b[0;34m(self, mask)\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmasked_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mMaskedSelect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0msizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Max/Coding/anaconda2/envs/torch/lib/python3.6/site-packages/torch/autograd/_functions/tensor.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tensor, mask)\u001b[0m\n\u001b[1;32m    438\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_for_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# A manual loop\n",
    "loader = train_loader\n",
    "\n",
    "for t, (review, target) in enumerate(loader):\n",
    "    review = Variable(review)\n",
    "    words = embd(review)\n",
    "    output = net(words)\n",
    "    \n",
    "    pred = nn.Sigmoid()(output)\n",
    "\n",
    "    if args.aspect == 'all':\n",
    "        target = Variable(target[:,:3]).double()\n",
    "    else:\n",
    "        target = Variable(target[:,int(args.aspect[-1])]).double()\n",
    "\n",
    "    loss = criterion(pred, target)\n",
    "    optimizer.zero_grad\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(loss.data[0])\n",
    "    print(net.enc_layer1.weight.grad.data.sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
