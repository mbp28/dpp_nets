{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "from dpp_nets.utils.io import make_embd, make_tensor_dataset\n",
    "from dpp_nets.my_torch.utilities import pad_tensor\n",
    "\n",
    "from dpp_nets.layers.layers import DeepSetBaseline\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "import string\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "import torch\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "\n",
    "import gzip\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = '/Users/Max/data/beer_reviews/reviews.all.train.chunks.txt.gz'\n",
    "word_path = '/Users/Max/data/beer_reviews/reviews.all.train.words.txt.gz'\n",
    "embd_path = '/Users/Max/data/beer_reviews/review+wiki.filtered.200.txt.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        # Basic Indexing\n",
    "        self.word2index = {}\n",
    "        self.index2word = {}\n",
    "        \n",
    "        # Keeping track of vocabulary\n",
    "        self.vocab_size = 0 \n",
    "        self.word2count = {}\n",
    "        \n",
    "        # Vector Dictionaries\n",
    "        self.pretrained = {}\n",
    "        self.random = {}\n",
    "        self.word2vec = {}\n",
    "        self.index2vec = {}\n",
    "\n",
    "        # Set of Stop Words\n",
    "        self.stop_words = set()\n",
    "        \n",
    "        self.Embedding = None\n",
    "        self.EmbeddingBag = None\n",
    "    \n",
    "    def setStops(self):\n",
    "        \n",
    "        self.stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "        make_stops = set(string.punctuation + '\\n' + '\\t' + '...')\n",
    "        unmake_stops = set(('no', 'not'))\n",
    "\n",
    "        self.stop_words = self.stop_words.union(make_stops)\n",
    "        self.stop_words = self.stop_words.difference(unmake_stops)      \n",
    "        \n",
    "    def loadPretrained(self, embd_path):\n",
    "        \n",
    "        self.pretrained = {}\n",
    "        with gzip.open(embd_path, 'rt') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    word, *embd = line.split()\n",
    "                    vec = torch.FloatTensor([float(dim) for dim in embd])            \n",
    "                    self.pretrained[word]  = vec\n",
    "                    \n",
    "    def loadCorpus(self, word_path):\n",
    "        \n",
    "        with gzip.open(data_path, 'rt') as f:\n",
    "\n",
    "            for line in f:\n",
    "                _, review = line.split('\\D')\n",
    "                review = tuple(tuple(chunk.split('\\W')) for chunk in review.split('\\T'))\n",
    "\n",
    "                for words in review:\n",
    "                    vocab.addWords(words)\n",
    "            \n",
    "    def addWords(self, words):\n",
    "        \"\"\"\n",
    "        words: seq containing variable no of words\n",
    "        \"\"\"\n",
    "        for word in words:\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "\n",
    "        if word not in self.word2index:\n",
    "            \n",
    "            # Keeping track of vocabulary\n",
    "            self.vocab_size += 1\n",
    "            self.word2count[word] = 1\n",
    "            \n",
    "            # Basic Indexing\n",
    "            self.word2index[word] = self.vocab_size\n",
    "            self.index2word[self.vocab_size] = word\n",
    "            \n",
    "            # Add word vector\n",
    "            if word in self.pretrained:\n",
    "                vec = self.pretrained[word]\n",
    "                self.word2vec[word] = vec\n",
    "                self.index2vec[self.vocab_size] = vec\n",
    "                \n",
    "            else:\n",
    "                vec = torch.randn(200)\n",
    "                self.random[word] = vec\n",
    "                self.word2vec[word] = vec\n",
    "                self.index2vec[self.vocab_size] = vec\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "            \n",
    "    def updateEmbedding(self):\n",
    "        \n",
    "        vocab_size = len(self.index2vec) + 1\n",
    "        EMBD_DIM = 200\n",
    "        \n",
    "        self.Embedding = nn.Embedding(vocab_size, EMBD_DIM, padding_idx=0)\n",
    "        self.EmbeddingBag = nn.EmbeddingBag(vocab_size, EMBD_DIM)\n",
    "        embd_matrix = torch.zeros(vocab_size, EMBD_DIM)\n",
    "        \n",
    "        for ix, vec in vocab.index2vec.items():\n",
    "            embd_matrix[ix] = vec\n",
    "        \n",
    "        embd_dict = OrderedDict([('weight', embd_matrix)])\n",
    "        self.Embedding.load_state_dict(embd_dict)\n",
    "        self.EmbeddingBag.load_state_dict(embd_dict)\n",
    "    \n",
    "    def checkWord(self, word, min_count):\n",
    "        if word not in vocab.stop_words and word in vocab.word2index and vocab.word2index[word] > min_count:\n",
    "            return word\n",
    "            \n",
    "    def filterReview(self, review):\n",
    "        \"\"\"\n",
    "        review should be like our data set\n",
    "        \"\"\"\n",
    "        f_review = []\n",
    "        seen = set()\n",
    "        \n",
    "        for tup in review:\n",
    "            f_tuple = []\n",
    "            \n",
    "            for word in tup:\n",
    "                word = self.checkWord(word, 10)\n",
    "                if word:\n",
    "                    f_tuple.append(word)\n",
    "            \n",
    "            f_tuple = tuple(f_tuple)    \n",
    "            \n",
    "            if f_tuple and f_tuple not in seen:\n",
    "                seen.add(f_tuple)\n",
    "                f_review.append(f_tuple)\n",
    "                \n",
    "        return f_review\n",
    "    \n",
    "    def mapIndicesBatch(self, reviews):\n",
    "        \n",
    "        f_review = []\n",
    "        offset = []\n",
    "        i = 0\n",
    "\n",
    "        for review in reviews:\n",
    "            seen = set()\n",
    "            \n",
    "            for tup in review: \n",
    "                f_tuple = []\n",
    "                \n",
    "                for word in tup:\n",
    "                    word = vocab.checkWord(word, 10)\n",
    "                    if word:\n",
    "                        f_tuple.append(word)\n",
    "\n",
    "                f_tuple = tuple(f_tuple)    \n",
    "\n",
    "                if f_tuple and f_tuple not in seen:\n",
    "                    seen.add(f_tuple)\n",
    "                    f_review.extend([vocab.word2index[word] for word in f_tuple])\n",
    "                    offset.append(i)\n",
    "                    i += len(f_tuple)\n",
    "            \n",
    "        f_review, offset = torch.LongTensor(f_review), torch.LongTensor(offset)   \n",
    "        return f_review, offset\n",
    "    \n",
    "    def mapIndices(self, review):\n",
    "        \n",
    "        f_review = []\n",
    "        offset = []\n",
    "        seen = set()\n",
    "        i = 0\n",
    "\n",
    "        for tup in review:\n",
    "            f_tuple = []\n",
    "\n",
    "            for word in tup:\n",
    "                word = vocab.checkWord(word, 10)\n",
    "                if word:\n",
    "                    f_tuple.append(word)\n",
    "\n",
    "            f_tuple = tuple(f_tuple)    \n",
    "\n",
    "            if f_tuple and f_tuple not in seen:\n",
    "                seen.add(f_tuple)\n",
    "                f_review.extend([vocab.word2index[word] for word in f_tuple])\n",
    "                offset.append(i)\n",
    "                i += len(f_tuple)\n",
    "\n",
    "        f_review, offset = torch.LongTensor(f_review), torch.LongTensor(offset)   \n",
    "        return f_review, offset\n",
    "    \n",
    "    def returnEmbds(self, review):\n",
    "        \n",
    "        f_review = []\n",
    "        offset = []\n",
    "        seen = set()\n",
    "        i = 0\n",
    "\n",
    "        for tup in review:\n",
    "            f_tuple = []\n",
    "\n",
    "            for word in tup:\n",
    "                word = vocab.checkWord(word, 10)\n",
    "                if word:\n",
    "                    f_tuple.append(word)\n",
    "\n",
    "            f_tuple = tuple(f_tuple)    \n",
    "\n",
    "            if f_tuple and f_tuple not in seen:\n",
    "                seen.add(f_tuple)\n",
    "                f_review.extend([vocab.word2index[word] for word in f_tuple])\n",
    "                offset.append(i)\n",
    "                i += len(f_tuple)\n",
    "\n",
    "        f_review, offset = Variable(torch.LongTensor(f_review)), Variable(torch.LongTensor(offset))\n",
    "        embd = self.EmbeddingBag(f_review, offset)\n",
    "\n",
    "        return embd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BeerDataset(Dataset):\n",
    "    \"\"\"BeerDataset.\"\"\"\n",
    "\n",
    "    def __init__(self, data_path, aspect='all'):\n",
    "        \n",
    "        # Compute size of the data set      \n",
    "        self.aspect = aspect\n",
    "        self.vocab = vocab\n",
    "        \n",
    "        with gzip.open(data_path, 'rt') as f:\n",
    "            self.lines = f.readlines()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lines)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        # Decode\n",
    "        target, review = self.lines[idx].split('\\D')\n",
    "        \n",
    "        # Target\n",
    "        target = torch.FloatTensor([float(t) for t in target.split()[:3]])\n",
    "        \n",
    "        # Review\n",
    "        review = tuple(tuple(chunk.split('\\W')) for chunk in review.split('\\T'))\n",
    "        #ixs, offset = self.vocab.mapIndices(review)\n",
    "        \n",
    "        #sample = {'ixs': ixs, 'offset': offset, 'target': target}\n",
    "        sample = {'review': review, 'target': target}\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocabulary()\n",
    "vocab.loadPretrained(embd_path)\n",
    "vocab.setStops()\n",
    "vocab.loadCorpus(word_path)\n",
    "vocab.updateEmbedding()\n",
    "\n",
    "ds = BeerDataset(data_path, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_collate(batch):\n",
    "    \"Puts each data field into a tensor with outer dimension batch size\"\n",
    "    return {'review': [d['review']for d in batch], 'target': torch.stack([d['target'] for d in batch], 0)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_collate(batch, vocab=vocab):\n",
    "\n",
    "    # Count sizes\n",
    "    max_no_chunks = 0\n",
    "    for d in batch:\n",
    "        max_no_chunks = max(max_no_chunks, len(vocab.filterReview(d['review'])))\n",
    "    \n",
    "    # Map to Embeddings\n",
    "    reps = []\n",
    "    for d in batch:\n",
    "        rep = vocab.returnEmbds(d['review'])\n",
    "        rep = torch.cat([rep, Variable(torch.zeros(max_no_chunks + 1 - rep.size(0), rep.size(1)))], dim=0)\n",
    "        reps.append(rep)\n",
    "    \n",
    "    data_tensor = torch.stack(reps) \n",
    "    \n",
    "    # Create target vector\n",
    "    # target_tensor = Variable(torch.stack([d['target'] for d in batch]))\n",
    "    target_tensor = Variable(torch.stack([d['target'] for d in batch]))\n",
    "    \n",
    "    return data_tensor, target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 1 using my_collate\n",
    "\n",
    "dl = DataLoader(ds, batch_size=50, collate_fn=my_collate)\n",
    "for batch in dl:\n",
    "    break\n",
    "words = batch[0]\n",
    "\n",
    "from dpp_nets.layers.layers import KernelVar, MarginalSampler, PredNet\n",
    "\n",
    "kernel_net = KernelVar(200,500,200)\n",
    "trainer = MarginalTrainer()\n",
    "\n",
    "embd_dim = 200\n",
    "hidden_dim = 500\n",
    "kernel_dim = 200\n",
    "enc_dim = 200\n",
    "target_dim = 3\n",
    "\n",
    "kernel_net = KernelVar(embd_dim, hidden_dim, kernel_dim)\n",
    "sampler = MarginalSampler()\n",
    "pred_net = PredNet(embd_dim, hidden_dim, enc_dim, target_dim)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "activation = nn.Sigmoid()\n",
    "\n",
    "pred = None\n",
    "\n",
    "pred_loss = None \n",
    "reg_loss = None\n",
    "loss = None\n",
    "\n",
    "reg = 10\n",
    "reg_mean = 0.1\n",
    "\n",
    "kernel, words = kernel_net(words) # returned words are masked now!\n",
    "\n",
    "sampler.s_ix = kernel_net.s_ix\n",
    "sampler.e_ix = kernel_net.e_ix\n",
    "\n",
    "weighted_words = sampler(kernel, words) \n",
    "\n",
    "pred_net.s_ix = sampler.s_ix\n",
    "pred_net.e_ix = sampler.e_ix\n",
    "\n",
    "pred = pred_net(weighted_words)\n",
    "\n",
    "target = batch[1]\n",
    "\n",
    "if activation:\n",
    "    pred = activation(pred)\n",
    "\n",
    "pred_loss = criterion(pred, target)\n",
    "\n",
    "if reg:\n",
    "    reg_loss = reg * (torch.stack(sampler.exp_sizes) - reg_mean).pow(2).mean()\n",
    "    loss = pred_loss + reg_loss\n",
    "else:\n",
    "    loss = pred_loss\n",
    "\n",
    "\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "   11     0\n",
       "   11     1\n",
       "   11     2\n",
       "     ⋮      \n",
       "  877   197\n",
       "  877   198\n",
       "  877   199\n",
       "[torch.LongTensor of size 154400x2]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.EmbeddingBag.weight.grad.data.nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "words = Variable(torch.FloatTensor([[[1,2,3,4],[3,4,5,6],[0,0,0,0]],[[1,2,3,4],[0,0,0,0],[0,0,0,0]]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.utils.backcompat.broadcast_warning.enabled = True\n",
    "torch.utils.backcompat.keepdim_warning.enabled = True\n",
    "words = Variable(torch.FloatTensor([[[1,2,3,4],[3,4,5,6],[0,0,0,0]],[[1,2,3,4],[0,0,0,0],[0,0,0,0]]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Solution 2 - using mycollate2 + new KernelNetwork\n",
    "def my_collate2(batch, vocab=vocab):\n",
    "\n",
    "    # Create indices\n",
    "    s_ix, e_ix, i = [], [], 0\n",
    "\n",
    "    for l in [len(vocab.filterReview(d['review'])) for d in batch]:\n",
    "        s_ix.append(i)\n",
    "        i += l\n",
    "        e_ix.append(i)\n",
    "    \n",
    "    # Map to Embeddings\n",
    "    batch_review = [review['review'] for review in batch]\n",
    "    ixs, offsets =  vocab.mapIndicesBatch(batch_review)\n",
    "    embd = vocab.EmbeddingBag(Variable(ixs), Variable(offsets))\n",
    "\n",
    "    # Create target vector\n",
    "    target_tensor = Variable(torch.stack([d['target'] for d in batch]))\n",
    "    \n",
    "    return embd, target_tensor, s_ix, e_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = [ds[ix] for ix in range(4)]\n",
    "batch_review = [review['review'] for review in batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "embd, target_tensor, s_ix, e_ix = my_collate2(batch, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = []\n",
    "for s, e in zip(s_ix, e_ix):\n",
    "    text = embd[s:e].sum(0, keepdim=True).expand_as(embd[s:e])\n",
    "    context.append(text)\n",
    "context = torch.cat(context, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " -2.9850  -1.7892   2.4351  ...    1.6741  -2.4876  -5.4286\n",
       " -2.9850  -1.7892   2.4351  ...    1.6741  -2.4876  -5.4286\n",
       " -2.9850  -1.7892   2.4351  ...    1.6741  -2.4876  -5.4286\n",
       "           ...               ⋱              ...            \n",
       " -0.8701  -2.0587   2.1200  ...    0.8378  -2.6831  -2.2940\n",
       " -0.8701  -2.0587   2.1200  ...    0.8378  -2.6831  -2.2940\n",
       " -0.8701  -2.0587   2.1200  ...    0.8378  -2.6831  -2.2940\n",
       "[torch.FloatTensor of size 389x200]"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1117 389\n",
      "356 125\n",
      "141 52\n",
      "389 143\n",
      "231 69\n"
     ]
    }
   ],
   "source": [
    "ixs, offsets =  vocab.mapIndicesBatch(batch_review)\n",
    "print(len(ixs), len(offsets))\n",
    "for review in batch_review:\n",
    "    ixs, offsets = vocab.mapIndices(review)\n",
    "    print(len(ixs), len(offsets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class KernelVar(nn.Module):\n",
    "\n",
    "    def __init__(self, embd_dim, hidden_dim, kernel_dim):\n",
    "        \"\"\"\n",
    "        Currently, this creates a 2-hidden-layer network \n",
    "        with ELU non-linearities.\n",
    "\n",
    "        \"\"\"\n",
    "        super(KernelVar, self).__init__()\n",
    "        self.embd_dim = embd_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_dim = kernel_dim\n",
    "\n",
    "        self.layer1 = nn.Linear(2 * embd_dim, hidden_dim)\n",
    "        self.layer2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.layer3 = nn.Linear(hidden_dim, kernel_dim)\n",
    "\n",
    "        self.net = nn.Sequential(self.layer1, nn.Tanh(), self.layer2, nn.Tanh(), self.layer3)\n",
    "\n",
    "        self.s_ix = None\n",
    "        self.e_ix = None\n",
    "\n",
    "\n",
    "    def forward(self, embd):\n",
    "        \"\"\"\n",
    "        Given words, returns batch_kernel of dimension\n",
    "        [-1, kernel_dim]\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create context\n",
    "        context = []\n",
    "        for s, e in zip(self.s_ix, self.e_ix):\n",
    "            text = embd[s:e].sum(0, keepdim=True).expand_as(embd[s:e])\n",
    "            context.append(text)\n",
    "        context = torch.cat(context, dim=0)\n",
    "        batch_x = torch.cat([embd, context], dim=1)\n",
    "        \n",
    "        batch_kernel = self.net(batch_x)\n",
    "\n",
    "        return batch_kernel , words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "233 634\n",
      "233 634\n"
     ]
    }
   ],
   "source": [
    "# Next check\n",
    "l_ixs, l_offsets =  vocab.mapIndicesBatch(batch_review)\n",
    "\n",
    "review = batch_review[2]\n",
    "ixs, offsets = vocab.mapIndices(review)\n",
    "\n",
    "a, b = 111, 49\n",
    "print(l_ixs[a+356+141],l_offsets[b+125 + 52])\n",
    "print(ixs[a], offsets[b]+356+141)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 3.8239e-02 -1.1865e-01 -8.4999e-02  ...  -4.0149e-02 -1.0817e-01 -8.7149e-02\n",
       "-1.0188e-01 -8.4496e-02 -3.7914e-02  ...  -3.8677e-02 -5.3894e-02  4.0471e-02\n",
       "-1.3008e-02  4.8421e-03  1.0499e-02  ...  -1.2304e-02 -4.7805e-02 -2.2940e-02\n",
       "                ...                   ⋱                   ...                \n",
       "-2.0457e-02 -6.7305e-02  5.4419e-02  ...  -1.3410e-02 -7.4654e-02 -4.6580e-02\n",
       "-6.0656e-02 -2.6737e-02  1.1477e-01  ...   4.4280e-02 -9.8683e-02 -4.5009e-02\n",
       "-6.0656e-02 -2.6737e-02  1.1477e-01  ...   4.4280e-02 -9.8683e-02 -4.5009e-02\n",
       "[torch.FloatTensor of size 389x200]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.EmbeddingBag(Variable(l_ixs), Variable(l_offsets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = [len(vocab.filterReview(d['review'])) for d in batch]\n",
    "cum_sum = [for i in range(len(lengths))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[125, 52, 143]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = [sum(numbers[:i]) for i in range(1, len(numbers)+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 125\n",
      "1 52\n",
      "2 143\n"
     ]
    }
   ],
   "source": [
    "for i, l in enumerate(lengths):\n",
    "    print(i, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## \n",
    "s_ix = []\n",
    "e_ix = []\n",
    "i = 0\n",
    "for l in [len(vocab.filterReview(d['review'])) for d in batch]:\n",
    "    s_ix.append(i)\n",
    "    i += l\n",
    "    e_ix.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 125, 177, 320]\n",
      "[125, 177, 320, 389]\n"
     ]
    }
   ],
   "source": [
    "print(s_ix)\n",
    "print(e_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[125, 52, 143, 69]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(vocab.filterReview(d['review'])) for d in batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
