{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "V = torch.randn(250, 200)\n",
    "L = V.mm(V.t())\n",
    "\n",
    "vecs, vals, _ = torch.svd(V)\n",
    "vals = vals.pow(2)\n",
    "n = vecs.size(0)\n",
    "n_vals = vals.size(0)\n",
    "\n",
    "subset = torch.FloatTensor([0,1,1,0,0] * 50)\n",
    "subset_sum = subset.long().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.0021\n",
      " 0.0022\n",
      " 0.0022\n",
      " 0.0022\n",
      " 0.0023\n",
      " 0.0023\n",
      " 0.0024\n",
      " 0.0024\n",
      " 0.0024\n",
      " 0.0026\n",
      " 0.0025\n",
      " 0.0026\n",
      " 0.0026\n",
      " 0.0027\n",
      " 0.0027\n",
      " 0.0028\n",
      " 0.0028\n",
      " 0.0028\n",
      " 0.0028\n",
      " 0.0029\n",
      " 0.0030\n",
      " 0.0030\n",
      " 0.0030\n",
      " 0.0031\n",
      " 0.0031\n",
      " 0.0032\n",
      " 0.0033\n",
      " 0.0034\n",
      " 0.0035\n",
      " 0.0033\n",
      " 0.0035\n",
      " 0.0033\n",
      " 0.0036\n",
      " 0.0036\n",
      " 0.0036\n",
      " 0.0037\n",
      " 0.0036\n",
      " 0.0038\n",
      " 0.0038\n",
      " 0.0038\n",
      " 0.0037\n",
      " 0.0038\n",
      " 0.0040\n",
      " 0.0041\n",
      " 0.0041\n",
      " 0.0039\n",
      " 0.0042\n",
      " 0.0043\n",
      " 0.0042\n",
      " 0.0045\n",
      " 0.0044\n",
      " 0.0042\n",
      " 0.0046\n",
      " 0.0047\n",
      " 0.0045\n",
      " 0.0047\n",
      " 0.0048\n",
      " 0.0049\n",
      " 0.0047\n",
      " 0.0048\n",
      " 0.0048\n",
      " 0.0050\n",
      " 0.0050\n",
      " 0.0052\n",
      " 0.0052\n",
      " 0.0054\n",
      " 0.0053\n",
      " 0.0056\n",
      " 0.0056\n",
      " 0.0055\n",
      " 0.0059\n",
      " 0.0060\n",
      " 0.0059\n",
      " 0.0059\n",
      " 0.0057\n",
      " 0.0056\n",
      " 0.0064\n",
      " 0.0066\n",
      " 0.0064\n",
      " 0.0063\n",
      " 0.0066\n",
      " 0.0066\n",
      " 0.0066\n",
      " 0.0066\n",
      " 0.0072\n",
      " 0.0069\n",
      " 0.0071\n",
      " 0.0072\n",
      " 0.0070\n",
      " 0.0073\n",
      " 0.0073\n",
      " 0.0076\n",
      " 0.0076\n",
      " 0.0076\n",
      " 0.0079\n",
      " 0.0081\n",
      " 0.0074\n",
      " 0.0081\n",
      " 0.0088\n",
      " 0.0086\n",
      " 0.0085\n",
      " 0.0086\n",
      " 0.0086\n",
      " 0.0083\n",
      " 0.0090\n",
      " 0.0089\n",
      " 0.0088\n",
      " 0.0093\n",
      " 0.0102\n",
      " 0.0099\n",
      " 0.0097\n",
      " 0.0108\n",
      " 0.0101\n",
      " 0.0099\n",
      " 0.0110\n",
      " 0.0119\n",
      " 0.0110\n",
      " 0.0114\n",
      " 0.0119\n",
      " 0.0122\n",
      " 0.0116\n",
      " 0.0110\n",
      " 0.0117\n",
      " 0.0115\n",
      " 0.0124\n",
      " 0.0123\n",
      " 0.0131\n",
      " 0.0120\n",
      " 0.0141\n",
      " 0.0141\n",
      " 0.0135\n",
      " 0.0149\n",
      " 0.0142\n",
      " 0.0165\n",
      " 0.0145\n",
      " 0.0158\n",
      " 0.0161\n",
      " 0.0161\n",
      " 0.0156\n",
      " 0.0173\n",
      " 0.0176\n",
      " 0.0170\n",
      " 0.0180\n",
      " 0.0198\n",
      " 0.0186\n",
      " 0.0187\n",
      " 0.0198\n",
      " 0.0209\n",
      " 0.0208\n",
      " 0.0221\n",
      " 0.0212\n",
      " 0.0225\n",
      " 0.0223\n",
      " 0.0228\n",
      " 0.0242\n",
      " 0.0247\n",
      " 0.0264\n",
      " 0.0284\n",
      " 0.0279\n",
      " 0.0277\n",
      " 0.0285\n",
      " 0.0298\n",
      " 0.0310\n",
      " 0.0311\n",
      " 0.0331\n",
      " 0.0319\n",
      " 0.0334\n",
      " 0.0333\n",
      " 0.0359\n",
      " 0.0393\n",
      " 0.0380\n",
      " 0.0394\n",
      " 0.0410\n",
      " 0.0409\n",
      " 0.0449\n",
      " 0.0452\n",
      " 0.0493\n",
      " 0.0502\n",
      " 0.0558\n",
      " 0.0563\n",
      " 0.0607\n",
      " 0.0630\n",
      " 0.0633\n",
      " 0.0703\n",
      " 0.0738\n",
      " 0.0766\n",
      " 0.0808\n",
      " 0.0881\n",
      " 0.1019\n",
      " 0.1134\n",
      " 0.1183\n",
      " 0.1329\n",
      " 0.1444\n",
      " 0.1580\n",
      " 0.1697\n",
      " 0.1838\n",
      " 0.2134\n",
      " 0.2261\n",
      " 0.2451\n",
      " 0.2648\n",
      "[torch.FloatTensor of size 200]\n",
      " \n",
      " 0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "-0.2590  0.1660  0.0106  ...  -0.0033  0.0072  0.0071\n",
      " 0.0152 -0.0390 -0.2499  ...   0.0029 -0.0064  0.0003\n",
      "          ...             â‹±             ...          \n",
      " 0.1966 -0.2418 -0.1696  ...  -0.0057 -0.0025  0.0063\n",
      " 0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      " 0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "[torch.FloatTensor of size 250x200]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grad_vals = 1 / vals\n",
    "grad_vecs = torch.zeros(n, n_vals)\n",
    "\n",
    "matrix = vecs.mm(vals.diag()).mm(vecs.t())\n",
    "\n",
    "P = torch.eye(n).masked_select(subset.expand(n,n).t().byte()).view(subset_sum, -1)\n",
    "submatrix = P.mm(matrix).mm(P.t())\n",
    "# ix = (subset * torch.arange(0,len(subset))).nonzero()\n",
    "# submatrix = matrix[ix,].squeeze(1).t()[ix,].squeeze(1)\n",
    "subinv = torch.inverse(submatrix)\n",
    "Pvecs = P.mm(vecs)\n",
    "# Pvecs = vecs[ix,:].squeeze(1)\n",
    "\n",
    "grad_vals += Pvecs.t().mm(subinv).mm(Pvecs).diag()\n",
    "grad_vecs += P.t().mm(subinv).mm(Pvecs).mm(vals.diag())\n",
    "print(grad_vals, grad_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.0021\n",
      " 0.0022\n",
      " 0.0022\n",
      " 0.0022\n",
      " 0.0023\n",
      " 0.0023\n",
      " 0.0024\n",
      " 0.0024\n",
      " 0.0024\n",
      " 0.0026\n",
      " 0.0025\n",
      " 0.0026\n",
      " 0.0026\n",
      " 0.0027\n",
      " 0.0027\n",
      " 0.0028\n",
      " 0.0028\n",
      " 0.0028\n",
      " 0.0028\n",
      " 0.0029\n",
      " 0.0030\n",
      " 0.0030\n",
      " 0.0030\n",
      " 0.0031\n",
      " 0.0031\n",
      " 0.0032\n",
      " 0.0033\n",
      " 0.0034\n",
      " 0.0035\n",
      " 0.0033\n",
      " 0.0035\n",
      " 0.0033\n",
      " 0.0036\n",
      " 0.0036\n",
      " 0.0036\n",
      " 0.0037\n",
      " 0.0036\n",
      " 0.0038\n",
      " 0.0038\n",
      " 0.0038\n",
      " 0.0037\n",
      " 0.0038\n",
      " 0.0040\n",
      " 0.0041\n",
      " 0.0041\n",
      " 0.0039\n",
      " 0.0042\n",
      " 0.0043\n",
      " 0.0042\n",
      " 0.0045\n",
      " 0.0044\n",
      " 0.0042\n",
      " 0.0046\n",
      " 0.0047\n",
      " 0.0045\n",
      " 0.0047\n",
      " 0.0048\n",
      " 0.0049\n",
      " 0.0047\n",
      " 0.0048\n",
      " 0.0048\n",
      " 0.0050\n",
      " 0.0050\n",
      " 0.0052\n",
      " 0.0052\n",
      " 0.0054\n",
      " 0.0053\n",
      " 0.0056\n",
      " 0.0056\n",
      " 0.0055\n",
      " 0.0059\n",
      " 0.0060\n",
      " 0.0059\n",
      " 0.0059\n",
      " 0.0057\n",
      " 0.0056\n",
      " 0.0064\n",
      " 0.0066\n",
      " 0.0064\n",
      " 0.0063\n",
      " 0.0066\n",
      " 0.0066\n",
      " 0.0066\n",
      " 0.0066\n",
      " 0.0072\n",
      " 0.0069\n",
      " 0.0071\n",
      " 0.0072\n",
      " 0.0070\n",
      " 0.0073\n",
      " 0.0073\n",
      " 0.0076\n",
      " 0.0076\n",
      " 0.0076\n",
      " 0.0079\n",
      " 0.0081\n",
      " 0.0074\n",
      " 0.0081\n",
      " 0.0088\n",
      " 0.0086\n",
      " 0.0085\n",
      " 0.0086\n",
      " 0.0086\n",
      " 0.0083\n",
      " 0.0090\n",
      " 0.0089\n",
      " 0.0088\n",
      " 0.0093\n",
      " 0.0102\n",
      " 0.0099\n",
      " 0.0097\n",
      " 0.0108\n",
      " 0.0101\n",
      " 0.0099\n",
      " 0.0110\n",
      " 0.0119\n",
      " 0.0110\n",
      " 0.0114\n",
      " 0.0119\n",
      " 0.0122\n",
      " 0.0116\n",
      " 0.0110\n",
      " 0.0117\n",
      " 0.0115\n",
      " 0.0124\n",
      " 0.0123\n",
      " 0.0131\n",
      " 0.0120\n",
      " 0.0141\n",
      " 0.0141\n",
      " 0.0135\n",
      " 0.0149\n",
      " 0.0142\n",
      " 0.0165\n",
      " 0.0145\n",
      " 0.0158\n",
      " 0.0161\n",
      " 0.0161\n",
      " 0.0156\n",
      " 0.0173\n",
      " 0.0176\n",
      " 0.0170\n",
      " 0.0180\n",
      " 0.0198\n",
      " 0.0186\n",
      " 0.0187\n",
      " 0.0198\n",
      " 0.0209\n",
      " 0.0208\n",
      " 0.0221\n",
      " 0.0212\n",
      " 0.0225\n",
      " 0.0223\n",
      " 0.0228\n",
      " 0.0242\n",
      " 0.0247\n",
      " 0.0264\n",
      " 0.0284\n",
      " 0.0279\n",
      " 0.0277\n",
      " 0.0285\n",
      " 0.0298\n",
      " 0.0310\n",
      " 0.0311\n",
      " 0.0331\n",
      " 0.0319\n",
      " 0.0334\n",
      " 0.0333\n",
      " 0.0359\n",
      " 0.0393\n",
      " 0.0380\n",
      " 0.0394\n",
      " 0.0410\n",
      " 0.0409\n",
      " 0.0449\n",
      " 0.0452\n",
      " 0.0493\n",
      " 0.0502\n",
      " 0.0558\n",
      " 0.0563\n",
      " 0.0607\n",
      " 0.0630\n",
      " 0.0633\n",
      " 0.0703\n",
      " 0.0738\n",
      " 0.0766\n",
      " 0.0808\n",
      " 0.0881\n",
      " 0.1019\n",
      " 0.1134\n",
      " 0.1183\n",
      " 0.1329\n",
      " 0.1444\n",
      " 0.1580\n",
      " 0.1697\n",
      " 0.1838\n",
      " 0.2134\n",
      " 0.2261\n",
      " 0.2451\n",
      " 0.2648\n",
      "[torch.FloatTensor of size 200]\n",
      " \n",
      " 0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "-0.2590  0.1660  0.0106  ...  -0.0033  0.0072  0.0071\n",
      " 0.0152 -0.0390 -0.2499  ...   0.0029 -0.0064  0.0003\n",
      "          ...             â‹±             ...          \n",
      " 0.1966 -0.2418 -0.1696  ...  -0.0057 -0.0025  0.0063\n",
      " 0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      " 0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "[torch.FloatTensor of size 250x200]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grad_vals = 1 / vals\n",
    "grad_vecs = vecs.new().resize_(n, n_vals).copy_(torch.zeros(n, n_vals))\n",
    "\n",
    "matrix = vecs.mm(vals.diag()).mm(vecs.t())\n",
    "\n",
    "ix = (subset * torch.arange(0,len(subset))).nonzero()\n",
    "submatrix = matrix[ix,].squeeze(1).t()[ix,].squeeze(1)\n",
    "subinv = torch.inverse(submatrix)\n",
    "Pvecs = vecs[ix,:].squeeze(1)\n",
    "\n",
    "grad_vals += Pvecs.t().mm(subinv).mm(Pvecs).diag()\n",
    "grad_vecs += P.t().mm(subinv).mm(Pvecs).mm(vals.diag())    \n",
    "\n",
    "print(grad_vals, grad_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_vecs = vecs.new().resize_(n, n_vals).copy_(torch.zeros(n, n_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.FloatTensor with no dimension]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
