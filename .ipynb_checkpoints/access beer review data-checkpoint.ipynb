{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "import shutil\n",
    "import time\n",
    "import gzip\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from dpp_nets.utils.io import make_embd, make_tensor_dataset, load_tensor_dataset\n",
    "from dpp_nets.utils.io import data_iterator, load_embd\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import time\n",
    "from dpp_nets.my_torch.utilities import pad_tensor\n",
    "\n",
    "\n",
    "root = '/Users/Max/data/beer_reviews'\n",
    "data_file = 'reviews.aspect3.train.txt.gz'\n",
    "embd_file = 'review+wiki.filtered.200.txt.gz'\n",
    "save_path = os.path.join(root,'pytorch/aspect3_train.pt')\n",
    "data_path = os.path.join(root, data_file)\n",
    "embd_path = os.path.join(root, embd_file)\n",
    "\n",
    "\n",
    "def read_rationales(path):\n",
    "    \"\"\"\n",
    "    This reads the json.annotations file. \n",
    "    Creates a list of dictionaries, which holds the 994 reviews for which\n",
    "    sentence-level annotations are available. \n",
    "    \"\"\"\n",
    "    data = []\n",
    "    fopen = gzip.open if path.endswith(\".gz\") else open\n",
    "    with fopen(path) as fin:\n",
    "        for line in fin:\n",
    "            item = json.loads(line)\n",
    "            data.append(item)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from dpp_nets.my_torch.linalg import custom_decomp\n",
    "from dpp_nets.my_torch.DPP import DPP\n",
    "from dpp_nets.my_torch.DPP import AllInOne\n",
    "from dpp_nets.my_torch.utilities import compute_baseline\n",
    "\n",
    "class DPP_Classifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, dtype):\n",
    "        \n",
    "        super(DPP_Classifier, self).__init__()\n",
    "        # Float vs Double\n",
    "        self.dtype = dtype\n",
    "\n",
    "        # Network parameters\n",
    "        self.kernel_in = kernel_in = 400\n",
    "        self.kernel_h = kernel_h = 1000\n",
    "        self.kernel_out = kernel_out = 400\n",
    "\n",
    "        self.pred_in = pred_in = 200 # kernel_in / 2\n",
    "        self.pred_h = pred_h = 500\n",
    "        self.pred_h2 = pred_h2 = 200\n",
    "        self.pred_out = pred_out = 3\n",
    "        \n",
    "        # 2-Hidden-Layer Networks \n",
    "        self.kernel_net = torch.nn.Sequential(nn.Linear(kernel_in, kernel_h), nn.ELU(),\n",
    "                                              nn.Linear(kernel_h, kernel_h), nn.ELU(), \n",
    "                                              nn.Linear(kernel_h, kernel_out))\n",
    "        # 3-Hidden-Layer-Networks\n",
    "        self.pred_net = torch.nn.Sequential(nn.Linear(pred_in, pred_h), nn.ReLU(),\n",
    "                                             nn.Linear(pred_h, pred_h), nn.ReLU(),\n",
    "                                             nn.Linear(pred_h, pred_h2), nn.ReLU(),\n",
    "                                             nn.Linear(pred_h2, pred_out), nn.Sigmoid())\n",
    "        \n",
    "        self.kernel_net.type(self.dtype)\n",
    "        self.pred_net.type(self.dtype)\n",
    "        \n",
    "        # Sampling Parameter\n",
    "        self.alpha_iter = 5\n",
    "\n",
    "        # Convenience\n",
    "        self.kernels = []\n",
    "        self.subsets = None\n",
    "        self.picks = None\n",
    "        self.preds = None\n",
    "        \n",
    "        self.saved_subsets = None\n",
    "        self.saved_losses = None # not really necesary\n",
    "        self.saved_baselines = None # not really necessary\n",
    "        \n",
    "    def forward(self, reviews):\n",
    "        \"\"\"\n",
    "        reviews: batch_size x max_set_size x embd_dim = 200\n",
    "        Output: batch_size x pred_out (the prediction)\n",
    "        Challenges: Need to resize tensor appropriately and \n",
    "        measure length etc. \n",
    "        \"\"\"\n",
    "        batch_size, max_set_size, embd_dim = reviews.size()\n",
    "        alpha_iter = self.alpha_iter\n",
    "        self.saved_subsets = actions = [[] for i in range(batch_size)]\n",
    "        picks = [[] for i in range(batch_size)]\n",
    "        \n",
    "        # Create context\n",
    "        lengths = reviews.sum(2).abs().sign().sum(1)\n",
    "        context = (reviews.sum(1) / lengths.expand_as(reviews.sum(1))).expand_as(reviews)\n",
    "        mask = reviews.sum(2).abs().sign().expand_as(reviews).byte()\n",
    "\n",
    "        # Mask out zero words\n",
    "        reviews = reviews.masked_select(mask).view(-1, embd_dim)\n",
    "        context = context.masked_select(mask).view(-1, embd_dim)\n",
    "\n",
    "        # Compute batched_kernel\n",
    "        kernel_input = torch.cat([reviews, context], dim=1)\n",
    "        kernel_output = self.kernel_net(kernel_input)\n",
    "        \n",
    "        # Extract the kernel for each review from batched_kernel\n",
    "        s = list(lengths.squeeze().cumsum(0).long().data - lengths.squeeze().long().data)\n",
    "        e = list(lengths.squeeze().cumsum(0).long().data)\n",
    "\n",
    "        for i, (s, e) in enumerate(zip(s, e)):\n",
    "            review = reviews[s:e] # original review, without zero words\n",
    "            kernel = kernel_output[s:e] # corresponding kernel \n",
    "            self.kernels.append(kernel.data)\n",
    "            #vals, vecs = custom_decomp()(kernel)\n",
    "            for j in range(alpha_iter):\n",
    "                subset = AllInOne()(kernel)\n",
    "                #subset = DPP()(vals, vecs)\n",
    "                actions[i].append(subset)\n",
    "                pick = subset.diag().mm(review).sum(0)\n",
    "                picks[i].append(pick)\n",
    "\n",
    "        # Predictions\n",
    "        picks = torch.stack([torch.stack(pick) for pick in picks]).view(-1, embd_dim)\n",
    "        preds = self.pred_net(picks).view(batch_size, alpha_iter, -1)\n",
    "        \n",
    "        return preds\n",
    "\n",
    "def register_rewards(preds, targets, criterion, net):\n",
    "    \n",
    "    #targets = targets.unsqueeze(1).unsqueeze(1).expand_as(preds)\n",
    "    targets = targets.unsqueeze(1).expand_as(preds)\n",
    "    loss = criterion(preds, targets)\n",
    "    \n",
    "    actions = net.saved_subsets\n",
    "    \n",
    "    losses = ((preds - targets)**2).mean(2)\n",
    "    losses = [[i.data[0] for i in row] for row in losses]\n",
    "    net.saved_losses = losses # not really necessary\n",
    "    baselines = [compute_baseline(i) for i in losses]\n",
    "    net.saved_baselines = baselines # not really necessary\n",
    "    \n",
    "    for actions, rewards in zip(actions, baselines):\n",
    "        for action, reward in zip(actions, rewards):\n",
    "            action.reinforce(reward)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Useful Support\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        \n",
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    \"\"\"\n",
    "    This is good!\n",
    "    \"\"\"\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'model_best.pth.tar')\n",
    "        \n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    lr = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "    lr = lr * (0.1 ** (epoch // 5))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(train_loader, embd, model, criterion, optimizer, epoch, dtype):\n",
    "    \n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    subset_size = AverageMeter()\n",
    "    \n",
    "    target_dim = 3\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (review, target) in enumerate(train_loader):\n",
    "        \n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        \n",
    "        targets = Variable(target[:,:target_dim].type(dtype))\n",
    "        reviews = embd(Variable(review)).type(dtype)\n",
    "\n",
    "        # compute output\n",
    "        model.alpha_iter = 2\n",
    "        pred = model(reviews)\n",
    "        loss = register_rewards(pred, targets, criterion, model)\n",
    "\n",
    "        ##measure accuracy and record loss ????????????????????????\n",
    "        # prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n",
    "        losses.update(loss.data[0], reviews.size(0))\n",
    "        for l in model.saved_subsets:\n",
    "            for s in l:\n",
    "                subset_size.update(s.data.sum())\n",
    "        # top1.update(prec1[0], input.size(0))\n",
    "        # top5.update(prec5[0], input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        #if i % args.print_freq == 0:\n",
    "        if i % print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'SSize {subset_size.val:.2f} ({subset_size.avg: .2f})'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'.format(\n",
    "                   epoch, i, len(train_loader), batch_time=batch_time,\n",
    "                   data_time=data_time, subset_size = subset_size, loss=losses))\n",
    "\n",
    "def validate(val_loader, model, criterion):\n",
    "    \n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    t_prec = AverageMeter()\n",
    "    t_recall = AverageMeter()\n",
    "    t_tp = AverageMeter()\n",
    "    t_fp = AverageMeter()\n",
    "    t_fn = AverageMeter()\n",
    "    \n",
    "    target_dim = 3\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    # model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (review, target) in enumerate(val_loader):\n",
    "        \n",
    "        target = target.sum(1).sign().type(dtype).squeeze().byte()\n",
    "        # targets = target[:,:target_dim,:].type(dtype)\n",
    "        reviews = embd(Variable(review, volatile=True)).type(dtype)\n",
    "\n",
    "        # compute output\n",
    "        model.alpha_iter = 1\n",
    "        preds = model(reviews)\n",
    "\n",
    "        subset = model.saved_subsets[0][0]\n",
    "        subset = pad_tensor(subset.data,0,0,412).byte()\n",
    "        # target = targets\n",
    "\n",
    "        # targets = target[:,:target_dim,:].type(dtype)\n",
    "        reviews = embd(Variable(review, volatile=True)).type(dtype)\n",
    "\n",
    "        # compute output\n",
    "        model.alpha_iter = 1\n",
    "        preds = model(reviews)\n",
    "        \n",
    "        subset = model.saved_subsets[0][0]\n",
    "        subset = pad_tensor(subset.data,0,0,412).byte()\n",
    "        # target = target[:,:target_dim,:].squeeze()\n",
    "\n",
    "        retriev = subset.sum()\n",
    "        relev = target.sum()\n",
    "\n",
    "        tp = target.masked_select(subset).sum()\n",
    "        fp = (1 - target.masked_select(subset)).sum()\n",
    "        fn = (1 - subset.masked_select(target)).sum()\n",
    "        t_tp.update(tp)\n",
    "        t_fp.update(fp)\n",
    "        t_fn.update(fn)\n",
    "\n",
    "        if retriev: \n",
    "            prec = tp / retriev\n",
    "            t_prec.update(prec)\n",
    "\n",
    "        if relev: \n",
    "            recall = tp / relev\n",
    "            t_recall.update(recall)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        #prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n",
    "        #losses.update(loss.data[0], input.size(0))\n",
    "        #top1.update(prec1[0], input.size(0))\n",
    "        #top5.update(prec5[0], input.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Precision {t_prec.val:.4f} ({t_prec.avg:.4f})\\t'\n",
    "                  'Recall {t_recall.val:.4f} ({t_recall.avg:.4f})\\t'.format(\n",
    "                   i, len(val_loader), batch_time=batch_time, t_prec=t_prec, t_recall=t_recall))\n",
    "            \n",
    "    return t_prec.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### MAIN PROGRAMME\n",
    "\n",
    "\n",
    "global best_prec1\n",
    "best_prec1 = 0\n",
    "\n",
    "# set parameters\n",
    "lr = 1e-1\n",
    "momentum = 0.9\n",
    "weight_decay = 0.\n",
    "start_epoch = 0\n",
    "epochs = 1\n",
    "batch_size = 20\n",
    "print_freq = 10\n",
    "\n",
    "data = '/Users/Max/data/beer_reviews/pytorch'\n",
    "dtype = torch.DoubleTensor\n",
    "\n",
    "# create model\n",
    "embd = load_embd('/Users/Max/data/beer_reviews/pytorch/embeddings.pt')\n",
    "model = DPP_Classifier(torch.DoubleTensor)\n",
    "\n",
    "# define loss function (criterion) and optimizer\n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr,\n",
    "                            momentum=momentum,\n",
    "                            weight_decay=weight_decay)\n",
    "\n",
    "# Data loading code\n",
    "trainpath = os.path.join(data, 'aspect1_train.pt')\n",
    "valpath = os.path.join(data, 'aspect1_heldout.pt')\n",
    "ratpath = os.path.join(data, 'annotated.pt')\n",
    "\n",
    "train_set = torch.load(trainpath)\n",
    "val_set = torch.load(valpath)\n",
    "rat_set = torch.load(ratpath)\n",
    "\n",
    "rat_train_set = torch.load(os.path.join(data, 'annotated_common.pt'))\n",
    "#train_loader = DataLoader(train_set, batch_size, shuffle=True)\n",
    "#val_loader = DataLoader(val_set)\n",
    "rat_train_loader = DataLoader(rat_train_set, batch_size, shuffle=True)\n",
    "rat_loader = DataLoader(rat_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "\n",
    "    # train for one epoch\n",
    "    train(rat_train_loader, embd, model, criterion, optimizer, epoch, dtype)\n",
    "\n",
    "    # evaluate on validation set\n",
    "    prec1 = validate(rat_loader, model, criterion)\n",
    "\n",
    "# remember best prec@1 and save checkpoint\n",
    "is_best = prec1 > best_prec1\n",
    "best_prec1 = max(prec1, best_prec1)\n",
    "save_checkpoint({\n",
    "    'epoch': epoch + 1,\n",
    "    'state_dict': model.state_dict(),\n",
    "    'best_prec1': best_prec1,\n",
    "    'optimizer' : optimizer.state_dict(),\n",
    "}, is_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "#word_to_ix = make_embd(embd_path, only_index_dict=True)\n",
    "#ix_to_word = {ix: word for word, ix in word_to_ix.items()}\n",
    "\n",
    "rat_set, ix_to_word\n",
    "def sample_words(rat_set, model, ix_to_word):\n",
    "    \n",
    "    # Sample a review\n",
    "    ix = random.randint(0,len(rat_set))\n",
    "\n",
    "    # Make a prediction\n",
    "    x = rat_set.data_tensor[ix].unsqueeze(0)\n",
    "    review = embd(Variable(x, volatile=True)).type(dtype)\n",
    "    model.alpha_iter = 1\n",
    "    model(review)\n",
    "\n",
    "    # What words were selected\n",
    "    subset = model.saved_subsets[0][0]\n",
    "    subset = pad_tensor(subset.data,0,0,412).byte()\n",
    "\n",
    "    # Convert to words\n",
    "    all_words = [ix_to_word[ix -1] for ix in x.squeeze() if ix > 0]\n",
    "    filtered_words = [ix_to_word[ix -1] for ix in x.masked_select(subset)]\n",
    "    print(\" \".join(all_words) )\n",
    "    print(\"DPP Selection: \", filtered_words)\n",
    "\n",
    "\n",
    "    ix = random.randint(0,len(rat_set))\n",
    "    rat_set.data_tensor[ix].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "   \n",
    "def sample_prediction(rat_set, model):\n",
    "    # Sample a review\n",
    "    ix = random.randint(0,len(rat_train_set))\n",
    "\n",
    "    # Make a prediction\n",
    "    x = rat_train_set.data_tensor[ix].unsqueeze(0)\n",
    "    target = rat_train_set.target_tensor[ix][:3]\n",
    "    review = embd(Variable(x, volatile=True)).type(dtype)\n",
    "    model.alpha_iter = 1\n",
    "    pred = model(review).data.squeeze()\n",
    "    print(pred, target)\n",
    "    return pred, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred, target = sample_prediction(rat_set, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion(Variable(pred), Variable(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(model.pred_net.state_dict(), 'pred_dict25.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "e = 0\n",
    "for i in range(100):\n",
    "    v = torch.normal(torch.FloatTensor([1,2,3,4,5]))\n",
    "    e += v\n",
    "e / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_lin = torch.sin\n",
    "torch.sin(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_lin(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_lin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "set_size = 3\n",
    "embd_dim = 4\n",
    "words = torch.randn(batch_size, set_size, embd_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = torch.normal(torch.FloatTensor([1,2,3,4,5])torch.cos(torch.sin(words).mean(1)).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = torch.normal(torch.FloatTensor([1,2,3,4,5]))\n",
    "torch.log(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "batch_size = 100\n",
    "n_clusters = 10\n",
    "set_size = 40\n",
    "embd_dim = pred_in = 50\n",
    "dtype = dtype = torch.DoubleTensor\n",
    "np.random.seed(0)\n",
    "means = dtype(np.random.randint(-50,50,[n_clusters, int(pred_in)]).astype(\"float\"))\n",
    "\n",
    "def generate(batch_size):\n",
    "    \"\"\"sdf\"\n",
    "    Arguments:\n",
    "    means: Probs best to make this an attribute of the class, \n",
    "    so that repeated training works with the same data distribution.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Generate index\n",
    "    index = torch.cat([torch.arange(0, float(n_clusters)).expand(batch_size, n_clusters).long(), \n",
    "                      torch.multinomial(torch.ones(batch_size, n_clusters), set_size - n_clusters, replacement=True)]\n",
    "                     ,dim=1)\n",
    "    index = index.t()[torch.randperm(set_size)].t().contiguous()\n",
    "\n",
    "    # Generate words, context, target\n",
    "    words = dtype(torch.normal(means.index_select(0,index.view(index.numel()))).view(batch_size, set_size, embd_dim))\n",
    "    context = dtype(words.sum(1).expand_as(words))\n",
    "\n",
    "    target = torch.pow(torch.log1p(words.abs()).mean(1),2).squeeze()\n",
    "\n",
    "    return words, context, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words, context, target = generate(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(torch.std(target, dim=0) / torch.mean(target, dim=0)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v1 = torch.randn(2,2)\n",
    "v2 = torch.randn(2,2)\n",
    "v3 = torch.randn(2,2)\n",
    "v4 = torch.randn(2,2)\n",
    "v5 = torch.randn(2,2)\n",
    "v6 = torch.randn(2,2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dpp_nets.my_torch.simulator import SimKDPPDeepSet\n",
    "import torch\n",
    "network_params = {'set_size': 40, 'n_clusters': 10}\n",
    "dtype = torch.DoubleTensor\n",
    "sim = SimKDPPDeepSet(network_params, dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at it  100  is:  \n",
      " 160.7437\n",
      "[torch.DoubleTensor of size 1]\n",
      "\n",
      "Loss at it  200  is:  \n",
      " 152.4175\n",
      "[torch.DoubleTensor of size 1]\n",
      "\n",
      "Loss at it  300  is:  \n",
      " 141.3500\n",
      "[torch.DoubleTensor of size 1]\n",
      "\n",
      "Loss at it  400  is:  \n",
      " 142.0629\n",
      "[torch.DoubleTensor of size 1]\n",
      "\n",
      "Loss at it  500  is:  \n",
      " 138.2602\n",
      "[torch.DoubleTensor of size 1]\n",
      "\n",
      "Loss at it  600  is:  \n",
      " 140.1330\n",
      "[torch.DoubleTensor of size 1]\n",
      "\n",
      "Loss at it  700  is:  \n",
      " 129.2678\n",
      "[torch.DoubleTensor of size 1]\n",
      "\n",
      "Loss at it  800  is:  \n",
      " 129.9090\n",
      "[torch.DoubleTensor of size 1]\n",
      "\n",
      "Loss at it  900  is:  \n",
      " 130.5670\n",
      "[torch.DoubleTensor of size 1]\n",
      "\n",
      "Loss at it  1000  is:  \n",
      " 128.3935\n",
      "[torch.DoubleTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Subset Size:  10.845999999999984\n",
      "Subset Variance:  0.9282839999999998\n",
      "Average Loss 179.53271306696047\n",
      "n_missed share 0.0023\n",
      "n_one share 0.9142\n",
      "n_many share 0.0835\n",
      "n_perfect share 0.419\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
