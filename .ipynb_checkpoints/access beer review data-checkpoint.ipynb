{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "import shutil\n",
    "import time\n",
    "import gzip\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from dpp_nets.utils.io import make_embd, make_tensor_dataset, load_tensor_dataset\n",
    "from dpp_nets.utils.io import data_iterator, load_embd\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import time\n",
    "from dpp_nets.my_torch.utilities import pad_tensor\n",
    "\n",
    "\n",
    "root = '/Users/Max/data/beer_reviews'\n",
    "data_file = 'reviews.aspect3.train.txt.gz'\n",
    "embd_file = 'review+wiki.filtered.200.txt.gz'\n",
    "save_path = os.path.join(root,'pytorch/aspect3_train.pt')\n",
    "data_path = os.path.join(root, data_file)\n",
    "embd_path = os.path.join(root, embd_file)\n",
    "\n",
    "\n",
    "def read_rationales(path):\n",
    "    \"\"\"\n",
    "    This reads the json.annotations file. \n",
    "    Creates a list of dictionaries, which holds the 994 reviews for which\n",
    "    sentence-level annotations are available. \n",
    "    \"\"\"\n",
    "    data = []\n",
    "    fopen = gzip.open if path.endswith(\".gz\") else open\n",
    "    with fopen(path) as fin:\n",
    "        for line in fin:\n",
    "            item = json.loads(line)\n",
    "            data.append(item)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from dpp_nets.my_torch.linalg import custom_decomp\n",
    "from dpp_nets.my_torch.DPP import DPP\n",
    "from dpp_nets.my_torch.DPP import AllInOne\n",
    "from dpp_nets.my_torch.utilities import compute_baseline\n",
    "\n",
    "class DPP_Classifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, dtype):\n",
    "        \n",
    "        super(DPP_Classifier, self).__init__()\n",
    "        # Float vs Double\n",
    "        self.dtype = dtype\n",
    "\n",
    "        # Network parameters\n",
    "        self.kernel_in = kernel_in = 400\n",
    "        self.kernel_h = kernel_h = 1000\n",
    "        self.kernel_out = kernel_out = 400\n",
    "\n",
    "        self.pred_in = pred_in = 200 # kernel_in / 2\n",
    "        self.pred_h = pred_h = 500\n",
    "        self.pred_h2 = pred_h2 = 200\n",
    "        self.pred_out = pred_out = 3\n",
    "        \n",
    "        # 2-Hidden-Layer Networks \n",
    "        self.kernel_net = torch.nn.Sequential(nn.Linear(kernel_in, kernel_h), nn.ELU(),\n",
    "                                              nn.Linear(kernel_h, kernel_h), nn.ELU(), \n",
    "                                              nn.Linear(kernel_h, kernel_out))\n",
    "        # 3-Hidden-Layer-Networks\n",
    "        self.pred_net = torch.nn.Sequential(nn.Linear(pred_in, pred_h), nn.ReLU(),\n",
    "                                             nn.Linear(pred_h, pred_h), nn.ReLU(),\n",
    "                                             nn.Linear(pred_h, pred_h2), nn.ReLU(),\n",
    "                                             nn.Linear(pred_h2, pred_out), nn.Sigmoid())\n",
    "        \n",
    "        self.kernel_net.type(self.dtype)\n",
    "        self.pred_net.type(self.dtype)\n",
    "        \n",
    "        # Sampling Parameter\n",
    "        self.alpha_iter = 5\n",
    "\n",
    "        # Convenience\n",
    "        self.kernels = []\n",
    "        self.subsets = None\n",
    "        self.picks = None\n",
    "        self.preds = None\n",
    "        \n",
    "        self.saved_subsets = None\n",
    "        self.saved_losses = None # not really necesary\n",
    "        self.saved_baselines = None # not really necessary\n",
    "        \n",
    "    def forward(self, reviews):\n",
    "        \"\"\"\n",
    "        reviews: batch_size x max_set_size x embd_dim = 200\n",
    "        Output: batch_size x pred_out (the prediction)\n",
    "        Challenges: Need to resize tensor appropriately and \n",
    "        measure length etc. \n",
    "        \"\"\"\n",
    "        batch_size, max_set_size, embd_dim = reviews.size()\n",
    "        alpha_iter = self.alpha_iter\n",
    "        self.saved_subsets = actions = [[] for i in range(batch_size)]\n",
    "        picks = [[] for i in range(batch_size)]\n",
    "        \n",
    "        # Create context\n",
    "        lengths = reviews.sum(2).abs().sign().sum(1)\n",
    "        context = (reviews.sum(1) / lengths.expand_as(reviews.sum(1))).expand_as(reviews)\n",
    "        mask = reviews.sum(2).abs().sign().expand_as(reviews).byte()\n",
    "\n",
    "        # Mask out zero words\n",
    "        reviews = reviews.masked_select(mask).view(-1, embd_dim)\n",
    "        context = context.masked_select(mask).view(-1, embd_dim)\n",
    "\n",
    "        # Compute batched_kernel\n",
    "        kernel_input = torch.cat([reviews, context], dim=1)\n",
    "        kernel_output = self.kernel_net(kernel_input)\n",
    "        \n",
    "        # Extract the kernel for each review from batched_kernel\n",
    "        s = list(lengths.squeeze().cumsum(0).long().data - lengths.squeeze().long().data)\n",
    "        e = list(lengths.squeeze().cumsum(0).long().data)\n",
    "\n",
    "        for i, (s, e) in enumerate(zip(s, e)):\n",
    "            review = reviews[s:e] # original review, without zero words\n",
    "            kernel = kernel_output[s:e] # corresponding kernel \n",
    "            self.kernels.append(kernel.data)\n",
    "            #vals, vecs = custom_decomp()(kernel)\n",
    "            for j in range(alpha_iter):\n",
    "                subset = AllInOne()(kernel)\n",
    "                #subset = DPP()(vals, vecs)\n",
    "                actions[i].append(subset)\n",
    "                pick = subset.diag().mm(review).sum(0)\n",
    "                picks[i].append(pick)\n",
    "\n",
    "        # Predictions\n",
    "        picks = torch.stack([torch.stack(pick) for pick in picks]).view(-1, embd_dim)\n",
    "        preds = self.pred_net(picks).view(batch_size, alpha_iter, -1)\n",
    "        \n",
    "        return preds\n",
    "\n",
    "def register_rewards(preds, targets, criterion, net):\n",
    "    \n",
    "    #targets = targets.unsqueeze(1).unsqueeze(1).expand_as(preds)\n",
    "    targets = targets.unsqueeze(1).expand_as(preds)\n",
    "    loss = criterion(preds, targets)\n",
    "    \n",
    "    actions = net.saved_subsets\n",
    "    \n",
    "    losses = ((preds - targets)**2).mean(2)\n",
    "    losses = [[i.data[0] for i in row] for row in losses]\n",
    "    net.saved_losses = losses # not really necessary\n",
    "    baselines = [compute_baseline(i) for i in losses]\n",
    "    net.saved_baselines = baselines # not really necessary\n",
    "    \n",
    "    for actions, rewards in zip(actions, baselines):\n",
    "        for action, reward in zip(actions, rewards):\n",
    "            action.reinforce(reward)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Useful Support\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        \n",
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    \"\"\"\n",
    "    This is good!\n",
    "    \"\"\"\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'model_best.pth.tar')\n",
    "        \n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    lr = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "    lr = lr * (0.1 ** (epoch // 5))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(train_loader, embd, model, criterion, optimizer, epoch, dtype):\n",
    "    \n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    subset_size = AverageMeter()\n",
    "    \n",
    "    target_dim = 3\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (review, target) in enumerate(train_loader):\n",
    "        \n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        \n",
    "        targets = Variable(target[:,:target_dim].type(dtype))\n",
    "        reviews = embd(Variable(review)).type(dtype)\n",
    "\n",
    "        # compute output\n",
    "        model.alpha_iter = 2\n",
    "        pred = model(reviews)\n",
    "        loss = register_rewards(pred, targets, criterion, model)\n",
    "\n",
    "        ##measure accuracy and record loss ????????????????????????\n",
    "        # prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n",
    "        losses.update(loss.data[0], reviews.size(0))\n",
    "        for l in model.saved_subsets:\n",
    "            for s in l:\n",
    "                subset_size.update(s.data.sum())\n",
    "        # top1.update(prec1[0], input.size(0))\n",
    "        # top5.update(prec5[0], input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        #if i % args.print_freq == 0:\n",
    "        if i % print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'SSize {subset_size.val:.2f} ({subset_size.avg: .2f})'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'.format(\n",
    "                   epoch, i, len(train_loader), batch_time=batch_time,\n",
    "                   data_time=data_time, subset_size = subset_size, loss=losses))\n",
    "\n",
    "def validate(val_loader, model, criterion):\n",
    "    \n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    t_prec = AverageMeter()\n",
    "    t_recall = AverageMeter()\n",
    "    t_tp = AverageMeter()\n",
    "    t_fp = AverageMeter()\n",
    "    t_fn = AverageMeter()\n",
    "    \n",
    "    target_dim = 3\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    # model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (review, target) in enumerate(val_loader):\n",
    "        \n",
    "        target = target.sum(1).sign().type(dtype).squeeze().byte()\n",
    "        # targets = target[:,:target_dim,:].type(dtype)\n",
    "        reviews = embd(Variable(review, volatile=True)).type(dtype)\n",
    "\n",
    "        # compute output\n",
    "        model.alpha_iter = 1\n",
    "        preds = model(reviews)\n",
    "\n",
    "        subset = model.saved_subsets[0][0]\n",
    "        subset = pad_tensor(subset.data,0,0,412).byte()\n",
    "        # target = targets\n",
    "\n",
    "        # targets = target[:,:target_dim,:].type(dtype)\n",
    "        reviews = embd(Variable(review, volatile=True)).type(dtype)\n",
    "\n",
    "        # compute output\n",
    "        model.alpha_iter = 1\n",
    "        preds = model(reviews)\n",
    "        \n",
    "        subset = model.saved_subsets[0][0]\n",
    "        subset = pad_tensor(subset.data,0,0,412).byte()\n",
    "        # target = target[:,:target_dim,:].squeeze()\n",
    "\n",
    "        retriev = subset.sum()\n",
    "        relev = target.sum()\n",
    "\n",
    "        tp = target.masked_select(subset).sum()\n",
    "        fp = (1 - target.masked_select(subset)).sum()\n",
    "        fn = (1 - subset.masked_select(target)).sum()\n",
    "        t_tp.update(tp)\n",
    "        t_fp.update(fp)\n",
    "        t_fn.update(fn)\n",
    "\n",
    "        if retriev: \n",
    "            prec = tp / retriev\n",
    "            t_prec.update(prec)\n",
    "\n",
    "        if relev: \n",
    "            recall = tp / relev\n",
    "            t_recall.update(recall)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        #prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n",
    "        #losses.update(loss.data[0], input.size(0))\n",
    "        #top1.update(prec1[0], input.size(0))\n",
    "        #top5.update(prec5[0], input.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Precision {t_prec.val:.4f} ({t_prec.avg:.4f})\\t'\n",
    "                  'Recall {t_recall.val:.4f} ({t_recall.avg:.4f})\\t'.format(\n",
    "                   i, len(val_loader), batch_time=batch_time, t_prec=t_prec, t_recall=t_recall))\n",
    "            \n",
    "    return t_prec.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### MAIN PROGRAMME\n",
    "\n",
    "\n",
    "global best_prec1\n",
    "best_prec1 = 0\n",
    "\n",
    "# set parameters\n",
    "lr = 1e-1\n",
    "momentum = 0.9\n",
    "weight_decay = 0.\n",
    "start_epoch = 0\n",
    "epochs = 1\n",
    "batch_size = 20\n",
    "print_freq = 10\n",
    "\n",
    "data = '/Users/Max/data/beer_reviews/pytorch'\n",
    "dtype = torch.DoubleTensor\n",
    "\n",
    "# create model\n",
    "embd = load_embd('/Users/Max/data/beer_reviews/pytorch/embeddings.pt')\n",
    "model = DPP_Classifier(torch.DoubleTensor)\n",
    "\n",
    "# define loss function (criterion) and optimizer\n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr,\n",
    "                            momentum=momentum,\n",
    "                            weight_decay=weight_decay)\n",
    "\n",
    "# Data loading code\n",
    "trainpath = os.path.join(data, 'aspect1_train.pt')\n",
    "valpath = os.path.join(data, 'aspect1_heldout.pt')\n",
    "ratpath = os.path.join(data, 'annotated.pt')\n",
    "\n",
    "train_set = torch.load(trainpath)\n",
    "val_set = torch.load(valpath)\n",
    "rat_set = torch.load(ratpath)\n",
    "\n",
    "rat_train_set = torch.load(os.path.join(data, 'annotated_common.pt'))\n",
    "#train_loader = DataLoader(train_set, batch_size, shuffle=True)\n",
    "#val_loader = DataLoader(val_set)\n",
    "rat_train_loader = DataLoader(rat_train_set, batch_size, shuffle=True)\n",
    "rat_loader = DataLoader(rat_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][0/50]\tTime 2.490 (2.490)\tData 0.003 (0.003)\tSSize 16.00 ( 20.30)Loss 0.0993 (0.0993)\t\n",
      "Epoch: [0][10/50]\tTime 3.409 (2.320)\tData 0.001 (0.001)\tSSize 14.00 ( 19.35)Loss 0.0844 (0.0835)\t\n",
      "Epoch: [0][20/50]\tTime 3.378 (2.552)\tData 0.000 (0.006)\tSSize 18.00 ( 19.47)Loss 0.0839 (0.0839)\t\n",
      "Epoch: [0][30/50]\tTime 3.880 (2.677)\tData 0.001 (0.004)\tSSize 19.00 ( 19.42)Loss 0.0829 (0.0850)\t\n",
      "Epoch: [0][40/50]\tTime 3.254 (2.706)\tData 0.000 (0.003)\tSSize 18.00 ( 19.44)Loss 0.0813 (0.0845)\t\n",
      "Test: [0/994]\tTime 0.207 (0.207)\tPrecision 0.3636 (0.3636)\tRecall 0.1127 (0.1127)\t\n",
      "Test: [100/994]\tTime 0.061 (0.095)\tPrecision 0.0526 (0.4306)\tRecall 0.0526 (0.1626)\t\n",
      "Test: [200/994]\tTime 0.112 (0.098)\tPrecision 0.5652 (0.4626)\tRecall 0.2500 (0.1689)\t\n",
      "Test: [300/994]\tTime 0.054 (0.097)\tPrecision 0.2500 (0.4488)\tRecall 0.1136 (0.1706)\t\n",
      "Test: [400/994]\tTime 0.048 (0.094)\tPrecision 0.5625 (0.4495)\tRecall 0.2571 (0.1749)\t\n",
      "Test: [500/994]\tTime 0.077 (0.091)\tPrecision 0.6000 (0.4525)\tRecall 0.1519 (0.1769)\t\n",
      "Test: [600/994]\tTime 0.202 (0.094)\tPrecision 0.2609 (0.4506)\tRecall 0.1091 (0.1768)\t\n",
      "Test: [700/994]\tTime 0.076 (0.092)\tPrecision 0.4615 (0.4519)\tRecall 0.1714 (0.1773)\t\n",
      "Test: [800/994]\tTime 0.193 (0.092)\tPrecision 0.4783 (0.4512)\tRecall 0.1467 (0.1795)\t\n",
      "Test: [900/994]\tTime 0.193 (0.095)\tPrecision 0.5417 (0.4506)\tRecall 0.1368 (0.1786)\t\n",
      "Epoch: [1][0/50]\tTime 1.684 (1.684)\tData 0.000 (0.000)\tSSize 18.00 ( 19.32)Loss 0.0741 (0.0741)\t\n",
      "Epoch: [1][10/50]\tTime 2.343 (2.331)\tData 0.000 (0.000)\tSSize 18.00 ( 19.40)Loss 0.0898 (0.0831)\t\n",
      "Epoch: [1][20/50]\tTime 2.401 (2.382)\tData 0.000 (0.000)\tSSize 23.00 ( 19.34)Loss 0.0921 (0.0850)\t\n",
      "Epoch: [1][30/50]\tTime 2.984 (2.717)\tData 0.000 (0.000)\tSSize 22.00 ( 19.31)Loss 0.0617 (0.0834)\t\n",
      "Epoch: [1][40/50]\tTime 3.413 (2.852)\tData 0.000 (0.000)\tSSize 18.00 ( 19.33)Loss 0.0863 (0.0840)\t\n",
      "Test: [0/994]\tTime 0.143 (0.143)\tPrecision 0.3810 (0.3810)\tRecall 0.1127 (0.1127)\t\n",
      "Test: [100/994]\tTime 0.070 (0.086)\tPrecision 0.2083 (0.4525)\tRecall 0.2632 (0.1787)\t\n",
      "Test: [200/994]\tTime 0.064 (0.095)\tPrecision 0.3043 (0.4594)\tRecall 0.1346 (0.1707)\t\n",
      "Test: [300/994]\tTime 0.054 (0.081)\tPrecision 0.3684 (0.4540)\tRecall 0.1591 (0.1765)\t\n",
      "Test: [400/994]\tTime 0.049 (0.078)\tPrecision 0.3158 (0.4539)\tRecall 0.1714 (0.1807)\t\n",
      "Test: [500/994]\tTime 0.087 (0.079)\tPrecision 0.6500 (0.4515)\tRecall 0.1646 (0.1788)\t\n",
      "Test: [600/994]\tTime 0.134 (0.083)\tPrecision 0.3000 (0.4497)\tRecall 0.1091 (0.1784)\t\n",
      "Test: [700/994]\tTime 0.051 (0.084)\tPrecision 0.7500 (0.4546)\tRecall 0.2571 (0.1804)\t\n",
      "Test: [800/994]\tTime 0.133 (0.085)\tPrecision 0.4500 (0.4514)\tRecall 0.1200 (0.1816)\t\n",
      "Test: [900/994]\tTime 0.294 (0.085)\tPrecision 0.5833 (0.4499)\tRecall 0.1474 (0.1805)\t\n",
      "Epoch: [2][0/50]\tTime 1.901 (1.901)\tData 0.001 (0.001)\tSSize 19.00 ( 18.98)Loss 0.0654 (0.0654)\t\n",
      "Epoch: [2][10/50]\tTime 3.371 (2.839)\tData 0.001 (0.001)\tSSize 17.00 ( 19.43)Loss 0.1011 (0.0853)\t\n",
      "Epoch: [2][20/50]\tTime 3.083 (2.806)\tData 0.001 (0.001)\tSSize 18.00 ( 19.46)Loss 0.0639 (0.0826)\t\n",
      "Epoch: [2][30/50]\tTime 2.527 (2.914)\tData 0.001 (0.001)\tSSize 23.00 ( 19.46)Loss 0.0828 (0.0839)\t\n",
      "Epoch: [2][40/50]\tTime 2.328 (2.823)\tData 0.000 (0.001)\tSSize 12.00 ( 19.36)Loss 0.0760 (0.0841)\t\n",
      "Test: [0/994]\tTime 0.121 (0.121)\tPrecision 0.4583 (0.4583)\tRecall 0.1549 (0.1549)\t\n",
      "Test: [100/994]\tTime 0.043 (0.060)\tPrecision 0.1765 (0.4483)\tRecall 0.1579 (0.1705)\t\n",
      "Test: [200/994]\tTime 0.067 (0.072)\tPrecision 0.4737 (0.4632)\tRecall 0.1731 (0.1686)\t\n",
      "Test: [300/994]\tTime 0.078 (0.076)\tPrecision 0.3333 (0.4547)\tRecall 0.1591 (0.1723)\t\n",
      "Test: [400/994]\tTime 0.066 (0.080)\tPrecision 0.6667 (0.4541)\tRecall 0.3429 (0.1765)\t\n",
      "Test: [500/994]\tTime 0.114 (0.078)\tPrecision 0.4737 (0.4554)\tRecall 0.1139 (0.1782)\t\n",
      "Test: [600/994]\tTime 0.102 (0.078)\tPrecision 0.5000 (0.4512)\tRecall 0.1636 (0.1769)\t\n",
      "Test: [700/994]\tTime 0.046 (0.081)\tPrecision 0.6154 (0.4536)\tRecall 0.2286 (0.1776)\t\n",
      "Test: [800/994]\tTime 0.184 (0.088)\tPrecision 0.3200 (0.4496)\tRecall 0.1067 (0.1784)\t\n",
      "Test: [900/994]\tTime 0.092 (0.086)\tPrecision 0.4583 (0.4493)\tRecall 0.1158 (0.1781)\t\n",
      "Epoch: [3][0/50]\tTime 2.418 (2.418)\tData 0.001 (0.001)\tSSize 12.00 ( 20.07)Loss 0.0740 (0.0740)\t\n",
      "Epoch: [3][10/50]\tTime 3.357 (2.639)\tData 0.001 (0.001)\tSSize 19.00 ( 19.49)Loss 0.0821 (0.0846)\t\n",
      "Epoch: [3][20/50]\tTime 2.316 (2.467)\tData 0.000 (0.001)\tSSize 19.00 ( 19.41)Loss 0.0765 (0.0849)\t\n",
      "Epoch: [3][30/50]\tTime 1.582 (2.446)\tData 0.000 (0.001)\tSSize 21.00 ( 19.31)Loss 0.0706 (0.0851)\t\n",
      "Epoch: [3][40/50]\tTime 2.907 (2.550)\tData 0.000 (0.001)\tSSize 19.00 ( 19.42)Loss 0.0796 (0.0849)\t\n",
      "Test: [0/994]\tTime 0.150 (0.150)\tPrecision 0.5217 (0.5217)\tRecall 0.1690 (0.1690)\t\n",
      "Test: [100/994]\tTime 0.060 (0.086)\tPrecision 0.1579 (0.4574)\tRecall 0.1579 (0.1780)\t\n",
      "Test: [200/994]\tTime 0.081 (0.096)\tPrecision 0.5455 (0.4650)\tRecall 0.2308 (0.1710)\t\n",
      "Test: [300/994]\tTime 0.081 (0.083)\tPrecision 0.4444 (0.4541)\tRecall 0.1818 (0.1730)\t\n",
      "Test: [400/994]\tTime 0.055 (0.084)\tPrecision 0.4706 (0.4522)\tRecall 0.2286 (0.1755)\t\n",
      "Test: [500/994]\tTime 0.077 (0.083)\tPrecision 0.7619 (0.4533)\tRecall 0.2025 (0.1759)\t\n",
      "Test: [600/994]\tTime 0.118 (0.086)\tPrecision 0.5000 (0.4521)\tRecall 0.1636 (0.1761)\t\n",
      "Test: [700/994]\tTime 0.043 (0.084)\tPrecision 0.6667 (0.4525)\tRecall 0.2857 (0.1760)\t\n",
      "Test: [800/994]\tTime 0.156 (0.083)\tPrecision 0.2500 (0.4481)\tRecall 0.0800 (0.1763)\t\n",
      "Test: [900/994]\tTime 0.138 (0.083)\tPrecision 0.3636 (0.4484)\tRecall 0.0842 (0.1759)\t\n",
      "Epoch: [4][0/50]\tTime 1.908 (1.908)\tData 0.001 (0.001)\tSSize 16.00 ( 19.32)Loss 0.0839 (0.0839)\t\n",
      "Epoch: [4][10/50]\tTime 2.814 (2.661)\tData 0.000 (0.000)\tSSize 18.00 ( 19.44)Loss 0.0747 (0.0842)\t\n",
      "Epoch: [4][20/50]\tTime 2.698 (2.521)\tData 0.000 (0.000)\tSSize 25.00 ( 19.39)Loss 0.0868 (0.0833)\t\n",
      "Epoch: [4][30/50]\tTime 1.716 (2.650)\tData 0.001 (0.001)\tSSize 17.00 ( 19.40)Loss 0.0906 (0.0840)\t\n",
      "Epoch: [4][40/50]\tTime 2.346 (2.563)\tData 0.001 (0.001)\tSSize 25.00 ( 19.37)Loss 0.0912 (0.0829)\t\n",
      "Test: [0/994]\tTime 0.148 (0.148)\tPrecision 0.4545 (0.4545)\tRecall 0.1408 (0.1408)\t\n",
      "Test: [100/994]\tTime 0.082 (0.106)\tPrecision 0.1667 (0.4473)\tRecall 0.1579 (0.1759)\t\n",
      "Test: [200/994]\tTime 0.080 (0.100)\tPrecision 0.5238 (0.4705)\tRecall 0.2115 (0.1733)\t\n",
      "Test: [300/994]\tTime 0.084 (0.097)\tPrecision 0.4762 (0.4566)\tRecall 0.2273 (0.1743)\t\n",
      "Test: [400/994]\tTime 0.053 (0.096)\tPrecision 0.5000 (0.4558)\tRecall 0.2571 (0.1768)\t\n",
      "Test: [500/994]\tTime 0.052 (0.100)\tPrecision 0.5882 (0.4558)\tRecall 0.1266 (0.1776)\t\n",
      "Test: [600/994]\tTime 0.089 (0.096)\tPrecision 0.5500 (0.4513)\tRecall 0.2000 (0.1762)\t\n",
      "Test: [700/994]\tTime 0.039 (0.094)\tPrecision 0.5294 (0.4536)\tRecall 0.2571 (0.1779)\t\n",
      "Test: [800/994]\tTime 0.177 (0.092)\tPrecision 0.2632 (0.4514)\tRecall 0.0667 (0.1792)\t\n",
      "Test: [900/994]\tTime 0.098 (0.092)\tPrecision 0.3478 (0.4490)\tRecall 0.0842 (0.1783)\t\n",
      "Epoch: [5][0/50]\tTime 2.480 (2.480)\tData 0.001 (0.001)\tSSize 19.00 ( 19.07)Loss 0.0707 (0.0707)\t\n",
      "Epoch: [5][10/50]\tTime 2.297 (2.445)\tData 0.001 (0.001)\tSSize 18.00 ( 19.47)Loss 0.0845 (0.0863)\t\n",
      "Epoch: [5][20/50]\tTime 2.114 (2.550)\tData 0.000 (0.001)\tSSize 18.00 ( 19.46)Loss 0.0796 (0.0820)\t\n",
      "Epoch: [5][30/50]\tTime 1.843 (2.559)\tData 0.000 (0.001)\tSSize 22.00 ( 19.45)Loss 0.0964 (0.0836)\t\n",
      "Epoch: [5][40/50]\tTime 2.682 (2.531)\tData 0.000 (0.001)\tSSize 18.00 ( 19.41)Loss 0.0833 (0.0842)\t\n",
      "Test: [0/994]\tTime 0.128 (0.128)\tPrecision 0.3043 (0.3043)\tRecall 0.0986 (0.0986)\t\n",
      "Test: [100/994]\tTime 0.045 (0.088)\tPrecision 0.3000 (0.4539)\tRecall 0.3158 (0.1774)\t\n",
      "Test: [200/994]\tTime 0.070 (0.080)\tPrecision 0.5714 (0.4730)\tRecall 0.2308 (0.1763)\t\n",
      "Test: [300/994]\tTime 0.083 (0.081)\tPrecision 0.5500 (0.4592)\tRecall 0.2500 (0.1774)\t\n",
      "Test: [400/994]\tTime 0.073 (0.085)\tPrecision 0.4444 (0.4609)\tRecall 0.2286 (0.1800)\t\n",
      "Test: [500/994]\tTime 0.052 (0.083)\tPrecision 0.5263 (0.4611)\tRecall 0.1266 (0.1804)\t\n",
      "Test: [600/994]\tTime 0.066 (0.081)\tPrecision 0.4583 (0.4533)\tRecall 0.2000 (0.1778)\t\n",
      "Test: [700/994]\tTime 0.038 (0.079)\tPrecision 0.5625 (0.4533)\tRecall 0.2571 (0.1772)\t\n",
      "Test: [800/994]\tTime 0.132 (0.079)\tPrecision 0.4000 (0.4501)\tRecall 0.1067 (0.1779)\t\n",
      "Test: [900/994]\tTime 0.195 (0.080)\tPrecision 0.3913 (0.4485)\tRecall 0.0947 (0.1765)\t\n",
      "Epoch: [6][0/50]\tTime 1.973 (1.973)\tData 0.000 (0.000)\tSSize 13.00 ( 19.80)Loss 0.0939 (0.0939)\t\n",
      "Epoch: [6][10/50]\tTime 2.916 (2.315)\tData 0.001 (0.001)\tSSize 19.00 ( 19.47)Loss 0.0735 (0.0850)\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [6][20/50]\tTime 2.776 (2.395)\tData 0.000 (0.000)\tSSize 19.00 ( 19.44)Loss 0.0705 (0.0838)\t\n",
      "Epoch: [6][30/50]\tTime 3.299 (2.404)\tData 0.000 (0.000)\tSSize 18.00 ( 19.47)Loss 0.0667 (0.0836)\t\n",
      "Epoch: [6][40/50]\tTime 2.417 (2.342)\tData 0.000 (0.000)\tSSize 18.00 ( 19.42)Loss 0.0657 (0.0837)\t\n",
      "Test: [0/994]\tTime 0.144 (0.144)\tPrecision 0.4545 (0.4545)\tRecall 0.1408 (0.1408)\t\n",
      "Test: [100/994]\tTime 0.042 (0.062)\tPrecision 0.2500 (0.4491)\tRecall 0.2632 (0.1697)\t\n",
      "Test: [200/994]\tTime 0.139 (0.071)\tPrecision 0.5238 (0.4712)\tRecall 0.2115 (0.1715)\t\n",
      "Test: [300/994]\tTime 0.078 (0.074)\tPrecision 0.2857 (0.4588)\tRecall 0.1364 (0.1744)\t\n",
      "Test: [400/994]\tTime 0.075 (0.077)\tPrecision 0.3529 (0.4553)\tRecall 0.1714 (0.1764)\t\n",
      "Test: [500/994]\tTime 0.078 (0.083)\tPrecision 0.7391 (0.4532)\tRecall 0.2152 (0.1751)\t\n",
      "Test: [600/994]\tTime 0.063 (0.079)\tPrecision 0.2857 (0.4495)\tRecall 0.1091 (0.1739)\t\n",
      "Test: [700/994]\tTime 0.039 (0.077)\tPrecision 0.6667 (0.4521)\tRecall 0.2857 (0.1752)\t\n",
      "Test: [800/994]\tTime 0.105 (0.076)\tPrecision 0.3182 (0.4492)\tRecall 0.0933 (0.1768)\t\n",
      "Test: [900/994]\tTime 0.140 (0.076)\tPrecision 0.3200 (0.4503)\tRecall 0.0842 (0.1771)\t\n",
      "Epoch: [7][0/50]\tTime 3.546 (3.546)\tData 0.001 (0.001)\tSSize 24.00 ( 19.27)Loss 0.0640 (0.0640)\t\n",
      "Epoch: [7][10/50]\tTime 2.173 (2.428)\tData 0.000 (0.000)\tSSize 18.00 ( 19.45)Loss 0.0813 (0.0830)\t\n",
      "Epoch: [7][20/50]\tTime 1.732 (2.517)\tData 0.000 (0.000)\tSSize 22.00 ( 19.43)Loss 0.0778 (0.0816)\t\n",
      "Epoch: [7][30/50]\tTime 3.605 (2.565)\tData 0.000 (0.000)\tSSize 15.00 ( 19.44)Loss 0.0704 (0.0830)\t\n",
      "Epoch: [7][40/50]\tTime 2.603 (2.492)\tData 0.000 (0.000)\tSSize 18.00 ( 19.40)Loss 0.0769 (0.0836)\t\n",
      "Test: [0/994]\tTime 0.121 (0.121)\tPrecision 0.3043 (0.3043)\tRecall 0.0986 (0.0986)\t\n",
      "Test: [100/994]\tTime 0.044 (0.058)\tPrecision 0.2500 (0.4379)\tRecall 0.2632 (0.1667)\t\n",
      "Test: [200/994]\tTime 0.069 (0.068)\tPrecision 0.2857 (0.4587)\tRecall 0.1154 (0.1673)\t\n",
      "Test: [300/994]\tTime 0.081 (0.070)\tPrecision 0.2857 (0.4516)\tRecall 0.1364 (0.1705)\t\n",
      "Test: [400/994]\tTime 0.047 (0.073)\tPrecision 0.5294 (0.4536)\tRecall 0.2571 (0.1755)\t\n",
      "Test: [500/994]\tTime 0.083 (0.078)\tPrecision 0.5000 (0.4531)\tRecall 0.1392 (0.1757)\t\n",
      "Test: [600/994]\tTime 0.058 (0.074)\tPrecision 0.2000 (0.4484)\tRecall 0.0727 (0.1748)\t\n",
      "Test: [700/994]\tTime 0.031 (0.072)\tPrecision 0.7647 (0.4502)\tRecall 0.3714 (0.1754)\t\n",
      "Test: [800/994]\tTime 0.109 (0.071)\tPrecision 0.3448 (0.4473)\tRecall 0.1333 (0.1766)\t\n",
      "Test: [900/994]\tTime 0.131 (0.073)\tPrecision 0.3043 (0.4450)\tRecall 0.0737 (0.1756)\t\n",
      "Epoch: [8][0/50]\tTime 2.470 (2.470)\tData 0.000 (0.000)\tSSize 21.00 ( 18.20)Loss 0.0798 (0.0798)\t\n",
      "Epoch: [8][10/50]\tTime 2.969 (2.467)\tData 0.000 (0.001)\tSSize 18.00 ( 19.37)Loss 0.0813 (0.0855)\t\n",
      "Epoch: [8][20/50]\tTime 1.768 (2.380)\tData 0.000 (0.001)\tSSize 16.00 ( 19.41)Loss 0.0718 (0.0834)\t\n",
      "Epoch: [8][30/50]\tTime 1.602 (2.401)\tData 0.000 (0.001)\tSSize 20.00 ( 19.34)Loss 0.1067 (0.0840)\t\n",
      "Epoch: [8][40/50]\tTime 2.443 (2.387)\tData 0.000 (0.001)\tSSize 21.00 ( 19.41)Loss 0.0763 (0.0839)\t\n",
      "Test: [0/994]\tTime 0.154 (0.154)\tPrecision 0.3478 (0.3478)\tRecall 0.1127 (0.1127)\t\n",
      "Test: [100/994]\tTime 0.062 (0.076)\tPrecision 0.2105 (0.4281)\tRecall 0.2105 (0.1660)\t\n",
      "Test: [200/994]\tTime 0.077 (0.089)\tPrecision 0.5000 (0.4509)\tRecall 0.1923 (0.1656)\t\n",
      "Test: [300/994]\tTime 0.097 (0.089)\tPrecision 0.2593 (0.4479)\tRecall 0.1591 (0.1717)\t\n",
      "Test: [400/994]\tTime 0.033 (0.086)\tPrecision 0.4211 (0.4502)\tRecall 0.2286 (0.1775)\t\n",
      "Test: [500/994]\tTime 0.055 (0.079)\tPrecision 0.6364 (0.4528)\tRecall 0.1772 (0.1789)\t\n",
      "Test: [600/994]\tTime 0.082 (0.078)\tPrecision 0.5000 (0.4499)\tRecall 0.1636 (0.1781)\t\n",
      "Test: [700/994]\tTime 0.040 (0.077)\tPrecision 0.5625 (0.4545)\tRecall 0.2571 (0.1802)\t\n",
      "Test: [800/994]\tTime 0.115 (0.078)\tPrecision 0.4762 (0.4531)\tRecall 0.1333 (0.1812)\t\n",
      "Test: [900/994]\tTime 0.135 (0.080)\tPrecision 0.4783 (0.4528)\tRecall 0.1158 (0.1810)\t\n",
      "Epoch: [9][0/50]\tTime 1.824 (1.824)\tData 0.000 (0.000)\tSSize 24.00 ( 19.27)Loss 0.0871 (0.0871)\t\n",
      "Epoch: [9][10/50]\tTime 2.588 (2.295)\tData 0.000 (0.000)\tSSize 16.00 ( 19.45)Loss 0.0702 (0.0830)\t\n",
      "Epoch: [9][20/50]\tTime 2.802 (2.369)\tData 0.001 (0.000)\tSSize 15.00 ( 19.40)Loss 0.0740 (0.0846)\t\n",
      "Epoch: [9][30/50]\tTime 3.388 (2.455)\tData 0.000 (0.000)\tSSize 18.00 ( 19.31)Loss 0.0977 (0.0855)\t\n",
      "Epoch: [9][40/50]\tTime 2.369 (2.447)\tData 0.000 (0.000)\tSSize 22.00 ( 19.28)Loss 0.0861 (0.0846)\t\n",
      "Test: [0/994]\tTime 0.172 (0.172)\tPrecision 0.4167 (0.4167)\tRecall 0.1408 (0.1408)\t\n",
      "Test: [100/994]\tTime 0.126 (0.091)\tPrecision 0.1765 (0.4553)\tRecall 0.1579 (0.1755)\t\n",
      "Test: [200/994]\tTime 0.095 (0.098)\tPrecision 0.4118 (0.4552)\tRecall 0.1346 (0.1656)\t\n",
      "Test: [300/994]\tTime 0.081 (0.090)\tPrecision 0.5455 (0.4547)\tRecall 0.2727 (0.1730)\t\n",
      "Test: [400/994]\tTime 0.074 (0.088)\tPrecision 0.5000 (0.4523)\tRecall 0.2857 (0.1760)\t\n",
      "Test: [500/994]\tTime 0.080 (0.087)\tPrecision 0.5909 (0.4517)\tRecall 0.1646 (0.1770)\t\n",
      "Test: [600/994]\tTime 0.076 (0.090)\tPrecision 0.5455 (0.4533)\tRecall 0.2182 (0.1782)\t\n",
      "Test: [700/994]\tTime 0.030 (0.089)\tPrecision 0.6875 (0.4556)\tRecall 0.3143 (0.1795)\t\n",
      "Test: [800/994]\tTime 0.150 (0.087)\tPrecision 0.3636 (0.4523)\tRecall 0.1067 (0.1802)\t\n",
      "Test: [900/994]\tTime 0.187 (0.087)\tPrecision 0.3913 (0.4492)\tRecall 0.0947 (0.1791)\t\n",
      "Epoch: [10][0/50]\tTime 2.387 (2.387)\tData 0.000 (0.000)\tSSize 25.00 ( 19.23)Loss 0.1227 (0.1227)\t\n",
      "Epoch: [10][10/50]\tTime 3.172 (2.780)\tData 0.001 (0.001)\tSSize 18.00 ( 19.39)Loss 0.0967 (0.0890)\t\n",
      "Epoch: [10][20/50]\tTime 2.783 (2.543)\tData 0.001 (0.001)\tSSize 21.00 ( 19.36)Loss 0.0992 (0.0894)\t\n",
      "Epoch: [10][30/50]\tTime 1.702 (2.593)\tData 0.000 (0.001)\tSSize 17.00 ( 19.44)Loss 0.1031 (0.0874)\t\n",
      "Epoch: [10][40/50]\tTime 2.560 (2.566)\tData 0.000 (0.001)\tSSize 17.00 ( 19.42)Loss 0.0771 (0.0847)\t\n",
      "Test: [0/994]\tTime 0.158 (0.158)\tPrecision 0.3750 (0.3750)\tRecall 0.1268 (0.1268)\t\n",
      "Test: [100/994]\tTime 0.057 (0.072)\tPrecision 0.2000 (0.4443)\tRecall 0.2105 (0.1695)\t\n",
      "Test: [200/994]\tTime 0.075 (0.079)\tPrecision 0.4211 (0.4631)\tRecall 0.1538 (0.1713)\t\n",
      "Test: [300/994]\tTime 0.280 (0.084)\tPrecision 0.4500 (0.4534)\tRecall 0.2045 (0.1731)\t\n",
      "Test: [400/994]\tTime 0.036 (0.086)\tPrecision 0.4500 (0.4539)\tRecall 0.2571 (0.1771)\t\n",
      "Test: [500/994]\tTime 0.055 (0.079)\tPrecision 0.5238 (0.4563)\tRecall 0.1392 (0.1790)\t\n",
      "Test: [600/994]\tTime 0.076 (0.080)\tPrecision 0.3333 (0.4522)\tRecall 0.1273 (0.1780)\t\n",
      "Test: [700/994]\tTime 0.036 (0.080)\tPrecision 0.7692 (0.4539)\tRecall 0.2857 (0.1786)\t\n",
      "Test: [800/994]\tTime 0.139 (0.080)\tPrecision 0.2727 (0.4497)\tRecall 0.0800 (0.1787)\t\n",
      "Test: [900/994]\tTime 0.140 (0.082)\tPrecision 0.2917 (0.4485)\tRecall 0.0737 (0.1778)\t\n",
      "Epoch: [11][0/50]\tTime 1.697 (1.697)\tData 0.000 (0.000)\tSSize 21.00 ( 19.48)Loss 0.0894 (0.0894)\t\n",
      "Epoch: [11][10/50]\tTime 2.943 (2.472)\tData 0.000 (0.000)\tSSize 17.00 ( 19.32)Loss 0.1195 (0.0905)\t\n",
      "Epoch: [11][20/50]\tTime 2.254 (2.443)\tData 0.000 (0.000)\tSSize 21.00 ( 19.32)Loss 0.0860 (0.0883)\t\n",
      "Epoch: [11][30/50]\tTime 2.510 (2.550)\tData 0.000 (0.000)\tSSize 25.00 ( 19.42)Loss 0.0599 (0.0862)\t\n",
      "Epoch: [11][40/50]\tTime 1.810 (2.524)\tData 0.000 (0.000)\tSSize 17.00 ( 19.47)Loss 0.0780 (0.0855)\t\n",
      "Test: [0/994]\tTime 0.164 (0.164)\tPrecision 0.3182 (0.3182)\tRecall 0.0986 (0.0986)\t\n",
      "Test: [100/994]\tTime 0.042 (0.075)\tPrecision 0.2500 (0.4509)\tRecall 0.2632 (0.1768)\t\n",
      "Test: [200/994]\tTime 0.049 (0.066)\tPrecision 0.5556 (0.4657)\tRecall 0.1923 (0.1733)\t\n",
      "Test: [300/994]\tTime 0.065 (0.069)\tPrecision 0.3158 (0.4569)\tRecall 0.1364 (0.1756)\t\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "\n",
    "    # train for one epoch\n",
    "    train(rat_train_loader, embd, model, criterion, optimizer, epoch, dtype)\n",
    "\n",
    "    # evaluate on validation set\n",
    "    prec1 = validate(rat_loader, model, criterion)\n",
    "\n",
    "# remember best prec@1 and save checkpoint\n",
    "is_best = prec1 > best_prec1\n",
    "best_prec1 = max(prec1, best_prec1)\n",
    "save_checkpoint({\n",
    "    'epoch': epoch + 1,\n",
    "    'state_dict': model.state_dict(),\n",
    "    'best_prec1': best_prec1,\n",
    "    'optimizer' : optimizer.state_dict(),\n",
    "}, is_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "#word_to_ix = make_embd(embd_path, only_index_dict=True)\n",
    "#ix_to_word = {ix: word for word, ix in word_to_ix.items()}\n",
    "\n",
    "rat_set, ix_to_word\n",
    "def sample_words(rat_set, model, ix_to_word):\n",
    "    \n",
    "    # Sample a review\n",
    "    ix = random.randint(0,len(rat_set))\n",
    "\n",
    "    # Make a prediction\n",
    "    x = rat_set.data_tensor[ix].unsqueeze(0)\n",
    "    review = embd(Variable(x, volatile=True)).type(dtype)\n",
    "    model.alpha_iter = 1\n",
    "    model(review)\n",
    "\n",
    "    # What words were selected\n",
    "    subset = model.saved_subsets[0][0]\n",
    "    subset = pad_tensor(subset.data,0,0,412).byte()\n",
    "\n",
    "    # Convert to words\n",
    "    all_words = [ix_to_word[ix -1] for ix in x.squeeze() if ix > 0]\n",
    "    filtered_words = [ix_to_word[ix -1] for ix in x.masked_select(subset)]\n",
    "    print(\" \".join(all_words) )\n",
    "    print(\"DPP Selection: \", filtered_words)\n",
    "\n",
    "\n",
    "    ix = random.randint(0,len(rat_set))\n",
    "    rat_set.data_tensor[ix].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "   \n",
    "def sample_prediction(rat_set, model):\n",
    "    # Sample a review\n",
    "    ix = random.randint(0,len(rat_train_set))\n",
    "\n",
    "    # Make a prediction\n",
    "    x = rat_train_set.data_tensor[ix].unsqueeze(0)\n",
    "    target = rat_train_set.target_tensor[ix][:3]\n",
    "    review = embd(Variable(x, volatile=True)).type(dtype)\n",
    "    model.alpha_iter = 1\n",
    "    pred = model(review).data.squeeze()\n",
    "    print(pred, target)\n",
    "    return pred, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.7798\n",
      " 0.7547\n",
      " 0.7600\n",
      "[torch.DoubleTensor of size 3]\n",
      " \n",
      " 0.8000\n",
      " 0.9000\n",
      " 0.8000\n",
      "[torch.FloatTensor of size 3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred, target = sample_prediction(rat_set, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "DoubleMSECriterion_updateOutput received an invalid combination of arguments - got (\u001b[32;1mint\u001b[0m, \u001b[32;1mtorch.DoubleTensor\u001b[0m, \u001b[31;1mtorch.FloatTensor\u001b[0m, \u001b[32;1mtorch.DoubleTensor\u001b[0m, \u001b[32;1mbool\u001b[0m), but expected (int state, torch.DoubleTensor input, torch.DoubleTensor target, torch.DoubleTensor output, bool sizeAverage)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-153-0c03a1158713>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/Max/Coding/anaconda2/envs/torch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Max/Coding/anaconda2/envs/torch/lib/python3.6/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0m_assert_no_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mbackend_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbackend_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Max/Coding/anaconda2/envs/torch/lib/python3.6/site-packages/torch/nn/_functions/thnn/auto.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         getattr(self._backend, update_output.name)(self._backend.library_state, input, target,\n\u001b[0;32m---> 41\u001b[0;31m                                                    output, *self.additional_args)\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: DoubleMSECriterion_updateOutput received an invalid combination of arguments - got (\u001b[32;1mint\u001b[0m, \u001b[32;1mtorch.DoubleTensor\u001b[0m, \u001b[31;1mtorch.FloatTensor\u001b[0m, \u001b[32;1mtorch.DoubleTensor\u001b[0m, \u001b[32;1mbool\u001b[0m), but expected (int state, torch.DoubleTensor input, torch.DoubleTensor target, torch.DoubleTensor output, bool sizeAverage)"
     ]
    }
   ],
   "source": [
    "criterion(Variable(pred), Variable(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(model.pred_net.state_dict(), 'pred_dict25.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
