{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotations contains the rationals. Roughly 1000, 994 to be precise.\n",
    "# heldouts seem to be the validation sets\n",
    "# what is the test set? just the 994 reviews that are also annotated? or all data?\n",
    "# review + wiki_filtered-200 could be the word embeddings. \n",
    "\n",
    "filepath = \"/Users/Max/Coding/Projects/Beeradvocate.txt\"\n",
    "filtered =  \"/Users/Max/Coding/Projects/review+wiki_filtered_200.txt\"\n",
    "anno = \"/Users/Max/Coding/Projects/annotations.json\"\n",
    "heldout = \"/Users/Max/Coding/Projects/reviews_aspect1_heldout.txt\"\n",
    "count = 0\n",
    "with open(anno, 'r') as f:\n",
    "    for line in f:\n",
    "        count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import word embedding \n",
    "# technically don't really need the explicit embedding layer\n",
    "# it depends on how I would like to store X\n",
    "# need to think about it \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "\n",
    "def load_embedding_iterator(path):\n",
    "    file_open = gzip.open if path.endswith(\".gz\") else open\n",
    "    with file_open(path) as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                parts = line.split()\n",
    "                word = parts[0]\n",
    "                vals = np.array([float(x) for x in parts[1:]])\n",
    "                yield word, vals\n",
    "                \n",
    "index_to_word = {}\n",
    "index_to_vals = {}\n",
    "\n",
    "for index, (word, vals) in enumerate(load_embedding_iterator(filtered)):\n",
    "    index_to_word[index] = word\n",
    "    index_to_vals[index] = vals\n",
    "    \n",
    "word_to_index = {v: k for k,v in index_to_word.items()}\n",
    "    \n",
    "my_array = np.empty([147759,200])\n",
    "for i, el in enumerate(index_to_vals.values()): \n",
    "    my_array[i] = el\n",
    "    \n",
    "    \n",
    "my_state_dict = OrderedDict([('weight',torch.Tensor(my_array))])\n",
    "\n",
    "embd = nn.Embedding(147759,200)\n",
    "embd.load_state_dict(my_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "errors = []\n",
    "data_x_enc =[]\n",
    "\n",
    "for bag in validation_x:\n",
    "    temp = []\n",
    "    for word in bag:\n",
    "        if word in word_to_index:\n",
    "            temp.append(word_to_index[word])\n",
    "        else:\n",
    "            errors.append(word)\n",
    "    data_x_enc.append(temp)word_to_index['with']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import gzip\n",
    "import numpy as np\n",
    "\n",
    "def read_annotations(path):\n",
    "    \"\"\"\n",
    "    This reads in the original data set.  \n",
    "    \"\"\"\n",
    "    data_x, data_y = [ ], [ ]\n",
    "    fopen = gzip.open if path.endswith(\".gz\") else open\n",
    "    with fopen(path) as fin:\n",
    "        for line in fin:\n",
    "            y, sep, x = line.partition(\"\\t\")\n",
    "            x, y = x.split(), y.split()\n",
    "            if len(x) == 0: continue\n",
    "            y = np.asarray([ float(v) for v in y ])\n",
    "            data_x.append(x)\n",
    "            data_y.append(y)\n",
    "    say(\"{} examples loaded from {}\\n\".format(\n",
    "            len(data_x), path\n",
    "        ))\n",
    "    say(\"max text length: {}\\n\".format(\n",
    "        max(len(x) for x in data_x)\n",
    "    ))\n",
    "    return data_x, data_y\n",
    "\n",
    "def read_rationales(path):\n",
    "    \"\"\"\n",
    "    This reads the json.annotations file. \n",
    "    Creates a list of dictionaries, which holds the 994 reviews for which\n",
    "    sentence-level annotations are available. \n",
    "    \"\"\"\n",
    "    data = [ ]\n",
    "    fopen = gzip.open if path.endswith(\".gz\") else open\n",
    "    with fopen(path) as fin:\n",
    "        for line in fin:\n",
    "            item = json.loads(line)\n",
    "            data.append(item)\n",
    "    return data\n",
    "\n",
    "def read_corpus(path):\n",
    "    with open(path) as fin:\n",
    "        lines = fin.readlines()\n",
    "    lines = [ x.strip().split() for x in lines ]\n",
    "    lines = [ x for x in lines if x ]\n",
    "    corpus_x = [ x[1:] for x in lines ]\n",
    "    corpus_y = [ int(x[0]) for x in lines ]\n",
    "    return corpus_x, corpus_y\n",
    "\n",
    "def say(s, stream=sys.stdout):\n",
    "    stream.write(\"{}\".format(s))\n",
    "    stream.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "annotated_rationales = read_rationales(anno)\n",
    "validation_x, validation_y = read_annotations(heldout)\n",
    "load_embedding_iterator(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_embedding_layer(path):\n",
    "    embedding_layer = EmbeddingLayer(\n",
    "            n_d = 200,\n",
    "            vocab = [ \"<unk>\", \"<padding>\" ],\n",
    "            embs = load_embedding_iterator(path),\n",
    "            oov = \"<unk>\",\n",
    "            #fix_init_embs = True\n",
    "            fix_init_embs = False\n",
    "        )\n",
    "    return embedding_layer\n",
    "\n",
    "\n",
    "def create_batches(x, y, batch_size, padding_id, sort=True):\n",
    "    batches_x, batches_y = [ ], [ ]\n",
    "    N = len(x)\n",
    "    M = (N-1)/batch_size + 1\n",
    "    if sort:\n",
    "        perm = range(N)\n",
    "        perm = sorted(perm, key=lambda i: len(x[i]))\n",
    "        x = [ x[i] for i in perm ]\n",
    "        y = [ y[i] for i in perm ]\n",
    "    for i in xrange(M):\n",
    "        bx, by = create_one_batch(\n",
    "                    x[i*batch_size:(i+1)*batch_size],\n",
    "                    y[i*batch_size:(i+1)*batch_size],\n",
    "                    padding_id\n",
    "                )\n",
    "        batches_x.append(bx)\n",
    "        batches_y.append(by)\n",
    "    if sort:\n",
    "        random.seed(5817)\n",
    "        perm2 = range(M)\n",
    "        random.shuffle(perm2)\n",
    "        batches_x = [ batches_x[i] for i in perm2 ]\n",
    "        batches_y = [ batches_y[i] for i in perm2 ]\n",
    "    return batches_x, batches_y\n",
    "\n",
    "def create_one_batch(lstx, lsty, padding_id):\n",
    "    max_len = max(len(x) for x in lstx)\n",
    "    assert min(len(x) for x in lstx) > 0\n",
    "    bx = np.column_stack([ np.pad(x, (max_len-len(x),0), \"constant\",\n",
    "                        constant_values=padding_id) for x in lstx ])\n",
    "    by = np.vstack(lsty).astype(theano.config.floatX)\n",
    "    return bx, by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embd = nn.Embedding(147759,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embd.weight.data[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embd(Variable(torch.LongTensor([1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embd(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters = ifilter(lambda p: p.requires_grad, net.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list(index_to_vals.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(index_to_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index_to_vals[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(index_to_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = Variable(torch.randn(2,3,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_size = x.size(0)\n",
    "x = nn.Linear()(x)\n",
    "x = x.view(b_size, -1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = x.view(-1, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "V = np.array([[1,2,3,1],[4,5,1,4],[3,2,2,2],[1,1,4,2]])\n",
    "L = V.dot(V.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "u, s, vT = np.linalg.svd(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.det(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "P = np.array([[1,0,0,0],[0,0,1,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submat = P.dot(L).dot(P.T)\n",
    "submat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P.dot(u).dot(np.diag(s)).dot(vT).dot(P.T).dot(P).dot(u).dot(np.diag(1/s)).dot(vT).dot(P.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submat_inv = np.linalg.inv(submat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P.dot(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u.dot(np.diag(1/s)).dot(vT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.inv(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vT.dot(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u.T.dot(P.T).dot(submat_inv).dot(P).dot(u).dot(np.diag(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.random.randn(5,5)\n",
    "B = np.random.randn(5,5)\n",
    "C = np.random.randn(5,5)\n",
    "(A + B).dot(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " A.dot(C) + B.dot(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.arange(5,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mm(A.unsqueeze(1),A.unsqueeze(1).t())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 / (A.expand(5,5) - A.expand(5,5).t())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.randn(10,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "u, s, v = torch.svd(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_u = torch.randn(10,10)\n",
    "\n",
    "s_2 = (s**2).expand(10,10)\n",
    "F = 1 / (s_2 - s_2.t())\n",
    "F[F.abs() == np.inf] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "med = (u.t() * F).dot(grad_u) + grad_u.mm(u * F.t())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u.mm(med).mm(torch.diag(s)).mm(v.t()).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((torch.eye(10) - 1)**2).byte()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.ones(10,10) - torch.diag(torch.ones(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "F_true = torch.zeros(10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.arange(0,9).view(3,3) * torch.arange(0,9).view(3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Columns 0 to 9 \n",
      " 0.2782  0.5713  0.2020  0.1689  0.8047  1.0493 -0.2938 -0.3923 -0.2603 -0.1073\n",
      " 0.2462 -0.4020  0.0399 -0.2578 -0.2002 -0.0709 -0.1185 -0.1929  0.0584 -0.5145\n",
      "-1.0031  0.0220 -1.6035 -0.0090  0.3267 -0.4548 -0.2745  0.5605  0.2248 -0.5334\n",
      "-0.5387  0.6086  0.3606  0.1042  0.0998  0.8309  0.0363 -0.4954 -0.1176 -0.3042\n",
      "-0.7078  0.9184 -0.1161  0.5348  0.5407  0.7822 -0.0821 -0.3181 -0.1689 -0.5797\n",
      "-0.7995  0.2824 -0.7480 -0.0803 -0.0840 -0.3373 -0.3828  0.4252  0.0594 -0.1748\n",
      "-0.5832 -1.4254 -1.6654 -0.5963 -1.0917 -2.2705 -0.1173  0.9456  1.1198  0.3220\n",
      " 1.4598 -0.8969  1.2927  0.2204 -0.9941 -0.8232  0.3514 -0.2719  0.0407  0.8251\n",
      " 1.3409 -1.7464  0.3600 -0.5079 -1.0135 -2.3076  0.9304 -0.1249  0.5767  1.0616\n",
      "-0.6035  0.8701  0.2136  0.4510  0.7387  1.7386 -0.3752 -0.1760  0.0957 -0.6979\n",
      "\n",
      "Columns 10 to 19 \n",
      "-0.0469  0.8988  0.3601  0.2902 -0.2552  0.4207  0.7199  0.0066  0.2183  0.4164\n",
      " 0.1189 -0.1461  0.0559 -0.5960  0.1857 -0.1259 -0.3736 -0.0919 -0.3495 -0.0076\n",
      "-0.4640 -0.7828 -0.6631  0.6631  2.2340  0.0583 -0.2075  0.8671  0.1713 -0.4104\n",
      "-0.2874  0.4748  0.5003  0.6244  0.2635  0.5818  0.2452  0.3420  0.5180  0.0087\n",
      "-0.2683  0.6140  0.5456  0.9617  0.0203  0.4818  0.5168 -0.2454 -0.3006  0.2201\n",
      "-0.3264  0.0041  0.0412  0.0521  0.9954 -0.1470 -0.0907  0.1844  0.3348  0.4018\n",
      " 0.3073 -1.4545 -1.5829 -0.7214  1.8614 -0.1318 -0.9580  1.2951 -0.6872 -0.0940\n",
      " 1.2157 -0.2938 -0.9470 -2.6509 -2.6060 -0.5627 -0.6887 -0.7075 -0.2632 -0.5673\n",
      " 2.0324 -1.0627 -1.0101 -2.9573 -1.8880 -0.7790 -1.2514 -0.3991 -0.2812 -0.5533\n",
      "-0.5777  0.8249  0.5458  1.7806  0.5196  0.4321  0.7328 -0.5087  0.6225  0.5328\n",
      "[torch.FloatTensor of size 10x20]\n",
      "\n",
      "\n",
      "\n",
      "Columns 0 to 9 \n",
      " 0.2763 -0.1306 -0.3745 -0.1195  0.3658 -0.0298 -0.2032 -0.0294  0.0967  0.1013\n",
      " 0.1394 -0.1683  0.0945 -0.1824 -0.0412  0.2574 -0.1825 -0.2552 -0.0234 -0.6297\n",
      "-0.0528  0.3796 -0.0958  0.3211  0.4373  0.3695 -0.0215 -0.1415 -0.2845 -0.2359\n",
      "-0.3479  0.0187  0.1221 -0.1011 -0.2920 -0.0203  0.1731 -0.2952  0.1161 -0.0471\n",
      "-0.4109  0.1536 -0.3621  0.2782  0.0267 -0.3072  0.1109 -0.0855  0.1171 -0.2252\n",
      "-0.4107  0.3195 -0.2206  0.0098 -0.1072 -0.1680 -0.2651  0.1944 -0.0935 -0.0205\n",
      "-0.0755  0.1786  0.2957  0.1599 -0.1486  0.3408 -0.1658 -0.1578  0.1305  0.0594\n",
      " 0.0490 -0.0806  0.1564  0.2833 -0.3153  0.0232 -0.1993  0.0757  0.1128 -0.0185\n",
      " 0.0205 -0.0543  0.0556 -0.0681  0.2026 -0.0894  0.2944 -0.2781  0.1730 -0.0048\n",
      "-0.1559 -0.1851 -0.0773  0.1042  0.0251  0.2466 -0.0970  0.1242  0.4770 -0.1928\n",
      "\n",
      "Columns 10 to 19 \n",
      " 0.2294  0.1658 -0.2796 -0.2919  0.2344  0.1785  0.2025  0.3784 -0.0216  0.2016\n",
      "-0.0366  0.0486  0.2416 -0.2918  0.2461 -0.0294 -0.2072 -0.1358 -0.2664  0.0750\n",
      "-0.0432  0.0267 -0.0956 -0.0166  0.0093  0.0416  0.1085 -0.0288  0.2650 -0.3991\n",
      " 0.0586 -0.0529  0.0116 -0.0625  0.2748  0.3499 -0.1791  0.5114  0.3106 -0.1918\n",
      " 0.2096 -0.0474 -0.0753  0.0201 -0.0681  0.1737 -0.0306 -0.0626 -0.5709 -0.0450\n",
      "-0.1111  0.2212  0.1738 -0.3169  0.1610 -0.1916 -0.0420 -0.1244  0.3357  0.3729\n",
      "-0.0264  0.4518  0.0070  0.0911 -0.3048  0.3474  0.2522  0.0708 -0.1542  0.3450\n",
      " 0.0586 -0.0903 -0.5626 -0.5209 -0.2356 -0.0723 -0.1648 -0.0885  0.0582 -0.1713\n",
      " 0.5826  0.0959  0.1948 -0.1916 -0.3117  0.0006 -0.0769 -0.3096  0.3369  0.1017\n",
      " 0.1041 -0.0702 -0.3013  0.4424  0.3186  0.0015 -0.0203 -0.2848  0.2484  0.1633\n",
      "[torch.FloatTensor of size 10x20]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import dpp_nets.my_torch as my_torch\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "data = torch.randn(10,20)\n",
    "A = Variable(data, requires_grad=True)\n",
    "B = Variable(data, requires_grad=True)\n",
    "u, s, v = my_torch.linalg.my_full_svd()(A)\n",
    "loss = u.sum() + s.sum()\n",
    "loss.backward()\n",
    "print(A.grad.data)\n",
    "u, s, v = my_torch.linalg.my_svd()(B)\n",
    "loss = u.sum() + s.sum()\n",
    "loss.backward()\n",
    "print(B.grad.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Variable containing:\n",
       " -0.0327 -0.0130 -0.0937  0.6865 -0.2033  0.5315 -0.4002  0.1116  0.0605 -0.1364\n",
       " -0.2437  0.0150 -0.1357 -0.5419  0.2331  0.6642 -0.0341  0.2993 -0.1903 -0.0761\n",
       " -0.3877 -0.3130 -0.3464  0.1240  0.1721 -0.0381  0.2378 -0.4650 -0.1118 -0.5477\n",
       " -0.2246  0.0021  0.3681  0.1461 -0.3745 -0.0828  0.4533  0.4887 -0.3291 -0.3048\n",
       " -0.0623  0.8020 -0.4454 -0.0124 -0.0643 -0.1635  0.0348  0.1354  0.1642 -0.2775\n",
       " -0.3966  0.3520  0.2597 -0.0941 -0.4027  0.1953 -0.0201 -0.5727 -0.1994  0.2721\n",
       " -0.1597 -0.0316 -0.0873 -0.0648  0.0143 -0.3994 -0.6519  0.1198 -0.5993 -0.0584\n",
       " -0.0815  0.3387  0.5113  0.2653  0.7234  0.0205 -0.0064 -0.0651 -0.0918 -0.1020\n",
       "  0.0684  0.0612 -0.4229  0.3218  0.1795  0.0560  0.3843  0.0488 -0.4880  0.5344\n",
       " -0.7354 -0.1228 -0.0601  0.1055  0.1115 -0.2001 -0.0488  0.2749  0.4084  0.3636\n",
       " [torch.FloatTensor of size 10x10], Variable containing:\n",
       "  6.3320\n",
       "  6.0221\n",
       "  4.9367\n",
       "  4.3146\n",
       "  3.7963\n",
       "  3.6935\n",
       "  3.1414\n",
       "  2.7449\n",
       "  2.5599\n",
       "  2.1110\n",
       " [torch.FloatTensor of size 10])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_torch.linalg.toy_svd()(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(10,20),nn.ReLU(),nn.Linear(20,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "\n",
    "x_batch = Variable(torch.randn(batch_size, 10))\n",
    "y_batch = Variable(torch.randn(batch_size, 5))\n",
    "\n",
    "para = nn.DataParallel(model, output_device=-1)\n",
    "para(x_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = torch.FloatTensor([1,1,0,1,0,1,0])\n",
    "n_selected = int(subset.sum())\n",
    "P = torch.diag(subset)\n",
    "P = P[subset.expand_as(P).t().byte()].view(n_selected, -1)\n",
    "P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.randn(7,7)\n",
    "u, s, v = torch.svd(A)\n",
    "vecs = u\n",
    "vals = s**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "subset = torch.FloatTensor([1,1,0,1,0,1,0])\n",
    "n_selected = int(subset.sum())\n",
    "P = torch.diag(subset)\n",
    "P = P[subset.expand_as(P).t().byte()].view(n_selected, -1)\n",
    "\n",
    "submat = P.mm(vecs).mm(vals.diag()).mm(vecs.t()).mm(P.t())\n",
    "submat_inv = submat.inverse()\n",
    "med = P.t().mm(submat_inv).mm(P).mm(vecs)\n",
    "\n",
    "grad_vals = vecs.t().mm(med).diag()\n",
    "grad_vecs = 2 * med.mm(vals.diag())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "\n",
       "Columns 0 to 7 \n",
       " -0.6175   1.3094   6.7233   0.4523   0.9438   0.3486   4.2410  -2.4286\n",
       " -8.0073  -7.0072   0.4030 -11.1050  -9.6503  19.2850   8.6864 -13.2689\n",
       " -0.7447  -1.5359  -1.3604  -3.4976  -1.5778   1.3433   0.0435  -2.9895\n",
       " -6.7001  -0.5004   8.9030  -8.7924  -1.6035  12.1108  14.5486  -6.9492\n",
       "  1.5238   2.3495   1.2337  -2.7375   0.5737  -3.7265   4.3543  -3.5819\n",
       " -6.8326  -0.7263  -1.4977  -7.1387  -5.8600  15.2269  -0.0144  -4.6161\n",
       "-13.4393   4.3327  -1.3268  -2.0925   3.9993  -2.9941   1.3475   3.1083\n",
       "  0.4552   1.9174   1.3903   0.3336   2.1463  -7.2314   2.1193   1.6646\n",
       "  0.5953  -0.5858  -2.5143   1.6440  -1.6141  -0.4569  -3.6980   1.0219\n",
       "  1.2297  -0.2376  -0.4566  -2.6996  -0.7943   4.8011   2.1031  -2.7577\n",
       "\n",
       "Columns 8 to 9 \n",
       "  6.5324   5.1951\n",
       " -1.4245  -2.6386\n",
       " -2.5175  -1.4165\n",
       " -2.0632  -2.1458\n",
       " -3.7329   2.0842\n",
       " -6.0017  -9.1910\n",
       " -6.1647   0.2139\n",
       " -2.3518  -0.6257\n",
       "  2.6611  -0.6329\n",
       " -2.6488  -1.0770\n",
       "[torch.FloatTensor of size 10x10]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import dpp_nets.my_torch as my_torch\n",
    "import numpy as np\n",
    "\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "set_seed(10)\n",
    "data = torch.randn(10,10)\n",
    "A = Variable(data, requires_grad = True)\n",
    "u, s, v = my_torch.linalg.my_full_svd()(A)\n",
    "vecs = u\n",
    "vals = s**2\n",
    "subset = my_torch.DPP2(torch.FloatTensor)(vals, vecs)\n",
    "loss = subset.sum()\n",
    "loss.backward()\n",
    "A.grad.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.1038 -0.1480  0.0342  0.1822 -0.0531 -0.0094  0.1461 -0.2121  0.1420 -0.2397\n",
       "-0.7341 -0.9468 -0.4023  1.1973 -0.5235  0.0223  0.5781 -0.3796  0.9819 -1.7848\n",
       "-0.0929 -0.0279  0.0295  0.1071  0.0028  0.0120  0.0109 -0.0218  0.0910 -0.1812\n",
       "-0.0692  0.4311 -0.0718  0.3018  0.0554 -0.0641 -0.3913 -0.2823  0.0789  0.0202\n",
       " 0.2869  0.6374  0.2405 -0.2309  0.2584  0.0451 -0.3900  0.0689 -0.3971  0.6714\n",
       "-0.6146 -0.7341 -0.5630  0.5980 -0.3218 -0.0641  0.2612 -0.0951  0.3978 -1.0782\n",
       " 0.2648  0.0076  0.2550  0.2591  0.1234  0.1647  0.1935 -0.1829  0.0443 -0.2823\n",
       " 0.2814  0.6007  0.3210 -0.3745  0.3127 -0.0111 -0.2544  0.0917 -0.3784  0.6601\n",
       "-0.2032 -0.5032 -0.0877  0.1740 -0.1213  0.0545  0.4044 -0.0043  0.2504 -0.5240\n",
       "-0.0960  0.1338 -0.1945 -0.0290  0.0151 -0.0945 -0.2541 -0.0289 -0.0825  0.1870\n",
       "[torch.FloatTensor of size 10x10]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import dpp_nets.my_torch as my_torch\n",
    "import numpy as np\n",
    "    \n",
    "set_seed(10)\n",
    "data = torch.randn(10,10)\n",
    "A = Variable(data, requires_grad = True)\n",
    "subset = my_torch.DPP(torch.FloatTensor)(A)\n",
    "loss = subset.sum()\n",
    "loss.backward()\n",
    "A.grad.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.randn(10,10)\n",
    "u, s, v = torch.svd(A)\n",
    "s.expand(u.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.expand(10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import gradcheck\n",
    "\n",
    "input = (Variable(torch.randn(20,20).double(), requires_grad=True),)\n",
    "test = gradcheck(my_torch.linalg.toy_svd(), input, eps=1e-5, atol=1e-5)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
