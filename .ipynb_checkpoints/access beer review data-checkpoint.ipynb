{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "import shutil\n",
    "import time\n",
    "import gzip\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from dpp_nets.utils.io import make_embd, make_tensor_dataset, load_tensor_dataset\n",
    "from dpp_nets.utils.io import data_iterator, load_embd\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import time\n",
    "from dpp_nets.my_torch.utilities import pad_tensor\n",
    "\n",
    "\n",
    "root = '/Users/Max/data/beer_reviews'\n",
    "data_file = 'reviews.aspect3.train.txt.gz'\n",
    "embd_file = 'review+wiki.filtered.200.txt.gz'\n",
    "save_path = os.path.join(root,'pytorch/aspect3_train.pt')\n",
    "data_path = os.path.join(root, data_file)\n",
    "embd_path = os.path.join(root, embd_file)\n",
    "\n",
    "\n",
    "def read_rationales(path):\n",
    "    \"\"\"\n",
    "    This reads the json.annotations file. \n",
    "    Creates a list of dictionaries, which holds the 994 reviews for which\n",
    "    sentence-level annotations are available. \n",
    "    \"\"\"\n",
    "    data = []\n",
    "    fopen = gzip.open if path.endswith(\".gz\") else open\n",
    "    with fopen(path) as fin:\n",
    "        for line in fin:\n",
    "            item = json.loads(line)\n",
    "            data.append(item)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from dpp_nets.my_torch.linalg import custom_decomp\n",
    "from dpp_nets.my_torch.DPP import DPP\n",
    "from dpp_nets.my_torch.DPP import AllInOne\n",
    "from dpp_nets.my_torch.utilities import compute_baseline\n",
    "\n",
    "class DPP_Classifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, dtype):\n",
    "        \n",
    "        super(DPP_Classifier, self).__init__()\n",
    "        # Float vs Double\n",
    "        self.dtype = dtype\n",
    "\n",
    "        # Network parameters\n",
    "        self.kernel_in = kernel_in = 400\n",
    "        self.kernel_h = kernel_h = 1000\n",
    "        self.kernel_out = kernel_out = 400\n",
    "\n",
    "        self.pred_in = pred_in = 200 # kernel_in / 2\n",
    "        self.pred_h = pred_h = 500\n",
    "        self.pred_h2 = pred_h2 = 200\n",
    "        self.pred_out = pred_out = 3\n",
    "        \n",
    "        # 2-Hidden-Layer Networks \n",
    "        self.kernel_net = torch.nn.Sequential(nn.Linear(kernel_in, kernel_h), nn.ELU(),\n",
    "                                              nn.Linear(kernel_h, kernel_h), nn.ELU(), \n",
    "                                              nn.Linear(kernel_h, kernel_out))\n",
    "        # 3-Hidden-Layer-Networks\n",
    "        self.pred_net = torch.nn.Sequential(nn.Linear(pred_in, pred_h), nn.ReLU(),\n",
    "                                             nn.Linear(pred_h, pred_h), nn.ReLU(),\n",
    "                                             nn.Linear(pred_h, pred_h2), nn.ReLU(),\n",
    "                                             nn.Linear(pred_h2, pred_out), nn.Sigmoid())\n",
    "        \n",
    "        self.kernel_net.type(self.dtype)\n",
    "        self.pred_net.type(self.dtype)\n",
    "        \n",
    "        # Sampling Parameter\n",
    "        self.alpha_iter = 5\n",
    "\n",
    "        # Convenience\n",
    "        self.kernels = []\n",
    "        self.subsets = None\n",
    "        self.picks = None\n",
    "        self.preds = None\n",
    "        \n",
    "        self.saved_subsets = None\n",
    "        self.saved_losses = None # not really necesary\n",
    "        self.saved_baselines = None # not really necessary\n",
    "        \n",
    "    def forward(self, reviews):\n",
    "        \"\"\"\n",
    "        reviews: batch_size x max_set_size x embd_dim = 200\n",
    "        Output: batch_size x pred_out (the prediction)\n",
    "        Challenges: Need to resize tensor appropriately and \n",
    "        measure length etc. \n",
    "        \"\"\"\n",
    "        batch_size, max_set_size, embd_dim = reviews.size()\n",
    "        alpha_iter = self.alpha_iter\n",
    "        self.saved_subsets = actions = [[] for i in range(batch_size)]\n",
    "        picks = [[] for i in range(batch_size)]\n",
    "        \n",
    "        # Create context\n",
    "        lengths = reviews.sum(2).abs().sign().sum(1)\n",
    "        context = (reviews.sum(1) / lengths.expand_as(reviews.sum(1))).expand_as(reviews)\n",
    "        mask = reviews.sum(2).abs().sign().expand_as(reviews).byte()\n",
    "\n",
    "        # Mask out zero words\n",
    "        reviews = reviews.masked_select(mask).view(-1, embd_dim)\n",
    "        context = context.masked_select(mask).view(-1, embd_dim)\n",
    "\n",
    "        # Compute batched_kernel\n",
    "        kernel_input = torch.cat([reviews, context], dim=1)\n",
    "        kernel_output = self.kernel_net(kernel_input)\n",
    "        \n",
    "        # Extract the kernel for each review from batched_kernel\n",
    "        s = list(lengths.squeeze().cumsum(0).long().data - lengths.squeeze().long().data)\n",
    "        e = list(lengths.squeeze().cumsum(0).long().data)\n",
    "\n",
    "        for i, (s, e) in enumerate(zip(s, e)):\n",
    "            review = reviews[s:e] # original review, without zero words\n",
    "            kernel = kernel_output[s:e] # corresponding kernel \n",
    "            self.kernels.append(kernel.data)\n",
    "            #vals, vecs = custom_decomp()(kernel)\n",
    "            for j in range(alpha_iter):\n",
    "                subset = AllInOne()(kernel)\n",
    "                #subset = DPP()(vals, vecs)\n",
    "                actions[i].append(subset)\n",
    "                pick = subset.diag().mm(review).sum(0)\n",
    "                picks[i].append(pick)\n",
    "\n",
    "        # Predictions\n",
    "        picks = torch.stack([torch.stack(pick) for pick in picks]).view(-1, embd_dim)\n",
    "        preds = self.pred_net(picks).view(batch_size, alpha_iter, -1)\n",
    "        \n",
    "        return preds\n",
    "\n",
    "def register_rewards(preds, targets, criterion, net):\n",
    "    \n",
    "    #targets = targets.unsqueeze(1).unsqueeze(1).expand_as(preds)\n",
    "    targets = targets.unsqueeze(1).expand_as(preds)\n",
    "    loss = criterion(preds, targets)\n",
    "    \n",
    "    actions = net.saved_subsets\n",
    "    \n",
    "    losses = ((preds - targets)**2).mean(2)\n",
    "    losses = [[i.data[0] for i in row] for row in losses]\n",
    "    net.saved_losses = losses # not really necessary\n",
    "    baselines = [compute_baseline(i) for i in losses]\n",
    "    net.saved_baselines = baselines # not really necessary\n",
    "    \n",
    "    for actions, rewards in zip(actions, baselines):\n",
    "        for action, reward in zip(actions, rewards):\n",
    "            action.reinforce(reward)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Useful Support\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        \n",
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    \"\"\"\n",
    "    This is good!\n",
    "    \"\"\"\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'model_best.pth.tar')\n",
    "        \n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    lr = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "    lr = lr * (0.1 ** (epoch // 5))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(train_loader, embd, model, criterion, optimizer, epoch, dtype):\n",
    "    \n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    subset_size = AverageMeter()\n",
    "    \n",
    "    target_dim = 3\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (review, target) in enumerate(train_loader):\n",
    "        \n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        \n",
    "        targets = Variable(target[:,:target_dim].type(dtype))\n",
    "        reviews = embd(Variable(review)).type(dtype)\n",
    "\n",
    "        # compute output\n",
    "        model.alpha_iter = 2\n",
    "        pred = model(reviews)\n",
    "        loss = register_rewards(pred, targets, criterion, model)\n",
    "\n",
    "        ##measure accuracy and record loss ????????????????????????\n",
    "        # prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n",
    "        losses.update(loss.data[0], reviews.size(0))\n",
    "        for l in model.saved_subsets:\n",
    "            for s in l:\n",
    "                subset_size.update(s.data.sum())\n",
    "        # top1.update(prec1[0], input.size(0))\n",
    "        # top5.update(prec5[0], input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        #if i % args.print_freq == 0:\n",
    "        if i % print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'SSize {subset_size.val:.2f} ({subset_size.avg: .2f})'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'.format(\n",
    "                   epoch, i, len(train_loader), batch_time=batch_time,\n",
    "                   data_time=data_time, subset_size = subset_size, loss=losses))\n",
    "\n",
    "def validate(val_loader, model, criterion):\n",
    "    \n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    t_prec = AverageMeter()\n",
    "    t_recall = AverageMeter()\n",
    "    t_tp = AverageMeter()\n",
    "    t_fp = AverageMeter()\n",
    "    t_fn = AverageMeter()\n",
    "    \n",
    "    target_dim = 3\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    # model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (review, target) in enumerate(val_loader):\n",
    "        \n",
    "        target = target.sum(1).sign().type(dtype).squeeze().byte()\n",
    "        # targets = target[:,:target_dim,:].type(dtype)\n",
    "        reviews = embd(Variable(review, volatile=True)).type(dtype)\n",
    "\n",
    "        # compute output\n",
    "        model.alpha_iter = 1\n",
    "        preds = model(reviews)\n",
    "\n",
    "        subset = model.saved_subsets[0][0]\n",
    "        subset = pad_tensor(subset.data,0,0,412).byte()\n",
    "        # target = targets\n",
    "\n",
    "        # targets = target[:,:target_dim,:].type(dtype)\n",
    "        reviews = embd(Variable(review, volatile=True)).type(dtype)\n",
    "\n",
    "        # compute output\n",
    "        model.alpha_iter = 1\n",
    "        preds = model(reviews)\n",
    "        \n",
    "        subset = model.saved_subsets[0][0]\n",
    "        subset = pad_tensor(subset.data,0,0,412).byte()\n",
    "        # target = target[:,:target_dim,:].squeeze()\n",
    "\n",
    "        retriev = subset.sum()\n",
    "        relev = target.sum()\n",
    "\n",
    "        tp = target.masked_select(subset).sum()\n",
    "        fp = (1 - target.masked_select(subset)).sum()\n",
    "        fn = (1 - subset.masked_select(target)).sum()\n",
    "        t_tp.update(tp)\n",
    "        t_fp.update(fp)\n",
    "        t_fn.update(fn)\n",
    "\n",
    "        if retriev: \n",
    "            prec = tp / retriev\n",
    "            t_prec.update(prec)\n",
    "\n",
    "        if relev: \n",
    "            recall = tp / relev\n",
    "            t_recall.update(recall)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        #prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n",
    "        #losses.update(loss.data[0], input.size(0))\n",
    "        #top1.update(prec1[0], input.size(0))\n",
    "        #top5.update(prec5[0], input.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Precision {t_prec.val:.4f} ({t_prec.avg:.4f})\\t'\n",
    "                  'Recall {t_recall.val:.4f} ({t_recall.avg:.4f})\\t'.format(\n",
    "                   i, len(val_loader), batch_time=batch_time, t_prec=t_prec, t_recall=t_recall))\n",
    "            \n",
    "    return t_prec.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### MAIN PROGRAMME\n",
    "\n",
    "\n",
    "global best_prec1\n",
    "best_prec1 = 0\n",
    "\n",
    "# set parameters\n",
    "lr = 1e-1\n",
    "momentum = 0.9\n",
    "weight_decay = 0.\n",
    "start_epoch = 0\n",
    "epochs = 1\n",
    "batch_size = 20\n",
    "print_freq = 10\n",
    "\n",
    "data = '/Users/Max/data/beer_reviews/pytorch'\n",
    "dtype = torch.DoubleTensor\n",
    "\n",
    "# create model\n",
    "embd = load_embd('/Users/Max/data/beer_reviews/pytorch/embeddings.pt')\n",
    "model = DPP_Classifier(torch.DoubleTensor)\n",
    "\n",
    "# define loss function (criterion) and optimizer\n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr,\n",
    "                            momentum=momentum,\n",
    "                            weight_decay=weight_decay)\n",
    "\n",
    "# Data loading code\n",
    "trainpath = os.path.join(data, 'aspect1_train.pt')\n",
    "valpath = os.path.join(data, 'aspect1_heldout.pt')\n",
    "ratpath = os.path.join(data, 'annotated.pt')\n",
    "\n",
    "train_set = torch.load(trainpath)\n",
    "val_set = torch.load(valpath)\n",
    "rat_set = torch.load(ratpath)\n",
    "\n",
    "rat_train_set = torch.load(os.path.join(data, 'annotated_common.pt'))\n",
    "#train_loader = DataLoader(train_set, batch_size, shuffle=True)\n",
    "#val_loader = DataLoader(val_set)\n",
    "rat_train_loader = DataLoader(rat_train_set, batch_size, shuffle=True)\n",
    "rat_loader = DataLoader(rat_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "\n",
    "    # train for one epoch\n",
    "    train(rat_train_loader, embd, model, criterion, optimizer, epoch, dtype)\n",
    "\n",
    "    # evaluate on validation set\n",
    "    prec1 = validate(rat_loader, model, criterion)\n",
    "\n",
    "# remember best prec@1 and save checkpoint\n",
    "is_best = prec1 > best_prec1\n",
    "best_prec1 = max(prec1, best_prec1)\n",
    "save_checkpoint({\n",
    "    'epoch': epoch + 1,\n",
    "    'state_dict': model.state_dict(),\n",
    "    'best_prec1': best_prec1,\n",
    "    'optimizer' : optimizer.state_dict(),\n",
    "}, is_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "#word_to_ix = make_embd(embd_path, only_index_dict=True)\n",
    "#ix_to_word = {ix: word for word, ix in word_to_ix.items()}\n",
    "\n",
    "rat_set, ix_to_word\n",
    "def sample_words(rat_set, model, ix_to_word):\n",
    "    \n",
    "    # Sample a review\n",
    "    ix = random.randint(0,len(rat_set))\n",
    "\n",
    "    # Make a prediction\n",
    "    x = rat_set.data_tensor[ix].unsqueeze(0)\n",
    "    review = embd(Variable(x, volatile=True)).type(dtype)\n",
    "    model.alpha_iter = 1\n",
    "    model(review)\n",
    "\n",
    "    # What words were selected\n",
    "    subset = model.saved_subsets[0][0]\n",
    "    subset = pad_tensor(subset.data,0,0,412).byte()\n",
    "\n",
    "    # Convert to words\n",
    "    all_words = [ix_to_word[ix -1] for ix in x.squeeze() if ix > 0]\n",
    "    filtered_words = [ix_to_word[ix -1] for ix in x.masked_select(subset)]\n",
    "    print(\" \".join(all_words) )\n",
    "    print(\"DPP Selection: \", filtered_words)\n",
    "\n",
    "\n",
    "    ix = random.randint(0,len(rat_set))\n",
    "    rat_set.data_tensor[ix].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "   \n",
    "def sample_prediction(rat_set, model):\n",
    "    # Sample a review\n",
    "    ix = random.randint(0,len(rat_train_set))\n",
    "\n",
    "    # Make a prediction\n",
    "    x = rat_train_set.data_tensor[ix].unsqueeze(0)\n",
    "    target = rat_train_set.target_tensor[ix][:3]\n",
    "    review = embd(Variable(x, volatile=True)).type(dtype)\n",
    "    model.alpha_iter = 1\n",
    "    pred = model(review).data.squeeze()\n",
    "    print(pred, target)\n",
    "    return pred, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred, target = sample_prediction(rat_set, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion(Variable(pred), Variable(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(model.pred_net.state_dict(), 'pred_dict25.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "e = 0\n",
    "for i in range(100):\n",
    "    v = torch.normal(torch.FloatTensor([1,2,3,4,5]))\n",
    "    e += v\n",
    "e / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "non_lin = torch.sin\n",
    "torch.sin(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "non_lin(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "non_lin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "set_size = 3\n",
    "embd_dim = 4\n",
    "words = torch.randn(batch_size, set_size, embd_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v = torch.normal(torch.FloatTensor([1,2,3,4,5])torch.cos(torch.sin(words).mean(1)).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v = torch.normal(torch.FloatTensor([1,2,3,4,5]))\n",
    "torch.log(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "batch_size = 100\n",
    "n_clusters = 10\n",
    "set_size = 40\n",
    "embd_dim = pred_in = 50\n",
    "dtype = dtype = torch.DoubleTensor\n",
    "np.random.seed(0)\n",
    "means = dtype(np.random.randint(-50,50,[n_clusters, int(pred_in)]).astype(\"float\"))\n",
    "\n",
    "def generate(batch_size):\n",
    "    \"\"\"sdf\"\n",
    "    Arguments:\n",
    "    means: Probs best to make this an attribute of the class, \n",
    "    so that repeated training works with the same data distribution.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Generate index\n",
    "    index = torch.cat([torch.arange(0, float(n_clusters)).expand(batch_size, n_clusters).long(), \n",
    "                      torch.multinomial(torch.ones(batch_size, n_clusters), set_size - n_clusters, replacement=True)]\n",
    "                     ,dim=1)\n",
    "    index = index.t()[torch.randperm(set_size)].t().contiguous()\n",
    "\n",
    "    # Generate words, context, target\n",
    "    words = dtype(torch.normal(means.index_select(0,index.view(index.numel()))).view(batch_size, set_size, embd_dim))\n",
    "    context = dtype(words.sum(1).expand_as(words))\n",
    "\n",
    "    target = torch.sin(torch.pow(words.abs(),2).mean(1)).squeeze()\n",
    "\n",
    "    return words, context, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words, context, target = generate(5)\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(torch.std(target, dim=0) / torch.mean(target, dim=0)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v1 = torch.randn(2,2)\n",
    "v2 = torch.randn(2,2)\n",
    "v3 = torch.randn(2,2)\n",
    "v4 = torch.randn(2,2)\n",
    "v5 = torch.randn(2,2)\n",
    "v6 = torch.randn(2,2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dpp_nets.my_torch.simulator import SimKDPPDeepSet\n",
    "import torch\n",
    "network_params = {'set_size': 40, 'n_clusters': 10}\n",
    "dtype = torch.DoubleTensor\n",
    "sim = SimKDPPDeepSet(network_params, dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mod = torch.nn.Sequential(nn.Linear(10,20), nn.ReLU(), nn.Linear(20,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for mod in mod.modules():\n",
    "    print(mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A = Variable(torch.randn(10,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mod(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 3\n",
    "set_size = 4\n",
    "embd_dim = 5\n",
    "words = Variable(torch.randn(batch_size, set_size, embd_dim))\n",
    "print(words)\n",
    "subset = Variable(torch.ByteTensor([1,0,0,1]),requires_grad=True)\n",
    "words[1].masked_select(Variable(subset.data.expand_as(words[1].t())).t()).view(-1,embd_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dpp_nets.layers.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 10])\n",
      "Variable containing:\n",
      "\n",
      "Columns 0 to 9 \n",
      "1.00000e-02 *\n",
      " -0.5300 -0.7917 -0.6452 -1.4612 -0.5058 -1.2006 -2.1787 -2.1236 -0.2686 -1.8842\n",
      "  0.0000  0.0000 -0.1811  0.0000  0.0000  0.0000 -0.1205 -0.0070  0.0000 -0.0515\n",
      " -0.0227 -0.0506  0.0000 -0.2353 -0.1385 -0.0243  0.0491  0.0257  0.0000 -0.1554\n",
      "  0.0347  0.0884  0.0000  0.3013  0.1799  0.5259  0.7815  0.5997  0.0063  0.5795\n",
      "  0.0925  0.1635  0.0052  0.0660  0.0037  0.0513  0.0426  0.2012  0.0783  0.0565\n",
      " -0.5808 -1.4151 -0.0097 -1.9906 -0.6326 -1.1522 -1.3508 -2.0787 -0.6067 -1.2313\n",
      "  0.5403  0.9329  0.5399  1.7154  0.4312  1.2229  2.0423  2.1862  0.1797  1.9186\n",
      " -0.0347 -0.0023 -0.0535 -0.0018  0.0255 -0.0330 -0.0577  0.0477  0.0363  0.0564\n",
      "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "  0.0716 -0.0007 -0.0414 -0.0732  0.0192  0.0011 -0.0176 -0.0481  0.0712 -0.2228\n",
      " -0.2158 -0.4081 -0.0718 -0.2687 -0.1963 -0.1647 -0.5586 -1.0627 -0.2558 -0.7300\n",
      " -0.4459 -0.5748 -0.0900 -0.6765 -0.2486 -0.1862 -0.5181 -1.0505 -0.4401 -0.5154\n",
      "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      " -0.1105 -0.0597 -0.2978 -0.1661 -0.1241 -0.4090 -0.7939 -0.7139 -0.1337 -1.0294\n",
      " -0.0383 -0.2825  0.1077 -0.2012 -0.1026 -0.1112  0.0942 -0.1156 -0.0367 -0.2427\n",
      "  0.4755  0.2978  0.4733  0.9197  0.2138  0.3510  0.8505  1.1786  0.1710  0.5548\n",
      "  0.0033 -0.0017  0.2455  0.0250  0.0784  0.8060  1.0754  1.2769  0.0029  1.2809\n",
      " -1.0585 -0.7105 -0.5265 -1.7514 -0.8186 -0.2568 -1.2413 -2.0500 -0.4174 -1.1151\n",
      "  0.0738 -0.0346  0.0942 -0.3139 -0.0665 -0.0144  0.0201 -0.1034  0.0718 -0.3042\n",
      "  0.7541  0.4976  0.6706  0.2516  0.1392  0.7331  1.1912  1.3219  0.3366  0.7088\n",
      "\n",
      "Columns 10 to 19 \n",
      "1.00000e-02 *\n",
      " -1.1414 -0.8958 -2.0838 -1.1952 -0.9582 -1.5042 -1.3870 -1.4778 -1.6667 -0.9836\n",
      " -0.0562  0.0000 -0.1469 -0.1335  0.0000  0.0000 -0.0150 -0.2211 -0.2473  0.0000\n",
      "  0.0043 -0.1714  0.0234 -0.0106 -0.2040 -0.3087 -0.0119  0.0008 -0.0033  0.0000\n",
      "  0.3678  0.2509  0.5340  0.3361  0.0656  0.3146  0.2078  0.2972  0.2312  0.2821\n",
      "  0.1362  0.0461  0.1468  0.0269  0.1029  0.0464  0.2683  0.1087  0.0624  0.1223\n",
      " -0.9918 -0.9414 -1.9826 -0.5643 -1.4345 -1.6836 -2.4194 -1.0016 -0.9054 -1.1984\n",
      "  1.0962  0.9961  2.0176  1.0081  1.0378  1.7302  1.4943  1.2395  1.4663  0.8612\n",
      "  0.0350 -0.0303  0.0353 -0.0135 -0.0432 -0.0561 -0.0026  0.0745 -0.0029  0.0840\n",
      "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      " -0.0944 -0.0130 -0.1087 -0.0381  0.1035 -0.0105  0.0398 -0.1470 -0.0613  0.0135\n",
      " -0.6144 -0.1774 -0.8162 -0.3075 -0.2623 -0.2309 -0.8238 -0.8254 -0.5836 -0.6102\n",
      " -0.1520 -0.3156 -0.5746 -0.1603 -0.8506 -0.5866 -0.9705 -0.4276 -0.4533 -0.7136\n",
      "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      " -0.1778 -0.0798 -1.1046 -0.5779 -0.1699 -0.0192 -0.3771 -0.9936 -0.9899 -0.5368\n",
      "  0.0668 -0.1565 -0.1026  0.0426 -0.3219 -0.2437 -0.4553 -0.1372  0.0589 -0.1347\n",
      "  0.5642  0.5049  0.6711  0.4623  0.6438  1.0040  0.2391  0.4386  0.8524  0.5730\n",
      "  0.3024  0.1775  1.9746  0.5922 -0.0136  0.1979  0.2352  1.5548  1.3386  0.6301\n",
      " -1.3901 -0.9000 -0.5198 -0.9222 -1.6470 -2.0547 -0.6960 -0.3470 -1.4108 -1.2673\n",
      " -0.0856 -0.0783 -0.0142  0.0796  0.0334 -0.1671 -0.0348 -0.0062  0.1076 -0.0825\n",
      "  0.7971  0.5086  1.4121  0.7049  0.7536  0.5009  0.9352  1.1469  1.5123  0.6694\n",
      "[torch.FloatTensor of size 20x20]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "embd_dim, hidden_dim, enc_dim, target_dim = 10, 20, 10, 2\n",
    "baseline = DeepSetBaseline(embd_dim, hidden_dim, enc_dim, target_dim)\n",
    "batch_size = 4\n",
    "max_set_size = 7\n",
    "x = Variable(torch.randn(batch_size, max_set_size, embd_dim))\n",
    "pred = baseline(x)\n",
    "targets = Variable(torch.randn(batch_size, target_dim))\n",
    "criterion = nn.MSELoss()\n",
    "loss = criterion(pred, targets)\n",
    "loss.backward()\n",
    "print(baseline.enc_layer2.weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
