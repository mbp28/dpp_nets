{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "from dpp_nets.utils.io import make_embd, make_tensor_dataset\n",
    "from dpp_nets.layers.layers import DeepSetBaseline\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Baseline (Deep Sets) Trainer')\n",
    "\n",
    "parser.add_argument('-a', '--aspect', type=str, choices=['aspect1', 'aspect2', 'aspect3', 'all'],\n",
    "                    help='what is the target?', required=True)\n",
    "parser.add_argument('--remote', type=int,\n",
    "                    help='training locally or on cluster?', required=True)\n",
    "\n",
    "parser.add_argument('--data_path_local', type=str, default='/Users/Max/data/beer_reviews',\n",
    "                    help='where is the data folder locally?')\n",
    "parser.add_argument('--data_path_remote', type=str, default='/cluster/home/paulusm/data/beer_reviews',\n",
    "                    help='where is the data folder?')\n",
    "\n",
    "parser.add_argument('--ckp_path_local', type=str, default='/Users/Max/checkpoints/beer_reviews',\n",
    "                    help='where is the data folder locally?')\n",
    "\n",
    "parser.add_argument('--ckp_path_remote', type=str, default='/cluster/home/paulusm/checkpoints/beer_reviews',\n",
    "                    help='where is the data folder?')\n",
    "\n",
    "parser.add_argument('-b', '--batch-size', default=50, type=int,\n",
    "                    metavar='N', help='mini-batch size (default: 50)')\n",
    "parser.add_argument('--epochs', default=100, type=int, metavar='N',\n",
    "                    help='number of total epochs to run')\n",
    "#parser.add_argument('--lr-k', '--learning-rate-k', default=0.1, type=float,\n",
    "#                    metavar='LRk', help='initial learning rate for kernel net')\n",
    "#parser.add_argument('--lr-p', '--learning-rate-p', default=0.1, type=float,\n",
    "#                    metavar='LRp', help='initial learning rate for pred net')\n",
    "parser.add_argument('--lr', '--learning-rate', default=1e-4, type=float,\n",
    "                    metavar='LR', help='initial learning rate for baseline')\n",
    "#parser.add_argument('--reg', type=float, required=True,\n",
    "#                    metavar='reg', help='regularization constant')\n",
    "#parser.add_argument('--reg-mean', type=float, required=True,\n",
    "#                    metavar='reg_mean', help='regularization_mean')\n",
    "\n",
    "def main():\n",
    "\n",
    "    global args, lowest_loss\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    lowest_loss = 100 # arbitrary high number as upper bound for loss\n",
    "\n",
    "    ### Load data\n",
    "    if args.remote:\n",
    "        # print('training remotely')\n",
    "        train_path = os.path.join(args.data_path_remote, str.join(\".\",['reviews', args.aspect, 'train.txt.gz']))\n",
    "        val_path   = os.path.join(args.data_path_remote, str.join(\".\",['reviews', args.aspect, 'heldout.txt.gz']))\n",
    "        embd_path = os.path.join(args.data_path_remote, 'review+wiki.filtered.200.txt.gz')\n",
    "\n",
    "    else:\n",
    "        # print('training locally')\n",
    "        train_path = os.path.join(args.data_path_local, str.join(\".\",['reviews', args.aspect, 'train.txt.gz']))\n",
    "        val_path   = os.path.join(args.data_path_local, str.join(\".\",['reviews', args.aspect, 'heldout.txt.gz']))\n",
    "        embd_path = os.path.join(args.data_path_local, 'review+wiki.filtered.200.txt.gz')\n",
    "\n",
    "    embd, word_to_ix = make_embd(embd_path)\n",
    "    train_set = make_tensor_dataset(train_path, word_to_ix)\n",
    "    val_set = make_tensor_dataset(val_path, word_to_ix)\n",
    "    print(\"loaded data\")\n",
    "\n",
    "    torch.manual_seed(0)\n",
    "    train_loader = DataLoader(train_set, args.batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_set, args.batch_size)\n",
    "    print(\"loader defined\")\n",
    "\n",
    "    ### Build model\n",
    "    # Network parameters\n",
    "    embd_dim = embd.weight.size(1)\n",
    "    hidden_dim = 500\n",
    "    enc_dim = 200\n",
    "    if args.aspect == 'all':\n",
    "        target_dim = 3\n",
    "    else: \n",
    "        target_dim = 1\n",
    "\n",
    "    # Model\n",
    "    torch.manual_seed(0)\n",
    "    trainer = MarginalTrainer(embd, hidden_dim, kernel_dim, enc_dim, target_dim)\n",
    "    trainer.activation = nn.Sigmoid()\n",
    "    trainer.reg = args.reg\n",
    "    trainer.reg_mean = args.reg_mean\n",
    "    print(\"created trainer\")\n",
    "\n",
    "    # Set-up Training\n",
    "    params = [{'params': trainer.kernel_net.parameters(), 'lr': args.lr_k},\n",
    "              {'params': trainer.pred_net.parameters(),   'lr': args.lr_p}]\n",
    "    optimizer = torch.optim.Adam(params)\n",
    "    print('set-up optimizer')\n",
    "\n",
    "    ### Loop\n",
    "    torch.manual_seed(0)\n",
    "    print(\"started loop\")\n",
    "    for epoch in range(args.epochs):\n",
    "\n",
    "        adjust_learning_rate(optimizer, epoch)\n",
    "\n",
    "        train(train_loader, trainer, optimizer)        \n",
    "        loss, pred_loss, reg_loss = validate(val_loader, trainer)\n",
    "        \n",
    "        log(epoch, loss, pred_loss, reg_loss)\n",
    "        print(\"logged\")\n",
    "\n",
    "        is_best = pred_loss < lowest_loss\n",
    "        lowest_loss = min(pred_loss, lowest_loss)    \n",
    "        save = {'epoch:': epoch + 1, \n",
    "                'model': 'Marginal Trainer',\n",
    "                'state_dict': trainer.state_dict(),\n",
    "                'lowest_loss': lowest_loss,\n",
    "                'optimizer': optimizer.state_dict()} \n",
    "\n",
    "        save_checkpoint(save, is_best)\n",
    "        print(\"saved a checkpoint\")\n",
    "\n",
    "    print('*'*20, 'SUCCESS','*'*20)\n",
    "\n",
    "\n",
    "def train(loader, model, criterion, optimizer):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for t, (review, target) in enumerate(loader):\n",
    "        review = Variable(review)\n",
    "\n",
    "        if args.aspect == 'all':\n",
    "            target = Variable(target[:,:3])\n",
    "        else:\n",
    "            target = Variable(target[:,int(args.aspect[-1])])\n",
    "\n",
    "        pred = model(review)\n",
    "        loss = criterion(pred, target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(\"trained one batch\")\n",
    "\n",
    "def validate(loader, model, criterion):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for i, (review, target) in enumerate(loader, 1):\n",
    "\n",
    "        review = Variable(review, volatile=True)\n",
    "\n",
    "        if args.aspect == 'all':\n",
    "            target = Variable(target[:,:3], volatile=True)\n",
    "        else:\n",
    "            target = Variable(target[:,int(args.aspect[-1])], volatile=True)\n",
    "\n",
    "        pred = model(review)\n",
    "        loss = criterion(pred, target)\n",
    "        \n",
    "        delta = loss.data[0] - total_loss\n",
    "        total_loss += (delta / i)\n",
    "\n",
    "        print(\"validated one batch\")\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "def log(epoch, loss):\n",
    "    string = str.join(\" | \", ['Epoch: %d' % (epoch), 'Validation Loss: %.5f' % (loss)])\n",
    "\n",
    "    if args.remote:\n",
    "        destination = os.path.join(args.ckp_path_remote, args.aspect + str(args.lr) + 'baseline_log.txt')\n",
    "    else:\n",
    "        destination = os.path.join(args.ckp_path_local, args.aspect + str(args.lr) + 'baseline_log.txt')\n",
    "\n",
    "    with open(destination, 'a') as log:\n",
    "        log.write(string + '\\n')\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR multiplied by factor 0.1 for every 20 epochs\"\"\"\n",
    "    lr = args.lr * (0.1 ** (epoch // 25))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "def save_checkpoint(state, is_best, filename='baseline_checkpoint.pth.tar'):\n",
    "    \"\"\"\n",
    "    State is a dictionary that cotains valuable information to be saved.\n",
    "    \"\"\"\n",
    "    if args.remote:\n",
    "        destination = os.path.join(args.ckp_path_remote, args.aspect + str(args.lr) + filename)\n",
    "    else:\n",
    "        destination = os.path.join(args.ckp_path_local, args.aspect + str(args.lr) + filename)\n",
    "    \n",
    "    torch.save(state, destination)\n",
    "    if is_best:\n",
    "        if args.remote:\n",
    "            best_destination = os.path.join(args.ckp_path_remote, args.aspect + str(args.lr) + 'baseline_model_best.pth.tar')\n",
    "        else:\n",
    "            best_destination = os.path.join(args.ckp_path_local, args.aspect + str(args.lr) + 'baseline_model_best.pth.tar')\n",
    "        \n",
    "        shutil.copyfile(destination, best_destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "args = parser.parse_args(\"-a all --remote 0\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_path   = os.path.join(args.data_path_local, str.join(\".\",['reviews', args.aspect, 'heldout.txt.gz']))\n",
    "embd_path = os.path.join(args.data_path_local, 'review+wiki.filtered.200.txt.gz')\n",
    "embd, word_to_ix = make_embd(embd_path)\n",
    "val_set = make_tensor_dataset(val_path, word_to_ix)\n",
    "val_loader = DataLoader(val_set, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "once\n",
      "once\n",
      "once\n",
      "once\n",
      "once\n",
      "once\n",
      "once\n",
      "once\n",
      "once\n",
      "once\n",
      "once\n",
      "once\n",
      "once\n",
      "once\n",
      "once\n",
      "once\n",
      "once\n",
      "once\n",
      "once\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "from dpp_nets.layers.layers import MarginalTrainer, ReinforceSampler\n",
    "\n",
    "load_path = '/Users/Max/checkpoints/full_backups/no_batch_normalization_elu/'\n",
    "model_name = 'allreg0.1reg_mean10.0marginal_checkpoint.pth.tar'\n",
    "a_dict = torch.load(os.path.join(load_path, model_name))\n",
    "reinstated_model = MarginalTrainer(embd, 200,500,200,3)\n",
    "#reinstated_model.load_state_dict(a_dict['state_dict'])\n",
    "for param_name, params in a_dict['state_dict'].items():\n",
    "    if '.0.' in param_name or '.2.' in param_name or '.4.' in param_name:\n",
    "        pass\n",
    "    else:\n",
    "        exec(str('reinstated_model.'+str(param_name)+' = nn.Parameter(params)'))\n",
    "        print('once')\n",
    "reinstated_model.activation = nn.Sigmoid()\n",
    "reinstated_model.reg = 0.1\n",
    "reinstated_model.reg_mean = 10\n",
    "ix_to_word = {}\n",
    "for k, v in word_to_ix.items():\n",
    "    ix_to_word[v + 1] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "ix = random.randint(0,10000)\n",
    "review, target = val_set[ix]\n",
    "sampler = ReinforceSampler(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "presentation : the brooklyn brewery style logo label with some waffling on about how it is their verison of an ipa . appearance : golden-amber , with a thin white head . smell : loads of spicy mint hop aromas , with a casual sweetness on the backend . taste : their hop use is applied more for flavour , rather than bittering . a very intruiging hop flavour to be found , and an over puckering reaction that runs dry at the end . very leafy , literally . notes : a bit overkill for this style . makes you wonder what they think an ipa is . seems like an over use of lower alpha hops .\n",
      "__________________________________________________\n",
      "is\n",
      "with\n",
      "a\n",
      "thin\n",
      "head\n",
      "spicy\n",
      "casual\n",
      "ipa\n",
      "seems\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "words = reinstated_model.embd(Variable(review.unsqueeze(0), volatile=True))\n",
    "kernel, words = reinstated_model.kernel_net(words)\n",
    "sampler.s_ix = reinstated_model.kernel_net.s_ix\n",
    "sampler.e_ix = reinstated_model.kernel_net.e_ix\n",
    "sampler(kernel, words) \n",
    "\n",
    "my_sample = sampler.saved_subsets[0][0]\n",
    "cut_review = review[:my_sample.size(0)]\n",
    "\n",
    "my_list = list(cut_review.masked_select(my_sample.data.byte()))\n",
    "original_review = [ix_to_word[i] for i in list(cut_review)]\n",
    "print(\" \".join(original_review))\n",
    "print(50*'_')\n",
    "for i in my_list:\n",
    "    print(ix_to_word[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampler.s_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0.9157  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      " 0.0000  0.6111  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      " 0.0000  0.0000  0.8441  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      " 0.0000  0.0000  0.0000  0.9733  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      " 0.0000  0.0000  0.0000  0.0000  0.9807  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      " 0.0000  0.0000  0.0000  0.0000  0.0000  0.9815  0.0000  0.0000  0.0000  0.0000\n",
      " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.9847  0.0000  0.0000  0.0000\n",
      " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.9817  0.0000  0.0000\n",
      " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.9707  0.0000\n",
      " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.9798\n",
      "[torch.FloatTensor of size 10x10]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from dpp_nets.my_torch.linalg import custom_decomp, custom_inverse \n",
    "\n",
    "# compute marginals\n",
    "for i, (s, e) in enumerate(zip(sampler.s_ix, sampler.e_ix)):\n",
    "    # unpack kernel and words\n",
    "    V = kernel[s:e]\n",
    "    word = words[s:e]\n",
    "    # compute marginal kernel K\n",
    "    #vals, vecs = custom_decomp()(V)\n",
    "    #K = vecs.mm((1 / (vals + 1)).diag()).mm(vecs.t()) # actually K = (identity - expression) \n",
    "    #marginals = (1 - K.diag()).diag() ## need to rewrite custom_decomp to return full svd + correct gradients. \n",
    "    # so this is the inefficient way\n",
    "    identity = Variable(torch.eye(word.size(0)).type(words.data.type()))\n",
    "    L = V.mm(V.t())\n",
    "    K = identity - custom_inverse()(L + identity)\n",
    "    marginals = (K.diag()).diag()\n",
    "print(marginals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0], [10])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampler.s_ix, sampler.e_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x = np.array([1,2,3,4,5,10,15,20])\n",
    "y = np.array([0.02244, 0.02219,0.02136,0.0206,0.02042,0.02029,0.0203,0.02034])\n",
    "plt.plot(x,y,marker='o')\n",
    "plt.xlabel('Expected Set Size (DPP)')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('Sparsity vs Loss (All Aspects)')\n",
    "plt.savefig('Reg.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(a_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(a_dict['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from SALib.sample import sobol_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = sobol_sequence.sample(10000,20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
