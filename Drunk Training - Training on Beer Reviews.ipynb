{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dpp_nets.layers.layers import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "import shutil\n",
    "import time\n",
    "import gzip\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from dpp_nets.utils.io import make_embd, make_tensor_dataset, load_tensor_dataset\n",
    "from dpp_nets.utils.io import data_iterator, load_embd\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import time\n",
    "from dpp_nets.my_torch.utilities import pad_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Data Sets\n",
    "train_set = torch.load('/Users/Max/data/full_beer/pytorch/annotated_common.pt')\n",
    "rat_set = torch.load('/Users/Max/data/full_beer/pytorch/annotated.pt')\n",
    "embd = load_embd('/Users/Max/data/full_beer/pytorch/embeddings.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "torch.manual_seed(12)\n",
    "batch_size = 25\n",
    "_, max_set_size = train_set.data_tensor.size()\n",
    "_, embd_dim = embd.weight.size()\n",
    "\n",
    "hidden_dim = 500\n",
    "enc_dim = 200\n",
    "target_dim = 3 # let's choose the first three aspects to learn!\n",
    "\n",
    "# Baseline\n",
    "baseline_nets = DeepSetBaseline(embd_dim, hidden_dim, enc_dim, target_dim)\n",
    "baseline = nn.Sequential(embd, baseline_nets, nn.Sigmoid())\n",
    "\n",
    "# Model\n",
    "kernel_dim = 200\n",
    "trainer = MarginalTrainer(embd, hidden_dim, kernel_dim, enc_dim, target_dim)\n",
    "\n",
    "trainer.reg = 0.1\n",
    "trainer.reg_mean = 10\n",
    "trainer.activation = nn.Sigmoid()\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Actual training loop for model\n",
    "torch.manual_seed(12)\n",
    "params = [{'params': trainer.kernel_net.parameters(), 'lr': 1e-3},\n",
    "          {'params': trainer.pred_net.parameters(), 'lr': 1e-4}]\n",
    "\n",
    "optimizer = torch.optim.Adam(params)\n",
    "trainer.reg = 0.1\n",
    "\n",
    "for epoch in range(10):\n",
    "    for t, (review, target) in enumerate(train_loader):\n",
    "        review = Variable(review)\n",
    "        target = Variable(target[:,:3])\n",
    "        loss  = trainer(review, target)\n",
    "        \n",
    "        # Backpropagate + parameter updates\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if not (t+1) % 10: \n",
    "            print('Loss at it :', t+1, 'is', loss.data[0])\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Need also a training script for RTrainer!!\n",
    "# incorporate embedding into trainer\n",
    "\n",
    "\n",
    "kernel_net = KernelVar(embd_dim, hidden_dim, kernel_dim)\n",
    "sampler = ReinforceSampler(3)\n",
    "pred_net = PredNet(embd_dim, hidden_dim, enc_dim, target_dim)\n",
    "\n",
    "Rtrainer = ReinforceTrainer(kernel_net, sampler, pred_net)\n",
    "Rtrainer.reg = 0.1\n",
    "Rtrainer.reg_mean = 10\n",
    "Rtrainer.activation = nn.Sigmoid()\n",
    "\n",
    "params = [{'params': Rtrainer.kernel_net.parameters(), 'lr': 1e-3},\n",
    "          {'params': Rtrainer.pred_net.parameters(), 'lr': 1e-4}]\n",
    "\n",
    "optimizer = torch.optim.Adam(params)\n",
    "\n",
    "Rtrainer.double()\n",
    "\n",
    "for epoch in range(20):\n",
    "    for t, (review, target) in enumerate(train_loader):\n",
    "        words = embd(Variable(review)).double()\n",
    "        target = Variable(target[:,:3]).double()\n",
    "        loss  = Rtrainer(words, target)\n",
    "        \n",
    "        # Backpropagate + parameter updates\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # print(Rtrainer.kernel_net.layer1.weight.grad)\n",
    "        optimizer.step()\n",
    "\n",
    "        if not (t+1) % 10: \n",
    "            print('Loss at it :', t+1, 'is', loss.data[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Actual training loop for baseline\n",
    "# Training\n",
    "criterion = nn.MSELoss()\n",
    "lr = 1e-4\n",
    "optimizer = torch.optim.Adam(baseline_nets.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "for epoch in range(10):\n",
    "    \n",
    "    for t, (review, target) in enumerate(train_loader):\n",
    "        target = Variable(target[:,:3])\n",
    "        words = Variable(review)\n",
    "        pred = baseline(words)\n",
    "        loss = criterion(pred, target)\n",
    "\n",
    "        # Backpropagate + parameter updates\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if not (t+1) % 10: \n",
    "            print('Loss at it :', t+1, 'is', loss.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validate_baseline(val_set, model, criterion):\n",
    "    x = Variable(val_set.data_tensor, volatile=True)\n",
    "    y = Variable(val_set.target_tensor[:,:3], volatile=True)\n",
    "    pred = model(x)\n",
    "    loss = criterion(pred, y)\n",
    "    print(loss.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validate_model(val_set, model):\n",
    "    model.reg = 0\n",
    "    x = Variable(val_set.data_tensor, volatile=True)\n",
    "    x = embd(x)\n",
    "    y = Variable(val_set.target_tensor[:,:3], volatile=True)\n",
    "    loss = model(x, y)\n",
    "    print(loss.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Rtrainer.float()\n",
    "validate_model(train_set, Rtrainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = Variable(train_set.data_tensor, volatile=True)\n",
    "x = embd(x)\n",
    "y = Variable(train_set.target_tensor[:,:3], volatile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sampler = ReinforceSampler(1)\n",
    "Rtrainer.sampler = sampler\n",
    "Rtrainer.alpha_iter = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validate_baseline(train_set, baseline, nn.MSELoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def sample(model, sampler, embd, dataset):\n",
    "    rand = random.randint(0, len(dataset))\n",
    "    x = dataset.data_tensor[rand:rand+2]\n",
    "    x = embd(Variable(x))\n",
    "    y = dataset.target_tensor[rand:rand+2]\n",
    "    kernel = trainer.kernel_net(x)\n",
    "    sampler.s_ix = trainer.kernel_net.s_ix\n",
    "    sampler.e_ix = trainer.kernel_net.e_ix\n",
    "    sampler(kernel, x)\n",
    "    print(sampler.saved_subsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rand = random.randint(0, len(train_set))\n",
    "x = train_set.data_tensor[rand:rand+10]\n",
    "x = embd(Variable(x))\n",
    "y = Variable(train_set.target_tensor[rand:rand+10,:3])\n",
    "Rtrainer(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[i.data.sum() for l in Rtrainer.sampler.saved_subsets for i in l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "help(argparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "help(argparse.ArgumentParser.add_argument)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--foo')\n",
    "parser.parse_args('--foo 1'.split())\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--foo', action='store_const', const=42)\n",
    "parser.parse_args('--foo'.split())\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--foo', action='store_true')\n",
    "parser.add_argument('--bar', action='store_false')\n",
    "args = parser.parse_args('--foo --bar'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "args.bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "int('aspect1'[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "from dpp_nets.utils.io import make_embd, make_tensor_dataset\n",
    "from dpp_nets.layers.layers import DeepSetBaseline\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Baseline (Deep Sets) Trainer')\n",
    "\n",
    "parser.add_argument('-a', '--aspect', type=str, choices=['aspect1', 'aspect2', 'aspect3', 'all'],\n",
    "                    help='what is the target?', required=True)\n",
    "parser.add_argument('--remote', type=int,\n",
    "                    help='training locally or on cluster?', required=True)\n",
    "\n",
    "parser.add_argument('--data_path_local', type=str, default='/Users/Max/data/beer_reviews',\n",
    "                    help='where is the data folder locally?')\n",
    "parser.add_argument('--data_path_remote', type=str, default='/cluster/home/paulusm/data/beer_reviews',\n",
    "                    help='where is the data folder?')\n",
    "\n",
    "parser.add_argument('--ckp_path_local', type=str, default='/Users/Max/checkpoints/beer_reviews',\n",
    "                    help='where is the data folder locally?')\n",
    "\n",
    "parser.add_argument('--ckp_path_remote', type=str, default='/cluster/home/paulusm/checkpoints/beer_reviews',\n",
    "                    help='where is the data folder?')\n",
    "\n",
    "parser.add_argument('-b', '--batch-size', default=50, type=int,\n",
    "                    metavar='N', help='mini-batch size (default: 50)')\n",
    "parser.add_argument('--epochs', default=30, type=int, metavar='N',\n",
    "                    help='number of total epochs to run')\n",
    "#parser.add_argument('--lr-k', '--learning-rate-k', default=0.1, type=float,\n",
    "#                    metavar='LRk', help='initial learning rate for kernel net')\n",
    "#parser.add_argument('--lr-p', '--learning-rate-p', default=0.1, type=float,\n",
    "#                    metavar='LRp', help='initial learning rate for pred net')\n",
    "parser.add_argument('--lr', '--learning-rate', default=0.1, type=float,\n",
    "                    metavar='LR', help='initial learning rate for baseline')\n",
    "#parser.add_argument('--reg', type=float, required=True,\n",
    "#                    metavar='reg', help='regularization constant')\n",
    "#parser.add_argument('--reg-mean', type=float, required=True,\n",
    "#                    metavar='reg_mean', help='regularization_mean')\n",
    "\n",
    "\n",
    "def train(loader, model, criterion, optimizer, aspect):\n",
    "\n",
    "    for t, (review, target) in enumerate(loader):\n",
    "        review = Variable(review)\n",
    "\n",
    "        if args.aspect == 'all':\n",
    "            target = Variable(target[:,:3]).double()\n",
    "        else:\n",
    "            target = Variable(target[:,int(args.aspect[-1])]).double()\n",
    "\n",
    "        pred = model(review)\n",
    "        loss = criterion(pred, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        print(\"Gradient in pred_net is:\", model[1].pred_net[2].weight.grad.data.sum())\n",
    "        optimizer.step()\n",
    "        print('it %d' %t, 'loss is', loss.data[0])\n",
    "\n",
    "def validate(loader, model, criterion, aspect):\n",
    "\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for i, (review, target) in enumerate(loader, 1):\n",
    "\n",
    "        review = Variable(review, volatile=True)\n",
    "\n",
    "        if args.aspect == 'all':\n",
    "            target = Variable(target[:,:3], volatile=True).double()\n",
    "        else:\n",
    "            target = Variable(target[:,int(args.aspect[-1])], volatile=True).double()\n",
    "\n",
    "        pred = model(review)\n",
    "        loss = criterion(pred, target)\n",
    "        \n",
    "        delta = loss.data[0] - total_loss\n",
    "        total_loss += (delta / i)\n",
    "\n",
    "        print(\"validated one batch\")\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "def log(epoch, loss):\n",
    "    string = str.join(\" | \", ['Epoch: %d' % (epoch), 'Validation Loss: %.5f' % (loss)])\n",
    "\n",
    "    if args.remote:\n",
    "        destination = os.path.join(args.ckp_path_remote, args.aspect + 'DeepSetBaseline_log.txt')\n",
    "    else:\n",
    "        destination = os.path.join(args.ckp_path_local, args.aspect + 'DeepSetBaseline_log.txt')\n",
    "\n",
    "    with open(destination, 'a') as log:\n",
    "        log.write(string + '\\n')\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 5 epochs\"\"\"\n",
    "    lr = args.lr * (0.1 ** (epoch // 5))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "def save_checkpoint(state, is_best, filename='baseline_checkpoint.pth.tar'):\n",
    "    \"\"\"\n",
    "    State is a dictionary that cotains valuable information to be saved.\n",
    "    \"\"\"\n",
    "    if args.remote:\n",
    "        destination = os.path.join(args.ckp_path_remote, args.aspect + filename)\n",
    "    else:\n",
    "        destination = os.path.join(args.ckp_path_local, args.aspect + filename)\n",
    "    \n",
    "    torch.save(state, destination)\n",
    "    if is_best:\n",
    "        if args.remote:\n",
    "            best_destination = os.path.join(args.ckp_path_remote, args.aspect + 'baseline_model_best.pth.tar')\n",
    "        else:\n",
    "            best_destination = os.path.join(args.ckp_path_local, args.aspect + 'baseline_model_best.pth.tar')\n",
    "        \n",
    "        shutil.copyfile(destination, best_destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded data\n",
      "loader defined\n"
     ]
    }
   ],
   "source": [
    "global args, lowest_loss\n",
    "\n",
    "args = parser.parse_args('-a aspect1 --remote 0'.split())\n",
    "\n",
    "lowest_loss = 100 # arbitrary high number as upper bound for loss\n",
    "\n",
    "### Load data\n",
    "if args.remote:\n",
    "    # print('training remotely')\n",
    "    train_path = os.path.join(args.data_path_remote, str.join(\".\",['reviews', args.aspect, 'train.txt.gz']))\n",
    "    val_path   = os.path.join(args.data_path_remote, str.join(\".\",['reviews', args.aspect, 'heldout.txt.gz']))\n",
    "    embd_path  = os.path.join(args.data_path_remote, 'review+wiki.filtered.200.txt.gz')\n",
    "\n",
    "else:\n",
    "    # print('training locally')\n",
    "    train_path = os.path.join(args.data_path_local, str.join(\".\",['reviews', args.aspect, 'train.txt.gz']))\n",
    "    val_path   = os.path.join(args.data_path_local, str.join(\".\",['reviews', args.aspect, 'heldout.txt.gz']))\n",
    "    embd_path = os.path.join(args.data_path_local, 'review+wiki.filtered.200.txt.gz')\n",
    "\n",
    "embd, word_to_ix = make_embd(embd_path)\n",
    "train_set = make_tensor_dataset(train_path, word_to_ix)\n",
    "val_set = make_tensor_dataset(val_path, word_to_ix)\n",
    "print(\"loaded data\")\n",
    "\n",
    "torch.manual_seed(0)\n",
    "train_loader = DataLoader(train_set, args.batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_set, args.batch_size)\n",
    "print(\"loader defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created model\n",
      "set up optimizer\n"
     ]
    }
   ],
   "source": [
    "### Build model\n",
    "# Network parameters\n",
    "embd_dim = embd.weight.size(1)\n",
    "hidden_dim = 500\n",
    "enc_dim = 200\n",
    "if args.aspect == 'all':\n",
    "    target_dim = 3\n",
    "else: \n",
    "    target_dim = 1\n",
    "\n",
    "# Model\n",
    "torch.manual_seed(0)\n",
    "net = DeepSetBaseline(embd_dim, hidden_dim, enc_dim, target_dim)\n",
    "activation = nn.Sigmoid()\n",
    "model = nn.Sequential(embd, net, activation)\n",
    "#model.double()\n",
    "print(\"created model\")\n",
    "\n",
    "### Set-up training\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=args.lr)\n",
    "print(\"set up optimizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started loop\n",
      "Gradient in pred_net is: 0.0\n",
      "it 0 loss is 0.18479999394416835\n",
      "Gradient in pred_net is: 0.0\n",
      "it 1 loss is 0.15859999790191665\n",
      "Gradient in pred_net is: 0.0\n",
      "it 2 loss is 0.15119999508857745\n",
      "Gradient in pred_net is: 0.0\n",
      "it 3 loss is 0.14079999823570274\n",
      "Gradient in pred_net is: 0.0\n",
      "it 4 loss is 0.1901999958038332\n",
      "Gradient in pred_net is: 0.0\n",
      "it 5 loss is 0.1645999945640566\n",
      "Gradient in pred_net is: 0.0\n",
      "it 6 loss is 0.17959999456405662\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-7cac1e1f066d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0madjust_learning_rate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maspect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maspect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-5c996fde682f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(loader, model, criterion, optimizer, aspect)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Gradient in pred_net is:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpred_net\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Max/Coding/anaconda2/envs/torch/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_variables)\u001b[0m\n\u001b[1;32m    144\u001b[0m                     'or with gradient w.r.t. the variable')\n\u001b[1;32m    145\u001b[0m             \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_as_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execution_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### Loop\n",
    "torch.manual_seed(0)\n",
    "print(\"started loop\")\n",
    "for epoch in range(args.epochs):\n",
    "\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "\n",
    "    train(train_loader, model, criterion, optimizer, args.aspect)    \n",
    "    loss = validate(val_loader, model, criterion, args.aspect)\n",
    "\n",
    "    log(epoch, loss)\n",
    "    print(\"logged\")\n",
    "\n",
    "    is_best = loss < lowest_loss\n",
    "    lowest_loss = min(loss, lowest_loss)    \n",
    "    save = {'epoch:': epoch + 1, \n",
    "            'model': 'Deep Set Baseline',\n",
    "            'state_dict': model.state_dict(),\n",
    "            'lowest_loss': lowest_loss,\n",
    "            'optimizer': optimizer.state_dict()} \n",
    "\n",
    "    save_checkpoint(save, is_best)\n",
    "    print(\"saved a checkpoint\")\n",
    "\n",
    "print('*'*20, 'SUCCESS','*'*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction is : \n",
      " 0.5172\n",
      " 0.5269\n",
      " 0.5146\n",
      " 0.5167\n",
      " 0.5268\n",
      " 0.5240\n",
      " 0.5447\n",
      " 0.5597\n",
      " 0.5388\n",
      " 0.5239\n",
      " 0.5368\n",
      " 0.5403\n",
      " 0.5190\n",
      " 0.5227\n",
      " 0.5545\n",
      " 0.5395\n",
      " 0.5251\n",
      " 0.5317\n",
      " 0.5403\n",
      " 0.5269\n",
      " 0.5220\n",
      " 0.5335\n",
      " 0.5372\n",
      " 0.5404\n",
      " 0.5165\n",
      " 0.5143\n",
      " 0.5450\n",
      " 0.5260\n",
      " 0.5126\n",
      " 0.5535\n",
      " 0.5305\n",
      " 0.5317\n",
      " 0.5267\n",
      " 0.5156\n",
      " 0.5205\n",
      " 0.5207\n",
      " 0.5343\n",
      " 0.5372\n",
      " 0.5422\n",
      " 0.5179\n",
      " 0.5264\n",
      " 0.5125\n",
      " 0.5168\n",
      " 0.5182\n",
      " 0.5213\n",
      " 0.5174\n",
      " 0.5299\n",
      " 0.5250\n",
      " 0.5203\n",
      " 0.5579\n",
      "[torch.FloatTensor of size 50x1]\n",
      "\n",
      "loss is:  0.07014837116003036\n",
      "weight grad is:  0.01934915469658871\n",
      "prediction is : \n",
      " 0.6669\n",
      " 0.7977\n",
      " 0.7743\n",
      " 0.6152\n",
      " 0.6556\n",
      " 0.6171\n",
      " 0.6603\n",
      " 0.6605\n",
      " 0.7936\n",
      " 0.6457\n",
      " 0.6201\n",
      " 0.6429\n",
      " 0.6651\n",
      " 0.6441\n",
      " 0.7832\n",
      " 0.8832\n",
      " 0.5803\n",
      " 0.6624\n",
      " 0.6808\n",
      " 0.6893\n",
      " 0.7007\n",
      " 0.8820\n",
      " 0.6278\n",
      " 0.6435\n",
      " 0.6583\n",
      " 0.6121\n",
      " 0.5920\n",
      " 0.6283\n",
      " 0.7124\n",
      " 0.7844\n",
      " 0.6065\n",
      " 0.6695\n",
      " 0.6448\n",
      " 0.6322\n",
      " 0.7636\n",
      " 0.6886\n",
      " 0.7784\n",
      " 0.6430\n",
      " 0.6169\n",
      " 0.7256\n",
      " 0.7379\n",
      " 0.8642\n",
      " 0.6288\n",
      " 0.8155\n",
      " 0.6472\n",
      " 0.5788\n",
      " 0.6433\n",
      " 0.6254\n",
      " 0.6049\n",
      " 0.6841\n",
      "[torch.FloatTensor of size 50x1]\n",
      "\n",
      "loss is:  0.0598050020635128\n",
      "weight grad is:  0.00044359280987094074\n",
      "prediction is : \n",
      " 0.6326\n",
      " 0.8549\n",
      " 0.9595\n",
      " 0.7150\n",
      " 0.7849\n",
      " 0.6454\n",
      " 0.8505\n",
      " 0.7703\n",
      " 0.8144\n",
      " 0.6898\n",
      " 0.8075\n",
      " 0.9508\n",
      " 0.7509\n",
      " 0.6472\n",
      " 0.7811\n",
      " 0.7116\n",
      " 0.6389\n",
      " 0.9834\n",
      " 0.7821\n",
      " 0.7433\n",
      " 0.6552\n",
      " 0.9226\n",
      " 0.6381\n",
      " 0.6387\n",
      " 0.7680\n",
      " 0.7887\n",
      " 0.8903\n",
      " 0.7872\n",
      " 0.6502\n",
      " 0.8384\n",
      " 0.7455\n",
      " 0.8084\n",
      " 0.6474\n",
      " 0.7606\n",
      " 0.8740\n",
      " 0.6054\n",
      " 0.7578\n",
      " 0.7840\n",
      " 0.7294\n",
      " 0.6830\n",
      " 0.7472\n",
      " 0.6146\n",
      " 0.8098\n",
      " 0.8293\n",
      " 0.9079\n",
      " 0.6730\n",
      " 0.7512\n",
      " 0.7498\n",
      " 0.9198\n",
      " 0.7400\n",
      "[torch.FloatTensor of size 50x1]\n",
      "\n",
      "loss is:  0.07411463558673859\n",
      "weight grad is:  0.00987993893422956\n",
      "prediction is : \n",
      " 0.8609\n",
      " 0.7629\n",
      " 0.8271\n",
      " 0.6502\n",
      " 0.6385\n",
      " 0.7385\n",
      " 0.9329\n",
      " 0.6419\n",
      " 0.7108\n",
      " 0.7524\n",
      " 0.7090\n",
      " 0.7194\n",
      " 0.9225\n",
      " 0.7170\n",
      " 0.7764\n",
      " 0.8291\n",
      " 0.7818\n",
      " 0.6556\n",
      " 0.9696\n",
      " 0.8596\n",
      " 0.8972\n",
      " 0.6454\n",
      " 0.7809\n",
      " 0.7884\n",
      " 0.7275\n",
      " 0.8707\n",
      " 0.7152\n",
      " 0.8070\n",
      " 0.7522\n",
      " 0.8283\n",
      " 0.6939\n",
      " 0.6892\n",
      " 0.7829\n",
      " 0.7101\n",
      " 0.8975\n",
      " 0.7398\n",
      " 0.6763\n",
      " 0.8340\n",
      " 0.7615\n",
      " 0.6797\n",
      " 0.7926\n",
      " 0.9572\n",
      " 0.8679\n",
      " 0.7812\n",
      " 0.8216\n",
      " 0.7939\n",
      " 0.8363\n",
      " 0.6289\n",
      " 0.7061\n",
      " 0.7498\n",
      "[torch.FloatTensor of size 50x1]\n",
      "\n",
      "loss is:  0.07525406032800674\n",
      "weight grad is:  0.011051712721934281\n",
      "prediction is : \n",
      " 0.8464\n",
      " 0.7734\n",
      " 0.6838\n",
      " 0.7732\n",
      " 0.7591\n",
      " 0.6745\n",
      " 0.9218\n",
      " 0.7416\n",
      " 0.7672\n",
      " 0.7621\n",
      " 0.6135\n",
      " 0.7850\n",
      " 0.9203\n",
      " 0.8517\n",
      " 0.8660\n",
      " 0.8295\n",
      " 0.7540\n",
      " 0.6920\n",
      " 0.9262\n",
      " 0.6222\n",
      " 0.6650\n",
      " 0.8560\n",
      " 0.7813\n",
      " 0.7873\n",
      " 0.6370\n",
      " 0.8032\n",
      " 0.7006\n",
      " 0.7973\n",
      " 0.8251\n",
      " 0.8886\n",
      " 0.7197\n",
      " 0.6179\n",
      " 0.6960\n",
      " 0.8118\n",
      " 0.8839\n",
      " 0.9696\n",
      " 0.8098\n",
      " 0.7492\n",
      " 0.7238\n",
      " 0.8412\n",
      " 0.6798\n",
      " 0.7624\n",
      " 0.8543\n",
      " 0.7455\n",
      " 0.6301\n",
      " 0.7803\n",
      " 0.6995\n",
      " 0.8073\n",
      " 0.9273\n",
      " 0.8994\n",
      "[torch.FloatTensor of size 50x1]\n",
      "\n",
      "loss is:  0.066806361079216\n",
      "weight grad is:  0.0077369672340346085\n",
      "prediction is : \n",
      " 0.9375\n",
      " 0.7649\n",
      " 0.7065\n",
      " 0.8915\n",
      " 0.6187\n",
      " 0.9525\n",
      " 0.6148\n",
      " 0.8824\n",
      " 0.6225\n",
      " 0.8536\n",
      " 0.6496\n",
      " 0.8189\n",
      " 0.6606\n",
      " 0.8300\n",
      " 0.7659\n",
      " 0.7445\n",
      " 0.7550\n",
      " 0.8591\n",
      " 0.6462\n",
      " 0.6210\n",
      " 0.8565\n",
      " 0.7943\n",
      " 0.7210\n",
      " 0.8685\n",
      " 0.6862\n",
      " 0.7526\n",
      " 0.7928\n",
      " 0.7350\n",
      " 0.6623\n",
      " 0.6472\n",
      " 0.6307\n",
      " 0.7438\n",
      " 0.6693\n",
      " 0.9126\n",
      " 0.9196\n",
      " 0.9786\n",
      " 0.5921\n",
      " 0.7980\n",
      " 0.7255\n",
      " 0.7826\n",
      " 0.8762\n",
      " 0.6997\n",
      " 0.7079\n",
      " 0.7966\n",
      " 0.6206\n",
      " 0.6122\n",
      " 0.6389\n",
      " 0.6932\n",
      " 0.7083\n",
      " 0.8739\n",
      "[torch.FloatTensor of size 50x1]\n",
      "\n",
      "loss is:  0.05019975081086159\n",
      "weight grad is:  0.01131209430351543\n",
      "prediction is : \n",
      " 0.7432\n",
      " 0.9196\n",
      " 0.7886\n",
      " 0.8651\n",
      " 0.6710\n",
      " 0.5867\n",
      " 0.6629\n",
      " 0.9456\n",
      " 0.8812\n",
      " 0.8095\n",
      " 0.6395\n",
      " 0.9033\n",
      " 0.8419\n",
      " 0.7221\n",
      " 0.8391\n",
      " 0.6452\n",
      " 0.7425\n",
      " 0.7956\n",
      " 0.6943\n",
      " 0.6848\n",
      " 0.7045\n",
      " 0.7009\n",
      " 0.6100\n",
      " 0.7622\n",
      " 0.7689\n",
      " 0.6225\n",
      " 0.8804\n",
      " 0.7536\n",
      " 0.5943\n",
      " 0.6394\n",
      " 0.7530\n",
      " 0.6698\n",
      " 0.6281\n",
      " 0.7474\n",
      " 0.5979\n",
      " 0.6891\n",
      " 0.6096\n",
      " 0.7241\n",
      " 0.7634\n",
      " 0.6060\n",
      " 0.6260\n",
      " 0.7612\n",
      " 0.7519\n",
      " 0.9580\n",
      " 0.6923\n",
      " 0.7161\n",
      " 0.6481\n",
      " 0.7334\n",
      " 0.6063\n",
      " 0.6907\n",
      "[torch.FloatTensor of size 50x1]\n",
      "\n",
      "loss is:  0.058398742228746414\n",
      "weight grad is:  0.004142957284178461\n",
      "prediction is : \n",
      " 0.6512\n",
      " 0.7611\n",
      " 0.7623\n",
      " 0.7033\n",
      " 0.7731\n",
      " 0.6549\n",
      " 0.6664\n",
      " 0.6609\n",
      " 0.7125\n",
      " 0.6428\n",
      " 0.6602\n",
      " 0.6383\n",
      " 0.6304\n",
      " 0.7169\n",
      " 0.6250\n",
      " 0.7226\n",
      " 0.7541\n",
      " 0.7108\n",
      " 0.7046\n",
      " 0.7224\n",
      " 0.7338\n",
      " 0.6617\n",
      " 0.6079\n",
      " 0.6552\n",
      " 0.6417\n",
      " 0.7310\n",
      " 0.7811\n",
      " 0.7278\n",
      " 0.7661\n",
      " 0.7791\n",
      " 0.6016\n",
      " 0.7479\n",
      " 0.6580\n",
      " 0.7852\n",
      " 0.6492\n",
      " 0.6237\n",
      " 0.6484\n",
      " 0.6807\n",
      " 0.5728\n",
      " 0.7729\n",
      " 0.6688\n",
      " 0.8561\n",
      " 0.6742\n",
      " 0.6446\n",
      " 0.7590\n",
      " 0.6388\n",
      " 0.7305\n",
      " 0.7261\n",
      " 0.6737\n",
      " 0.7040\n",
      "[torch.FloatTensor of size 50x1]\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-ece696eeb2b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss is: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Max/Coding/anaconda2/envs/torch/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_variables)\u001b[0m\n\u001b[1;32m    144\u001b[0m                     'or with gradient w.r.t. the variable')\n\u001b[1;32m    145\u001b[0m             \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_as_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execution_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# A manual loop\n",
    "loader = train_loader\n",
    "net = DeepSetBaseline(embd_dim, hidden_dim, enc_dim, target_dim)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-4)\n",
    "#optimizer = torch.optim.SGD(net.parameters(), lr=1e-1, momentum=0.9)\n",
    "embd = embd.float()\n",
    "for t, (review, target) in enumerate(loader):\n",
    "    review = Variable(review)\n",
    "    words = embd(review)\n",
    "    output = net(words)\n",
    "    \n",
    "    pred = nn.Sigmoid()(output)\n",
    "    print(\"prediction is :\", pred.data)\n",
    "\n",
    "    if args.aspect == 'all':\n",
    "        target = Variable(target[:,:3])\n",
    "    else:\n",
    "        target = Variable(target[:,int(args.aspect[-1])])\n",
    "\n",
    "    loss = criterion(pred, target)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print('loss is: ', loss.data[0])\n",
    "    print('weight grad is: ',net.enc_layer1.weight.grad.data.sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "-4.0917e-02  6.2794e-02  5.8092e-03  ...  -6.3790e-02 -1.8017e-02 -2.8331e-02\n",
       " 4.1147e-02  3.6015e-02 -5.7746e-02  ...   3.8079e-02  2.4789e-02  3.2314e-02\n",
       "-5.6748e-02  6.3613e-02  2.3695e-02  ...  -6.8372e-02 -9.2158e-03  4.3017e-02\n",
       "                ...                   ⋱                   ...                \n",
       "-3.5129e-02  4.6058e-03  5.5488e-02  ...  -5.1774e-02 -2.6169e-02 -3.1497e-02\n",
       "-5.9451e-02  5.8607e-02  8.8602e-04  ...  -4.3175e-03  3.9091e-02 -5.3066e-02\n",
       " 2.5660e-02  4.8838e-02 -4.3077e-02  ...  -6.5688e-02 -4.9512e-02 -5.4340e-02\n",
       "[torch.FloatTensor of size 500x200]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.enc_layer1.weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       " 0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       "-0.0425 -0.0521  0.0683  ...   0.1130  0.0135  0.0482\n",
       "-0.0534 -0.0038 -0.0476  ...  -0.0365  0.0941 -0.0478\n",
       "          ...             ⋱             ...          \n",
       " 0.0093 -0.0598  0.0637  ...  -0.0051  0.0202 -0.0329\n",
       " 0.0317 -0.0415 -0.0221  ...   0.0125 -0.0892 -0.0764\n",
       "-0.0223 -0.0166  0.0155  ...   0.0024 -0.0372  0.0276\n",
       "[torch.DoubleTensor of size 147760x200]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embd.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
