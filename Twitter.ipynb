{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import csv\n",
    "import gzip\n",
    "import nltk\n",
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dictionary structure \n",
    "import gzip\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from dpp_nets.utils.io import embd_iterator, make_embd\n",
    "from collections import namedtuple\n",
    "from nltk import word_tokenize\n",
    "import torch\n",
    "import numpy as np\n",
    "from dpp_nets.my_torch.utilities import pad_tensor\n",
    "\n",
    "id_to_content = {}\n",
    "errors = []\n",
    "#embd_path = '/Users/Max/data/askubuntu/vectors_stackexchange.txt.gz'\n",
    "embd_path = '/Users/Max/data/askubuntu/vectors_pruned.200.txt.gz'\n",
    "embd_layer, word_to_ix  = make_embd(embd_path)\n",
    "question = namedtuple('question', 'id title body title_ix body_ix')\n",
    "path = '/Users/Max/data/askubuntu/text_tokenized.txt.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_path = '/Users/Max/data/askubuntu/dev.txt'\n",
    "count = 0\n",
    "import torch\n",
    "\n",
    "with open(val_path) as f:\n",
    "    data_tensor = []\n",
    "    target_tensor = []\n",
    "    \n",
    "    for line in f:\n",
    "        q_id, pos, candidates, _ = line.split(\"\\t\")\n",
    "        q_id = int(q_id)\n",
    "        pos = [int(id) for id in pos.split()]\n",
    "        candidates = [int(id) for id in candidates.split()]\n",
    "        \n",
    "        # Check\n",
    "        if len(pos) == len(candidates):\n",
    "            continue\n",
    "        \n",
    "        # Create target\n",
    "        target = [1 if i in pos else 0 for i in candidates]\n",
    "        target = [-1] + target\n",
    "        target = torch.ByteTensor(target)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'id_to_content' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-b0883ba00de8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# Create x Tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mall_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mq_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mall_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mid_to_content\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mall_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpad_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle_ix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_title_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mall_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-b0883ba00de8>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# Create x Tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mall_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mq_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mall_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mid_to_content\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mall_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpad_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle_ix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_title_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mall_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'id_to_content' is not defined"
     ]
    }
   ],
   "source": [
    "        # Create x Tensor \n",
    "        all_ids = [q_id] + candidates \n",
    "        all_ids = [id_to_content[id] for id in all_ids]\n",
    "        all_ids = [pad_tensor(torch.LongTensor(np.array(id.title_ix)),0,0,max_title_size) for id in all_ids]\n",
    "        all_ids = torch.stack(all_ids)\n",
    "        data_tensor.append(all_ids)\n",
    "        target_tensor.append(target)\n",
    "        \n",
    "    data_tensor = torch.stack(data_tensor)\n",
    "    target_tensor = torch.stack(target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([193, 20])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_ds.target_tensor.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([193, 21, 38])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_ds.data_tensor.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embd_layer, word_to_ix  = make_embd(embd_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_content, word_to_ix, max_title_size, max_body_size = create_id_to_content(data_base_path, word_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embd = update_embd(embd_layer, word_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "title_ds, _ = create_train_set(train_path, id_to_content, max_title_size, max_body_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titles = []\n",
    "bodies = []\n",
    "targets = []\n",
    "\n",
    "with open(train_path) as f:\n",
    "    for line in f:\n",
    "        q_id, pos, neg = line.split(\"\\t\")\n",
    "        q_id = int(q_id)\n",
    "        q_content = id_to_content[q_id]\n",
    "        q_title = pad_tensor(torch.LongTensor(np.array(q_content.title_ix)),0,0, max_title_size)\n",
    "        q_body = pad_tensor(torch.LongTensor(np.array(q_content.body_ix)), 0,0, max_body_size)\n",
    "        \n",
    "        pos = [id_to_content[int(id)] for id in pos.split()]\n",
    "        neg = [id_to_content[int(id)] for id in neg.split()]\n",
    "        \n",
    "        targets = [for q in ]\n",
    "        targets.extend([1] * len(pos))\n",
    "        targets.extend([-1] * len(neg))\n",
    "\n",
    "        pos_pairs_title = [torch.stack([q_title, pad_tensor(torch.LongTensor(np.array(q.title_ix)),0,0,max_title_size)]) \n",
    "                           for q in pos]\n",
    "        neg_pairs_title = [torch.stack([q_title, pad_tensor(torch.LongTensor(np.array(q.title_ix)),0,0,max_title_size)])\n",
    "                            for q in neg]\n",
    "\n",
    "        titles.extend(pos_pairs_title)\n",
    "        titles.extend(neg_pairs_title)        \n",
    "\n",
    "        pos_pairs_body = [torch.stack([q_body, pad_tensor(torch.LongTensor(np.array(q.body_ix)),0,0,max_body_size)]) \n",
    "                           for q in pos]\n",
    "        neg_pairs_body = [torch.stack([q_body, pad_tensor(torch.LongTensor(np.array(q.body_ix)),0,0,max_body_size)]) \n",
    "                           for q in neg]\n",
    "\n",
    "        bodies.extend(pos_pairs_body)\n",
    "        bodies.extend(neg_pairs_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embd_path = '/Users/Max/data/askubuntu/vectors_pruned.200.txt.gz'\n",
    "data_base_path = '/Users/Max/data/askubuntu/text_tokenized.txt.gz'\n",
    "train_path = '/Users/Max/data/askubuntu/train_random.txt'\n",
    "val_path = '/Users/Max/data/askubuntu/dev.txt'\n",
    "\n",
    "from dpp_nets.utils.ubuntu_io import create_ubuntu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "embd, train_ds, val_ds = create_ubuntu(embd_path, data_base_path, train_path, val_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(loader, model, criterion, optimizer):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for t, (review, target) in enumerate(loader):\n",
    "        \n",
    "        review = Variable(review)\n",
    "\n",
    "        if args.aspect == 'all':\n",
    "            target = Variable(target[:,:3])\n",
    "        else:\n",
    "            target = Variable(target[:,int(args.aspect[-1])])\n",
    "\n",
    "        pred = model(review)\n",
    "        loss = criterion(pred, target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(\"trained one batch\")\n",
    "\n",
    "def validate(loader, model, criterion):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    MAP = 0.0\n",
    "\n",
    "    for i, (review, target) in enumerate(loader, 1):\n",
    "\n",
    "        review = Variable(review, volatile=True)\n",
    "\n",
    "        if args.aspect == 'all':\n",
    "            target = Variable(target[:,:3], volatile=True)\n",
    "        else:\n",
    "            target = Variable(target[:,int(args.aspect[-1])], volatile=True)\n",
    "\n",
    "        pred = model(review)\n",
    "        loss = criterion(pred, target)\n",
    "        \n",
    "        delta = loss.data[0] - total_loss\n",
    "        total_loss += (delta / i)\n",
    "\n",
    "        print(\"validated one batch\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn.functional.C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embd_dim = embd.weight.size(1)\n",
    "hidden_dim = 500\n",
    "enc_dim = 200\n",
    "target_dim = 50\n",
    "\n",
    "# Model\n",
    "torch.manual_seed(0)\n",
    "net = DeepSetBaseline(embd_dim, hidden_dim, enc_dim, target_dim)\n",
    "model = nn.Sequential(embd, net)\n",
    "print(\"created model\")\n",
    "\n",
    "### Set-up training\n",
    "criterion = nn.CosineEmbeddingLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.003)\n",
    "print(\"set up optimizer\")\n",
    "\n",
    "embd.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds.data_tensor[1:10,0,:].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_ds, 20, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.train()\n",
    "\n",
    "for t, (qs, target) in enumerate(train_loader):\n",
    "    q1 = Variable(qs[:,0,:])\n",
    "    q2 = Variable(qs[:,1,:])\n",
    "\n",
    "    target = Variable(target)\n",
    "\n",
    "    pred1 = model(q1)\n",
    "    pred2 = model(q2)\n",
    "\n",
    "    loss = criterion(pred1, pred2, target)\n",
    "    loss.backward()\n",
    "    print(loss.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch = Variable(qs[:,0,:])\n",
    "words = embd(batch)\n",
    "embd.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embd_dim = 200\n",
    "hidden_dim = 500\n",
    "enc_dim = 200\n",
    "target_dim = 50\n",
    "\n",
    "batch_size, max_set_size, embd_dim = words.size()\n",
    "\n",
    "# Unpacking to send through encoder network\n",
    "# Register indices of individual instances in batch for reconstruction\n",
    "lengths = words.sum(2).abs().sign().sum(1)\n",
    "s_ix = list(lengths.squeeze().cumsum(0).long().data - lengths.squeeze().long().data)\n",
    "e_ix = list(lengths.squeeze().cumsum(0).long().data)\n",
    "\n",
    "# Filter out zero words \n",
    "mask = words.sum(2).abs().sign().expand_as(words).byte()\n",
    "words = words.masked_select(mask).view(-1, embd_dim)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_ap(scores, target):\n",
    "    \n",
    "    if not target.sum():\n",
    "        return 0 \n",
    "    \n",
    "    # sort the targets according to scores\n",
    "    order = scores.argsort()\n",
    "    copy = target.copy()\n",
    "    target = copy[order]\n",
    "    \n",
    "    # compute average precision\n",
    "    tp_k = target.cumsum() * target\n",
    "    tot_k = np.arange(1, len(target) + 1)\n",
    "    p_k = tp_k / tot_k\n",
    "    ap = np.mean(p_k[p_k > 0])\n",
    "    \n",
    "    return ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "scores = np.array([3.1, 1.2, 4.4, 2.1, 5.5])\n",
    "target = np.array([0, 0, 0, 0, 1])\n",
    "compute_ap(scores, target)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
