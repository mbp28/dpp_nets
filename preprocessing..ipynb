{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "import gzip\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_path = '/Users/Max/data/beer_reviews/reviews.all.train.txt.gz'\n",
    "val_path = '/Users/Max/data/beer_reviews/reviews.all.heldout.txt.gz'\n",
    "\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70000\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "check_path = '/Users/Max/data/full_beer/aspects/reviews.aspect1.train.txt.gz'\n",
    "\n",
    "with gzip.open(check_path, 'rt') as f:\n",
    "    lines = f.readlines()\n",
    "print(len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:22<00:00, 56.87it/s]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "for line in tqdm.tqdm(lines[:1000]):\n",
    "    target, _, review = line.partition(\"\\t\")\n",
    "    doc = nlp(review)\n",
    "    \n",
    "    # parsing in spacy style\n",
    "    words = [tuple([token.text]) for token in doc]\n",
    "    sents = [tuple([token.text for token in sent]) for sent in doc.sents]\n",
    "    chunks = [tuple([word.text for word in token.subtree if word.text != '\\n' and word.text != '\\t']) for token in doc]\n",
    "    \n",
    "    # creating encodings\n",
    "    enc_words  = target + '\\D' + '\\T'.join(['\\W'.join(tup) for tup in words])\n",
    "    enc_sents  = target + '\\D' + '\\T'.join(['\\W'.join(tup) for tup in sents])\n",
    "    enc_chunks = target + '\\D' + '\\T'.join(['\\W'.join(tup) for tup in chunks]) + '\\n'\n",
    "    \n",
    "    with gzip.open('/Users/Max/data/beer_reviews/reviews.short.train.words.txt.gz', 'at') as f:\n",
    "        f.write(enc_words)\n",
    "    \n",
    "    with gzip.open('/Users/Max/data/beer_reviews/reviews.short.train.chunks.txt.gz', 'at') as f:\n",
    "        f.write(enc_chunks)\n",
    "        \n",
    "    with gzip.open('/Users/Max/data/beer_reviews/reviews.short.train.sents.txt.gz', 'at') as f:\n",
    "        f.write(enc_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with gzip.open('/Users/Max/data/beer_reviews/reviews.all.train.chunks.txt.gz', 'rt') as f:\n",
    "    my_lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Decode\n",
    "obj = my_lines[128383]\n",
    "target, review = obj.split('\\D')\n",
    "[tuple(chunk.split('\\W')) for chunk in review.split('\\T')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_lines[129322]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parsing in spacy style\n",
    "words = [tuple([token.text]) for token in doc]\n",
    "sents = [tuple([token.text for token in sent]) for sent in doc.sents]\n",
    "chunks = [tuple([word.text for word in token.subtree]) for token in doc]\n",
    "\n",
    "# creating encodings\n",
    "enc_words  = target + '\\D' + '\\T'.join(['\\W'.join(tup) for tup in words])\n",
    "enc_sents  = target + '\\D' + '\\T'.join(['\\W'.join(tup) for tup in sents])\n",
    "enc_chunks = target + '\\D' + '\\T'.join(['\\W'.join(tup) for tup in chunks])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target, review = enc_chunks.split('\\D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "review.split('\\T')[0].split('\\W')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "enc_chunks[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import gzip\n",
    "\n",
    "def read_rationales(path):\n",
    "    \"\"\"\n",
    "    This reads the json.annotations file. \n",
    "    Creates a list of dictionaries, which holds the 994 reviews for which\n",
    "    sentence-level annotations are available. \n",
    "    \"\"\"\n",
    "    data = []\n",
    "    fopen = gzip.open if path.endswith(\".gz\") else open\n",
    "    with fopen(path) as fin:\n",
    "        for line in fin:\n",
    "            item = json.loads(line)\n",
    "            data.append(item)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import ast\n",
    "nlp = spacy.load('en')\n",
    "anno = '/Users/Max/data/beer_reviews/annotations.json'\n",
    "annotations = read_rationales(anno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Better\n",
    "# For each sentence that deserves a label, we extract the sentence from doc that is closest. \n",
    "\n",
    "def map_sentence(sen, tup):\n",
    "    sen = set([str(token).lower() for token in sen])\n",
    "    n = len(sen) #+ len(tup[0])\n",
    "    s = len(sen & tup[0])\n",
    "    score = s / n\n",
    "    return score\n",
    "\n",
    "# Which sentences deserve a label and what label?\n",
    "ix = 2\n",
    "review = annotations[ix]\n",
    "doc = nlp(ast.literal_eval(annotations[ix]['raw'])['review/text'])\n",
    "all_words = review['x']\n",
    "label_sens = []\n",
    "for label in ['0','1','2']:\n",
    "    label_sens.extend([(set(all_words[s:e]), label) for s, e in review[label]])\n",
    "\n",
    "# Label the sentences in doc\n",
    "sentences = [(sen, set()) for sen in doc.sents]\n",
    "for tup in label_sens:\n",
    "    scores = ([map_sentence(sen, tup) for sen, _ in sentences])\n",
    "    # print(scores)\n",
    "    sentences[scores.index(max(scores))][1].add(tup[1])\n",
    "\n",
    "# Process the sentences\n",
    "words = []\n",
    "chunks= []\n",
    "#mode = 'chunks'\n",
    "\n",
    "for tup in sentences:    \n",
    "    words1 = [tuple([tuple([token.text]),tup[1]]) for token in tup[0]]\n",
    "    chunks1 = [tuple([tuple([word.text for word in token.subtree if word.text != '\\n' and word.text != '\\t']),tup[1]]) for token in tup[0]]\n",
    "    words.extend(words1)\n",
    "    chunks.extend(chunks1)\n",
    "sents = sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[review[label] for label in ['0','1','2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
